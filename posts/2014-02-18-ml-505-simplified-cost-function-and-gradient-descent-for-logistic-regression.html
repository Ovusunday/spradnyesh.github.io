<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Today is better than yesterday!: ml-505: Simplified Cost Function and Gradient Descent for Logistic Regression</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='http://fonts.googleapis.com/css?family=Alegreya:400italic,700italic,400,700' rel='stylesheet'
          type='text/css'>
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/default.min.css">
    <link href="../css/screen.css" rel="stylesheet" type="text/css" />
</head>
<body>


<nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/index.html">Today is better than yesterday!</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="/index.html">Home</a></li>
                <li
                ><a href="/archives.html">Archives</a></li>
                
                <li
                >
                <a href="/pages/resume.html">Resume</a>
                </li>
                
                <li><a href="/feed.xml">RSS</a></li>
            </ul>
        </div><!--/.nav-collapse -->
    </div><!--/.container-fluid -->
</nav>


<div class="container">


    <div class="row">
        <div class="col-lg-9">
            <div id="content">
                
<div id="post">
    <div class="post-header">
    <div id="post-meta" class="row">
        <div class="col-lg-6">18 February, 2014</div>
        
    </div>
    <h2>ml-505: Simplified Cost Function and Gradient Descent for Logistic Regression</h2>
</div>
<div>
    </ol class="contents">
    <p>Hello, welcome back! In the last <a href='http://www.golb.in/ml-504-cost-function-for-logistic-regression-54.html'>post</a> we defined the cost function for our logistic regression implementation. Today we will simplify it, and also move onto the next stage, viz minimization of the cost function.</p><p>As a short recap, in the last <a href='http://www.golb.in/ml-504-cost-function-for-logistic-regression-54.html'>post</a>, we defined the cost function as</p><blockquote><p> cost(h<sub>&theta;</sub>(x), y) = -log(h<sub>&theta;</sub>(x)), if y = 1 </p></blockquote><blockquote><p> -log(1 = h<sub>&theta;</sub>(x)), if y = 0 </p></blockquote><p>The same can be rewritten simply as follows:</p><blockquote><p> cost(h<sub>&theta;</sub>(x), y) = -y log(h<sub>&theta;</sub>(x)) - (1 - y) log(1 - h<sub>&theta;</sub>(x)) </p></blockquote><p>To see how this is the same as the previous one, we need to remember that our output variable "y" can take only 1 of 2 possible values, viz "0" and "1". So let's try to substitute these values in our 2nd equation to see if we can get to the 1st</p><p>When "y = 1"</p><blockquote><p> cost(h<sub>&theta;</sub>(x), y) = -1 log(h<sub>&theta;</sub>(x)) - (1 - 1) log(1 - h<sub>&theta;</sub>(x)) </p></blockquote><blockquote><p> = -log(h<sub>&theta;</sub>(x)) - 0 * log(1 - h<sub>&theta;</sub>(x)) </p></blockquote><blockquote><p> = -log(h<sub>&theta;</sub>(x)) </p></blockquote><p>Similarly, when "y = 0"</p><blockquote><p> cost(h<sub>&theta;</sub>(x), y) = -0 * log(h<sub>&theta;</sub>(x)) - (1 - 0) log(1 - h<sub>&theta;</sub>(x)) </p></blockquote><blockquote><p> = -0 - 1 * log(1 - h<sub>&theta;</sub>(x)) </p></blockquote><blockquote><p> = -log(1 - h<sub>&theta;</sub>(x)) </p></blockquote><p>Combining the above 2 new equations, we get</p><blockquote><p> cost(h<sub>&theta;</sub>(x), y) = -log(h<sub>&theta;</sub>(x)), when "y = 1" </p></blockquote><blockquote><p> = -log(1 - h<sub>&theta;</sub>(x)), when "y = 0" </p></blockquote><p>which is exactly the same as our original cost function as defined in the 1st equation.</p><p>Using this new definition, our real cost function J(&theta;) becomes:</p><blockquote><p> J(&theta;) = -(y log(h<sub>&theta;</sub>(x)) + (1 - y) log(1 - h<sub>&theta;</sub>(x))) / m </p></blockquote><p>With that out of the way, let us move onto the next step, viz minimization. For this, we will use our old friend which was introduced in these earlier posts <a href='http://www.golb.in/ml-303-gradient-descent-41.html'>here</a> and <a href='http://www.golb.in/ml-306-multivariate-linear-regression-44.html'>here</a>, viz gradient descent. I hope you remember that the general template for gradient descent is:</p><blockquote><p> repeat until convergence { </p></blockquote><blockquote><p> &theta;<sub>j</sub> = &theta;<sub>j</sub> - &alpha; * &delta;/&delta;&theta;<sub>j</sub> J(&theta;) </p></blockquote><blockquote><p> update all &theta;<sub>j</sub> simultaneously </p></blockquote><blockquote><p> } </p></blockquote><p>Without going into the mathematical derivation, the partial derivative of J(&theta;) w.r.t &theta;<sub>j</sub> is:</p><blockquote><p> &delta;/&delta;&theta;<sub>j</sub> J(&theta;) = &Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>) x<sub>j</sub><sup>(i)</sup> / m </p></blockquote><p>With this information, our gradient descent step becomes</p><blockquote><p> repeat until convergence { </p></blockquote><blockquote><p> &theta;<sub>j</sub> = &theta;<sub>j</sub> - &alpha; * &Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>) x<sub>j</sub><sup>(i)</sup> / m </p></blockquote><blockquote><p> update all &theta;<sub>j</sub> simultaneously </p></blockquote><blockquote><p> } </p></blockquote><p>You might notice that this gradient descent definition is exactly the same as that for linear regression as defined <a href='http://www.golb.in/ml-306-multivariate-linear-regression-44.html'>here</a>. Does this mean that both instances of the gradient descent implementations (for linear and logistic regression) are the same? Close inspection will show that this is not the case. That is because the hypothesis function (h<sub>theta</sub>(x)) for linear and logistic are different. Thus the two gradient descent too are different, although they look the same superficially.</p><p>Once we have the optimized values of theta, it is trivial to substitute them in the definition for the hypothesis function which can be used for prediction of new values of y, given x. However, do remember, that the hypothesis function for logistic regression is actually a probability function (as mentioned in <a href='http://www.golb.in/ml-502-hypothesis-representation-and-decision-boundary-52.html'>this</a> post). Thus</p><blockquote><p> hypothesis function = h<sub>&theta;</sub>(x) = p(y = 1|x; &theta;) </p></blockquote><blockquote><p> = probability that "y = 1" given "x" and parameterized by "&theta;" </p></blockquote><p>Lastly, we had described a method for verifying that the gradient descent implementation is indeed working correctly and is minimizing the cost function in <a href='http://www.golb.in/ml-304-gradient-descent-and-learning-rate-%CE%B1-42.html'>this</a> post. Although that method was used for linear regression, the exact same method can be used for logistic regression too.</p><p>This almost completes the logistic regression part of ML algorithms. The only thing left is to look at cases when y can take a value from a predefined set of values, instead of just 2 values "0" and "1"; that is multi-class classification instead of binary classification. We will look at this last detail in the next post. So stay tuned!</p>
</div>


    <div id="prev-next">
        
        <a href="/posts/2014-02-19-ml-506-multi-class-classification.html">&laquo; ml-506: Multi-class Classification</a>
        
        
        <a class="right" href="/posts/2014-02-17-ml-504-cost-function-for-logistic-regression.html">ml-504: Cost Function and Logistic Regression &raquo;</a>
        
    </div>

    


</div>

            </div>
        </div>

        <div class="col-md-3">
            <div id="sidebar">
                <h3>Links</h3>
                <ul id="links">
                    <li><a href="http://cryogenweb.org/docs/home.html">Cryogen Docs</a></li>
                    <li><a href="http://carmenla.me/blog/index.html">Carmen's Blog</a></li>
                    
                </ul>
                
                <div id="recent">
                    <h3>Recent Posts</h3>
                    <ul>
                        
                        <li><a href="/posts/2014-02-19-ml-506-multi-class-classification.html">ml-506: Multi-class Classification</a></li>
                        
                        <li><a href="/posts/2014-02-18-ml-505-simplified-cost-function-and-gradient-descent-for-logistic-regression.html">ml-505: Simplified Cost Function and Gradient Descent for Logistic Regression</a></li>
                        
                        <li><a href="/posts/2014-02-17-ml-504-cost-function-for-logistic-regression.html">ml-504: Cost Function and Logistic Regression</a></li>
                        
                    </ul>
                </div>
                
                
            </div>
        </div>
    </div>
    <footer>Copyright &copy; 2015 Pradnyesh Sawant
        <p style="text-align: center;">Powered by <a href="http://cryogenweb.org">Cryogen</a></p></footer>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<script src="../js/highlight.pack.js" type="text/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>
