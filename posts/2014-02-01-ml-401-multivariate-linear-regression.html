<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Today is better than yesterday!: ml-401: Multivariate Linear Regression</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='http://fonts.googleapis.com/css?family=Alegreya:400italic,700italic,400,700' rel='stylesheet'
          type='text/css'>
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/default.min.css">
    <link href="../css/screen.css" rel="stylesheet" type="text/css" />
</head>
<body>


<nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/index.html">Today is better than yesterday!</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="/index.html">Home</a></li>
                <li
                ><a href="/archives.html">Archives</a></li>
                
                <li><a href="/feed.xml">RSS</a></li>
            </ul>
        </div><!--/.nav-collapse -->
    </div><!--/.container-fluid -->
</nav>


<div class="container">


    <div class="row">
        <div class="col-lg-9">
            <div id="content">
                
<div id="post">
    <div class="post-header">
    <div id="post-meta" class="row">
        <div class="col-lg-6">1 February, 2014</div>
        
    </div>
    <h2>ml-401: Multivariate Linear Regression</h2>
</div>
<div>
    </ol class="contents">
    <p>Hi! In the last <a href="http://www.golb.in/ml-305-univariate-linear-regression-43.html">post</a>, we completed learning our very first machine learning algorithm, viz "linear regression". As you might remember from <a href="http://www.golb.in/ml-102-supervised-learning-34.html">here</a>, this is a type of "supervised learning" ML algorithm. However, during our study, we had only 1 feature (or input variable), so that what we learnt was really the "univariate linear regression". Today we will take a look at how to handle a more real life situation, that is when we have more than one feature. Such an algorithm is called the "multivariate linear regression" ML algorithm.</p><p>To recap, for univariate linear regression, the steps were:</p><ul><li><b>x</b> =&gt; input variable/feature (in "univariate", we have only 1 feature)</li><li><b>y</b> =&gt; output variable/target (this is the value that we need to predict)</li><li><b>m</b> =&gt; #training examples</li><li><b>h(x)</b> =&gt; hypothesis function, defined as</li></ul><blockquote><p> h<sub>&Theta;</sub>(x) = &Theta;<sub>0</sub> + &Theta;<sub>1</sub> x </p></blockquote><ul><li><b>cost function</b> =&gt; J(&Theta;<sub>0</sub>, &Theta;<sub>1</sub>) = (&Sigma;<sub>i</sub>=1<sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>)<sup>2</sup>)/(2&#42;m)</li><li>minimization of the cost function was done using the "gradient descent" algorithm which was implemented as</li></ul><blockquote><p> repeat until convergence { </p></blockquote><blockquote><p> &Theta;<sub>0</sub> = &Theta;<sub>0</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>)), for j = 0 </p></blockquote><blockquote><p> &Theta;<sub>1</sub> = &Theta;<sub>1</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>) * (x<sup>(i)</sup>)), for j = 1 </p></blockquote><blockquote><p> } </p></blockquote><p>For the "multivariate" version of "linear regression" ML algorithm, we will have multiple features x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, etc. We define one more notation</p><ul><li><b>n</b> =&gt; number of features</li></ul><p>In this case our hypothesis function "h(x)" will change as:</p><blockquote><p> h<sub>&Theta;</sub>(x) = &Theta;<sub>0</sub> + &Theta;<sub>1</sub> x<sub>1</sub> + &Theta;<sub>2</sub> x<sub>2</sub> + &Theta;<sub>3</sub> x<sub>3</sub> + &hellip; + &Theta;<sub>n</sub> x<sub>n</sub> </p></blockquote><p>or</p><blockquote><p> h<sub>&Theta;</sub>(x) = &Theta;<sub>0</sub> + &Sigma;<sub>j=1</sub><sup>n</sup> &Theta;<sub>j</sub> x<sub>j</sub> </p></blockquote><p>It is customary (makes later maths easier) to add an extra feature x<sub>0</sub> whose value is always "1", so that h(x) becomes</p><blockquote><p> h<sub>&Theta;</sub>(x) = &Sigma;<sub>j=0</sub><sup>n</sup> &Theta;<sub>j</sub> x<sub>j</sub> </p></blockquote><p>Our cost function too will change as:</p><blockquote><p> J(&Theta;<sub>0</sub>, &Theta;<sub>1</sub>, &Theta;<sub>2</sub>, &hellip;, &Theta;<sub>n</sub>) = (&Sigma;<sub>i=1</sub><sup>m</sup> &Sigma;<sub>j=1</sub><sup>n</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup><sub>j</sub>) - y<sup>(i)</sup>)<sup>2</sup>)/(2&#42;m) </p></blockquote><p>And so will the gradient descent:</p><blockquote><p> repeat until convergence { </p></blockquote><blockquote><p> &Theta;<sub>0</sub> = &Theta;<sub>0</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>)), for j = 0 </p></blockquote><blockquote><p> &Theta;<sub>1</sub> = &Theta;<sub>1</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup><sub>1</sub>) - y<sup>(i)</sup>) * (x<sup>(i)</sup><sub>1</sub>)), for j = 1 </p></blockquote><blockquote><p> &Theta;<sub>2</sub> = &Theta;<sub>2</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup><sub>2</sub>) - y<sup>(i)</sup>) * (x<sup>(i)</sup><sub>2</sub>)), for j = 2 </p></blockquote><blockquote><p> &hellip; </p></blockquote><blockquote><p> &Theta;<sub>n</sub> = &Theta;<sub>n</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup><sub>n</sub>) - y<sup>(i)</sup>) * (x<sup>(i)</sup><sub>n</sub>)), for j = n </p></blockquote><blockquote><p> } </p></blockquote><p>or, re-written simply:</p><blockquote><p> repeat until convergence { </p></blockquote><blockquote><p> &Theta;<sub>j</sub> = &Theta;<sub>j</sub> - &alpha;&#42;(1/m)&#42; (&Sigma;<sub>i=1</sub><sup>m</sup> (h<sub>&Theta;</sub>(x<sup>(i)</sup><sub>j</sub>) - y<sup>(i)</sup>) * (x<sup>(i)</sup><sub>j</sub>)), for j = 0 to n </p></blockquote><blockquote><p> } </p></blockquote><p>As always, remember that the above updates to all &Theta;<sub>j</sub> (for j = 0 to n) should happen simultaneously for a correct implementation (see <a href="http://www.golb.in/ml-305-univariate-linear-regression-43.html">here</a>)</p><p>Once we compute our hypothesis function using the above implementation, it is then trivial to predict "y" for any given input set of features</p><blockquote><p> y = h(x) = &Sigma;<sub>j=0</sub><sup>n</sup> &Theta;<sub>j</sub> x<sub>j</sub> </p></blockquote><p>That's it for today. In the next post we will learn how to implement these algorithms using "vectorization" for faster performance. So stay tuned!</p>
</div>


    <div id="prev-next">
        
        <a href="/posts/2014-02-02-ml-402-multivariate-linear-regression-in-octave.html">&laquo; ml-402: Multivariate Linear Regression in Octave</a>
        
        
        <a class="right" href="/posts/2014-01-30-ml-305-univariate-linear-regression.html">ml-305: Univariate Linear Regression &raquo;</a>
        
    </div>

    


</div>

            </div>
        </div>

        <div class="col-md-3">
            <div id="sidebar">
                <h3>Links</h3>
                <ul id="links">
                    <li><a href="http://cryogenweb.org/docs/home.html">Cryogen Docs</a></li>
                    <li><a href="http://carmenla.me/blog/index.html">Carmen's Blog</a></li>
                    
                </ul>
                
                <div id="recent">
                    <h3>Recent Posts</h3>
                    <ul>
                        
                        <li><a href="/posts/2014-02-19-ml-506-multi-class-classification.html">ml-506: Multi-class Classification</a></li>
                        
                        <li><a href="/posts/2014-02-18-ml-505-simplified-cost-function-and-gradient-descent-for-logistic-regression.html">ml-505: Simplified Cost Function and Gradient Descent for Logistic Regression</a></li>
                        
                        <li><a href="/posts/2014-02-17-ml-504-cost-function-for-logistic-regression.html">ml-504: Cost Function and Logistic Regression</a></li>
                        
                    </ul>
                </div>
                
                
            </div>
        </div>
    </div>
    <footer>Copyright &copy; 2015 Pradnyesh Sawant
        <p style="text-align: center;">Powered by <a href="http://cryogenweb.org">Cryogen</a></p></footer>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<script src="../js/highlight.pack.js" type="text/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>
