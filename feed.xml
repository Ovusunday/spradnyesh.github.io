<?xml version='1.0' encoding='UTF-8'?>
<rss version='2.0' xmlns:atom='http://www.w3.org/2005/Atom'>
<channel>
<atom:link href='http://spradnyesh.github.io/' rel='self' type='application/rss+xml'/>
<title>
Today is better than yesterday!
</title>
<link>
http://spradnyesh.github.io/
</link>
<description>
This blog is awesome
</description>
<lastBuildDate>
Fri, 08 Jul 2016 12:10:38 +0530
</lastBuildDate>
<generator>
clj-rss
</generator>
<item>
<guid>
http://spradnyesh.github.io/posts/2016-07-08-default-value-for-input-when-using-reagent.html
</guid>
<link>
http://spradnyesh.github.io/posts/2016-07-08-default-value-for-input-when-using-reagent.html
</link>
<title>
Issue w/ default-value using reagent and rendering multiple times
</title>
<description>
&lt;p&gt;currently i am creating a hybrid mobile app using &lt;a href='https://github.com/clojure/clojurescript'&gt;clojurescript&lt;/a&gt; and the amazing &lt;a href='https://reagent-project.github.io/'&gt;reagent&lt;/a&gt; library&lt;/p&gt;&lt;p&gt;during development i faced an issue w/ the &quot;default-value&quot; of an &quot;input&quot; tag. the way to reproduce the issue is as follows:&lt;/p&gt;&lt;p&gt;&lt;code data-gist-id=&quot;e64770bd276ddb99d60b81ac5b0f6ead&quot; data-gist-file=&quot;reagent-default-1.cljs&quot;&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;ul&gt;&lt;li&gt;we have 2 pages &quot;from&quot; and &quot;to&quot; and control cycles between them (they are rendered one after the other in cycles), starting from &quot;from&quot;&lt;/li&gt;&lt;li&gt;&quot;to&quot; has a &quot;form&quot; w/ an &quot;input&quot; (type=text) field that has the &quot;default-value&quot; set to &quot;hi there&quot;&lt;/li&gt;&lt;li&gt;what i noticed is that the 1st time that &quot;to&quot; was rendered, &quot;hi there&quot; was shown in the &quot;input&quot;&lt;/li&gt;&lt;li&gt;but if you&lt;ul&gt;&lt;li&gt;change the value in the input (to say &quot;hello world&quot;)&lt;/li&gt;&lt;li&gt;go to &quot;from&quot; (by clicking &quot;Submit&quot;)&lt;/li&gt;&lt;li&gt;come again to &quot;to&quot; (by clicking &quot;Click Me&quot;)&lt;/li&gt;&lt;li&gt;then the value shown for &quot;input&quot; becomes &quot;hello world&quot;, instead of the expected &quot;hi there&quot;&lt;/li&gt;&lt;li&gt;this was confusing because, i expected &quot;hi there&quot; to be shown since (according to me) &quot;to&quot; was being rendered &quot;freshly&quot; (everytime control moved from &quot;from&quot; to &quot;to&quot;)&lt;/li&gt;&lt;li&gt;the reason for this is that, as mentioned in the &quot;Default Value -&gt; Note&quot; on &lt;a href='https://facebook.github.io/react/docs/forms.html'&gt;react forms docs&lt;/a&gt;, the &quot;defaultValue&quot; works only on the initial render. it also says that &quot;If you need to update the value in a subsequent render, you will need to use a &lt;a href='https://facebook.github.io/react/docs/forms.html#controlled-components'&gt;controlled component&lt;/a&gt;.&quot;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;so we need to maintain the state for the &quot;input&quot; component as follows&lt;/p&gt;&lt;p&gt;&lt;code data-gist-id=&quot;e64770bd276ddb99d60b81ac5b0f6ead&quot; data-gist-file=&quot;reagent-default-2.cljs&quot;&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;fortunately, react has come up w/ a new &lt;a href='https://facebook.github.io/react/blog/2013/07/11/react-v0-4-prop-validation-and-default-values.html'&gt;version v0.4&lt;/a&gt; which says that the behavior discussed above was a common pattern and has been implemented. and we should hopefully see the same supported in the next version of reagent too!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 08 Jul 2016 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2016-06-11-cryogen-on-github.html
</guid>
<link>
http://spradnyesh.github.io/posts/2016-06-11-cryogen-on-github.html
</link>
<title>
cryogen based static site on github
</title>
<description>
&lt;p&gt;&lt;a href='https://pages.github.com/'&gt;Github Pages&lt;/a&gt; are a good way to host your own blog. &lt;a href='http://jekyllrb.com/'&gt;Jekylly&lt;/a&gt; is the de-facto standard of achieving this. but being a &lt;a href='http://clojure.org/'&gt;Clojure&lt;/a&gt; guy, i wanted to find something to achieve the same using Clojure. and find i did =&gt; &lt;a href='http://cryogenweb.org/index.html'&gt;Cryogen&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Cryogen does have a &quot;&lt;a href='http://cryogenweb.org/docs/deploying-to-github-pages.html'&gt;deploying-to-github-pages&lt;/a&gt;&quot; documentation. the Cryogen docs suggest that you push the &quot;public&quot; directory to github gh-pages&lt;/p&gt;&lt;p&gt;however, i'd like to store both the source (.md files, images, etc) and the generated static content (.html files) on github. in order to do this, i have a different setup that i'd like to share with you&lt;/p&gt;&lt;h2&gt;&lt;a name=&quot;one&amp;#95;time&amp;#95;setup&amp;#95;steps&quot;&gt;&lt;/a&gt;one time setup steps&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;create cryogen project as shown &lt;a href='http://cryogenweb.org/docs/getting-started.html'&gt;here&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;lein new cryogen my-blog
cd my-blog
&amp;#126;/my-blog $ lein ring server&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;add pages/posts to ~/my-blog/resources/templates/md/&lt;/li&gt;&lt;li&gt;&quot;lein ring server&quot; will automatically generate static site whenever a new post/page is added/edited&lt;/li&gt;&lt;li&gt;add the &quot;resources/templates/public/&quot; to github account as is suggested in the Cryogen docs &lt;a href='http://cryogenweb.org/docs/deploying-to-github-pages.html'&gt;here&lt;/a&gt;&lt;/li&gt;&lt;li&gt;add the &quot;resources/templates/md/&quot; to another github account as backup of your blog source&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;this approach has the following benefits&lt;/p&gt;&lt;ul&gt;&lt;li&gt;backup blog source (that you can move easily from 1 machine to another)&lt;/li&gt;&lt;li&gt;store generated static files that will be served by github pages&lt;/li&gt;&lt;li&gt;upgrade cryogen without it conflicting w/ existing source&lt;/li&gt;&lt;/ul&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 11 Jun 2016 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2016-06-11-lightdm-passwordless-login.html
</guid>
<link>
http://spradnyesh.github.io/posts/2016-06-11-lightdm-passwordless-login.html
</link>
<title>
passwordless login in lightdm
</title>
<description>
&lt;p&gt;i've mentioned before &lt;a href='/posts/2014-01-17-stumpwm.html'&gt;here&lt;/a&gt;, &lt;a href='/posts/2014-01-16-kill-that-rat.html'&gt;here&lt;/a&gt;, &lt;a href='/posts/2014-01-18-vimperator.html'&gt;here&lt;/a&gt; and &lt;a href='/posts/2014-01-19-vimperator-part-2.html'&gt;here&lt;/a&gt; that i prefer using the keyboard than the mouse for increased productivity. i also use &lt;a href='https://en.wikipedia.org/wiki/LightDM'&gt;lightdm&lt;/a&gt; as my &lt;a href='https://en.wikipedia.org/wiki/X_display_manager_&amp;#40;program_type&amp;#41;'&gt;display manager&lt;/a&gt; from which i invoke &lt;a href='https://en.wikipedia.org/wiki/StumpWM'&gt;stumpwm&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;since mostly i'm the only person using my desktop i like to be auto-logged-in into my system. i don't care much about security because i use &lt;a href='http://colemak.com/'&gt;colemak&lt;/a&gt; as my keyboard layout and hence any person using my system won't be able to use it anyways ;)&lt;/p&gt;&lt;p&gt;it's really very easy to enable passwordless login in lightdm. simply uncomment &quot;autologin-user&quot; and put in your userid there, as below&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; autologin-user=[userid] &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;that's it! although it's super simple, i'm keeping it here for my own future reference O:-)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 11 Jun 2016 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-19-ml-506-multi-class-classification.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-19-ml-506-multi-class-classification.html
</link>
<title>
ml-506: Multi-class Classification
</title>
<description>
&lt;p&gt;Hello and welcome back! Previously, we have discussed the various aspects in much detail in these posts &lt;a href='http://www.golb.in/ml-501-logistic-regression-51.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-502-hypothesis-representation-and-decision-boundary-52.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-503-non-linear-decision-boundary-53.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-504-cost-function-for-logistic-regression-54.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-505-simplified-cost-function-and-gradient-descent-for-logistic-regression-55.html'&gt;here&lt;/a&gt;. Today we will look at the final topic in classification ML algorithms, viz multi-class classification.&lt;/p&gt;&lt;p&gt;Before starting with the actual algorithm, let us first take a look at some examples of multi-classes to ensure that our understanding about them is correct:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;email foldering/tagging: work (y = 1), friends (y = 2), family (y = 3), hobby (y = 4)&lt;/li&gt;&lt;li&gt;medical diagnosis: not ill (y = 1), cold (y = 2), flu (y = 3)&lt;/li&gt;&lt;li&gt;weather: sunny (y = 1), cloudy (y = 2), rain (y = 3), snow (y = 4)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;In all of the above examples, the output variable &quot;y&quot; takes on values from a discrete set (size greater than 2), instead of just &quot;0&quot; and &quot;1&quot; (as in binary classification). This is the problem statement multi-class classification. Note that it does not matter whether the class index starts from 0 or 1.&lt;/p&gt;&lt;p&gt;The technique that is used to solve multi-class classification problems is known as the &quot;one vs all&quot; (or &quot;one vs rest&quot;) algorithm. Let us say that our multi-class classification problem has &quot;k&quot; classes (where, k &amp;gt; 2). Then, in &quot;one vs all&quot; classification technique, we take only one class at a time and combine all the other classes into a single class; thus turning the k-class classification problem into k binary classification problems as shown in below graphs&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;Then we will get k hypothesis functions as below:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;&lt;sup&gt;(i)&lt;/sup&gt;(x) = p(y = i|x; &amp;theta;), where i = 1, 2, &amp;hellip;, k &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The actual prediction (finding y) is done as follows:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; y = i, where class i is one that maximizes h&lt;sub&gt;&amp;theta;&lt;/sub&gt;&lt;sup&gt;(i)&lt;/sup&gt;(x) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This completes our &lt;a href='http://www.golb.in/tag/50x/'&gt;50x&lt;/a&gt; series of ML algorithms which dealt with the topic of classification or logistic regression. I hope you have been following me till now and have enjoyed reading and learning as much as I have writing it and learning from it too :) Do share your thoughts on how I can improve this series further by leaving comments below.&lt;/p&gt;&lt;p&gt;From the next time we will move onto more complex topics and delve much deeper into the realm of ML algorithms. So do come back and enjoy :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 19 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-18-ml-505-simplified-cost-function-and-gradient-descent-for-logistic-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-18-ml-505-simplified-cost-function-and-gradient-descent-for-logistic-regression.html
</link>
<title>
ml-505: Simplified Cost Function and Gradient Descent for Logistic Regression
</title>
<description>
&lt;p&gt;Hello, welcome back! In the last &lt;a href='http://www.golb.in/ml-504-cost-function-for-logistic-regression-54.html'&gt;post&lt;/a&gt; we defined the cost function for our logistic regression implementation. Today we will simplify it, and also move onto the next stage, viz minimization of the cost function.&lt;/p&gt;&lt;p&gt;As a short recap, in the last &lt;a href='http://www.golb.in/ml-504-cost-function-for-logistic-regression-54.html'&gt;post&lt;/a&gt;, we defined the cost function as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), if y = 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; -log(1 = h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), if y = 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The same can be rewritten simply as follows:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -y log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) - (1 - y) log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;To see how this is the same as the previous one, we need to remember that our output variable &quot;y&quot; can take only 1 of 2 possible values, viz &quot;0&quot; and &quot;1&quot;. So let's try to substitute these values in our 2nd equation to see if we can get to the 1st&lt;/p&gt;&lt;p&gt;When &quot;y = 1&quot;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -1 log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) - (1 - 1) log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; = -log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) - 0 * log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; = -log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Similarly, when &quot;y = 0&quot;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -0 * log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) - (1 - 0) log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; = -0 - 1 * log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; = -log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Combining the above 2 new equations, we get&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), when &quot;y = 1&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; = -log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), when &quot;y = 0&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;which is exactly the same as our original cost function as defined in the 1st equation.&lt;/p&gt;&lt;p&gt;Using this new definition, our real cost function J(&amp;theta;) becomes:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;theta;) = -(y log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)) + (1 - y) log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x))) / m &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;With that out of the way, let us move onto the next step, viz minimization. For this, we will use our old friend which was introduced in these earlier posts &lt;a href='http://www.golb.in/ml-303-gradient-descent-41.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-306-multivariate-linear-regression-44.html'&gt;here&lt;/a&gt;, viz gradient descent. I hope you remember that the general template for gradient descent is:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha; * &amp;delta;/&amp;delta;&amp;theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;theta;) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; update all &amp;theta;&lt;sub&gt;j&lt;/sub&gt; simultaneously &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Without going into the mathematical derivation, the partial derivative of J(&amp;theta;) w.r.t &amp;theta;&lt;sub&gt;j&lt;/sub&gt; is:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &amp;delta;/&amp;delta;&amp;theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;theta;) = &amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) x&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(i)&lt;/sup&gt; / m &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;With this information, our gradient descent step becomes&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha; * &amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) x&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(i)&lt;/sup&gt; / m &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; update all &amp;theta;&lt;sub&gt;j&lt;/sub&gt; simultaneously &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;You might notice that this gradient descent definition is exactly the same as that for linear regression as defined &lt;a href='http://www.golb.in/ml-306-multivariate-linear-regression-44.html'&gt;here&lt;/a&gt;. Does this mean that both instances of the gradient descent implementations (for linear and logistic regression) are the same? Close inspection will show that this is not the case. That is because the hypothesis function (h&lt;sub&gt;theta&lt;/sub&gt;(x)) for linear and logistic are different. Thus the two gradient descent too are different, although they look the same superficially.&lt;/p&gt;&lt;p&gt;Once we have the optimized values of theta, it is trivial to substitute them in the definition for the hypothesis function which can be used for prediction of new values of y, given x. However, do remember, that the hypothesis function for logistic regression is actually a probability function (as mentioned in &lt;a href='http://www.golb.in/ml-502-hypothesis-representation-and-decision-boundary-52.html'&gt;this&lt;/a&gt; post). Thus&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; hypothesis function = h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = p(y = 1|x; &amp;theta;) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; = probability that &quot;y = 1&quot; given &quot;x&quot; and parameterized by &quot;&amp;theta;&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Lastly, we had described a method for verifying that the gradient descent implementation is indeed working correctly and is minimizing the cost function in &lt;a href='http://www.golb.in/ml-304-gradient-descent-and-learning-rate-%CE%B1-42.html'&gt;this&lt;/a&gt; post. Although that method was used for linear regression, the exact same method can be used for logistic regression too.&lt;/p&gt;&lt;p&gt;This almost completes the logistic regression part of ML algorithms. The only thing left is to look at cases when y can take a value from a predefined set of values, instead of just 2 values &quot;0&quot; and &quot;1&quot;; that is multi-class classification instead of binary classification. We will look at this last detail in the next post. So stay tuned!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 18 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-17-ml-504-cost-function-for-logistic-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-17-ml-504-cost-function-for-logistic-regression.html
</link>
<title>
ml-504: Cost Function and Logistic Regression
</title>
<description>
&lt;p&gt;Hello. In the previous few posts (&lt;a href='http://www.golb.in/ml-501-logistic-regression-51.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-502-hypothesis-representation-and-decision-boundary-52.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-503-non-linear-decision-boundary-53.html'&gt;here&lt;/a&gt; we learned about linear regression and also formulated its hypothesis function using the sigmoid function so that the output variable &quot;y&quot; always is either 0 or 1. Today we will move onto the next stage and learn about the cost function that shall be used in the logistic regression ML problem.&lt;/p&gt;&lt;p&gt;In &lt;a href='http://www.golb.in/ml-502-hypothesis-representation-and-decision-boundary-52.html'&gt;post&lt;/a&gt; we defined our hypothesis function as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = g(&amp;theta;' &lt;em&gt; x) = 1 / (1 + e&lt;sup&gt;-(&amp;theta;' &lt;/em&gt; x)&lt;/sup&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Also, in &lt;a href='http://www.golb.in/ml-301-linear-regression-with-one-variable-38.html'&gt;post&lt;/a&gt;, we had defined the cost function for linear regression as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;theta;) = (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Let's re-write it as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;theta;) = (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; ((h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;) / 2) / m &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;theta;) = cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;), y&lt;sup&gt;(i)&lt;/sup&gt;) / m &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;that is&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;), y&lt;sup&gt;(i)&lt;/sup&gt;) = (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; ((h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;) / 2) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;But this cost-function is non-convex :( (unlike that of linear regression (see &lt;a href='http://www.golb.in/ml-302-intuition-for-univariate-linear-regression-39.html'&gt;post&lt;/a&gt;)). This is because the squared cost of sigmoid function is non-linear (remember that our hypothesis function is the sigmoid of (&amp;theta;' * x)). Due to this finding the optimal (global) minimum is (mostly, unless by luck) impossible. Hence we need to define a new cost-function for our logistic regression problem.&lt;/p&gt;&lt;p&gt;Let us define the new cost-function as follows:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), if &quot;y = 1&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; -log(1 = h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), if &quot;y = 0&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Now we need to prove that the above cost-function is convex. To do this let us first consider the case when &quot;y = 1&quot;&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;The above graph is a plot for the function &quot;-log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x))&quot;. As can bee seen from this graph, our newly defined cost function has interesting and desirable properties:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;for true-positive (hypothesis predicts y = 1, and really y = 1), then cost = 0&lt;/li&gt;&lt;li&gt;but false-negative (hypothesis predicts y = 0, but really y = 1), then cost grows to infinity&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Now see what happens when &quot;y = 0&quot;&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;The above graph is a plot for the function &quot;-log(1 - h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x))&quot;. Similar to the previous graph, we have some interesting and desirable properties here too:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;for true-negative (hypothesis predicts y = 0, and really y = 0), then cost = 0&lt;/li&gt;&lt;li&gt;but false-positive (hypothesis predicts y = 1, but really y = 0), then cost grows to infinity&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These properties are good because when our implementation is working as expected the cost (error) is 0, but when it is not, then it gets penalized heavily. When, in later stages, we apply minimization algorithms to this cost function, we are sure it will work in the right direction and produce the correct values of &amp;theta;. From there, we will follow the standards steps of defining the hypothesis function which will be used to predict &quot;y&quot; given &quot;x&quot;.&lt;/p&gt;&lt;p&gt;Coming back to the point, our real cost function &quot;J(&amp;theta;)&quot; will now be defined as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;theta;) = cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;), y&lt;sup&gt;(i)&lt;/sup&gt;) / m &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;where&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; cost(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x), y) = -log(h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), if &quot;y = 1&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; -log(1 = h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x)), if &quot;y = 0&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Keep watching this space until next time when we will move onto the next stage of the logistic regression ML algorithm and learn about minimization techniques for our new cost function!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 17 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-10-ml-503-non-linear-decision-boundary.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-10-ml-503-non-linear-decision-boundary.html
</link>
<title>
ml-503: Non Linear Decision Boundary
</title>
<description>
&lt;p&gt;Hi there! Welcome back to this series of posts on ML algorithms. In the last &lt;a href='http://www.golb.in/ml-502-hypothesis-representation-and-decision-boundary-52.html'&gt;post&lt;/a&gt; we looked at the hypothesis representation and decision boundary concepts in the logistic regression ML algorithm. Today we will look at some more examples of decision boundaries and get a deeper understanding regarding them.&lt;/p&gt;&lt;p&gt;But, let's do a quick review of what we learned last time so that we can continue from there. We defined our hypothesis function as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = g(&amp;theta;' * x) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;where,&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; g(z) = 1 / (1 + e&lt;sup&gt;-z&lt;/sup&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;is the sigmoid function. We also saw an important result that:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 1&quot; if (&amp;theta;' &amp;#42;x) &amp;gt;= 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 0&quot; if (&amp;theta;' &amp;#42;x) &amp;lt; 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The decision boundary we saw last time was a simple straight line. But what if our dataset is like below?&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;If you remember what we discussed in one of the previous &lt;a href='http://www.golb.in/ml-404-feature-choice-and-polynomial-regression-48.html'&gt;posts&lt;/a&gt;, then the solution is simple. We should use higher order polynomials. Let us define our hypothesis function as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;theta;&lt;sub&gt;1&lt;/sub&gt; x1 + &amp;theta;&lt;sub&gt;2&lt;/sub&gt; x2 + &amp;theta;&lt;sub&gt;3&lt;/sub&gt; x1&lt;sup&gt;2&lt;/sup&gt; + &amp;theta;&lt;sub&gt;4&lt;/sub&gt; x2&lt;sup&gt;2&lt;/sup&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;And, furthermore, let us say that we have somehow found the correct values for &amp;theta; already as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &amp;theta; = [-1 0 0 1 1]' &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;I know that we have not yet looked at how to find the optimal values of &amp;theta; for logistic regression. But fear not, we will get there.&lt;/p&gt;&lt;p&gt;Coming back to the problem at hand. When we use the above &amp;theta;, we get the following hypothesis function output:&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;You see that the decision boundary is now a circle, and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 1&quot; if x1&lt;sup&gt;2&lt;/sup&gt; + x2&lt;sup&gt;2&lt;/sup&gt; &amp;gt;= 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 0&quot; if x1&lt;sup&gt;2&lt;/sup&gt; + x2&lt;sup&gt;2&lt;/sup&gt; &amp;lt; 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Similarly, if we take more complex higher order polynomials like,&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;theta;&lt;sub&gt;1&lt;/sub&gt; x1 + &amp;theta;&lt;sub&gt;2&lt;/sub&gt; x2 + &amp;theta;&lt;sub&gt;3&lt;/sub&gt; x1&lt;sup&gt;2&lt;/sup&gt; + &amp;theta;&lt;sub&gt;4&lt;/sub&gt; x2&lt;sup&gt;2&lt;/sup&gt; + &amp;theta;&lt;sub&gt;5&lt;/sub&gt; x1x2 + &amp;theta;&lt;sub&gt;5&lt;/sub&gt; x1&lt;sup&gt;2&lt;/sup&gt; + &amp;hellip; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;then we can get even more complex forms of the decision boundary which will help us correctly classify our dataset.&lt;/p&gt;&lt;p&gt;I hope you are with me till here, and are loving reading about these algorithms as much as I am loving writing about them. In the next post we will take a look at the methods to compute the optimal values of &amp;theta;. So keep watching this space!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 10 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-09-ml-502-hypothesis-representation-and-decision-boundary.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-09-ml-502-hypothesis-representation-and-decision-boundary.html
</link>
<title>
ml-502: Hypothesis Representation and Decision Boundary
</title>
<description>
&lt;p&gt;&lt;p&gt;Hello! In the last &lt;a href='http://www.golb.in/ml-501-logistic-regression-51.html'&gt;post&lt;/a&gt; we took an initial swing at a new type of ML problem, viz classification. We also looked at how the ML linear regression algorithm fails to work for this type of problem, and saw that there is another ML algorithm, logistic regression, that is used to solve them. Today we will do a deeper dive into various concepts surrounding the logistic regression ML algorithm.&lt;/p&gt;&lt;p&gt;We saw in the last &lt;a href='http://www.golb.in/ml-501-logistic-regression-51.html'&gt;post&lt;/a&gt; that we want:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; 0 &amp;lt;= h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) &amp;lt;= 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In linear regression, we know that:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;theta;' * x &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In logistic regression, we will define:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = g(&amp;theta;' * x) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;where,&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; g(z) = 1 / (1 + e&lt;sup&gt;-z&lt;/sup&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;g(z) is called the logistic or sigmoid function (hence the name &quot;logistic&quot; regression for this algorithm). The graph (z v/s g(z)) of the sigmoid function looks like:&lt;/p&gt;&lt;p&gt;![]() &lt;/p&gt;&lt;p&gt;As you might notice, the sigmoid function has this interesting that it asymptotes at 0 as g(z) nears -infinity, and asymptotes at 1 as g(z) nears +infinity. This property makes this function extremely useful in our logistic regression implementation. Also, notice in the above graph that&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; g(z) &amp;gt;= 0.5 when z &amp;gt;= 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; g(z) &amp;lt; 0.5 when z &amp;lt; 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;that is&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = g(&amp;theta;' &lt;em&gt; x) &amp;gt;= 0.5, whenever (&amp;theta;' &lt;/em&gt; x) &amp;gt;= 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = g(&amp;theta;' &lt;em&gt; x) &amp;lt; 0.5, whenever (&amp;theta;' &lt;/em&gt; x) &amp;lt; 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;We would define our threshold such that&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; y = 1 if h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) &amp;gt;= 0.5 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; y = 0 if h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) &amp;lt; 0.5 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Combining the above 2 we get&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 1&quot; if (&amp;theta;' &amp;#42;x) &amp;gt;= 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 0&quot; if (&amp;theta;' &amp;#42;x) &amp;lt; 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Remember this condition, because it will be used when we try to understand the next important concept of &quot;decision boundary&quot;.&lt;/p&gt;&lt;p&gt;Anyways, coming back to our hypothesis function, it becomes:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = 1 / (1 + e&lt;sup&gt;-(&amp;theta;' * x)&lt;/sup&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This might seem confusing to understand, so let's try to put it in simpler terms. The meaning of the output of hypothesis function is &quot;estimated probability that 'y = 1' on input 'x'&quot;. For example, if&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; x = [x&lt;sub&gt;0&lt;/sub&gt; x&lt;sub&gt;1]'&lt;/sub&gt; = [1 tumorSize]' &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and if, for this x&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = 0.7 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;then, it means that the probability that the tumor is malignant is 70%. Mathematically, it is written as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = P(y = 1 | x; &amp;theta;) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and is read is &quot;probability that 'y = 1', given 'x', parameterized by '&amp;theta;'&quot;. Conversely, probability for 'y = 0' is&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; P(y = 0 | x; &amp;theta;) = 1 - P(y = 1 | x; &amp;theta;) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Next, let's look at another important concept called the &quot;decision boundary&quot;. To explain this let us consider the following dataset:&lt;/p&gt;&lt;p&gt;![]() &lt;/p&gt;&lt;p&gt;Let us also define our hypothesis function as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;theta;&lt;sub&gt;1&lt;/sub&gt; x&lt;sub&gt;1&lt;/sub&gt;, &amp;theta;&lt;sub&gt;2&lt;/sub&gt; x&lt;sub&gt;2&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and assume that we have somehow found the values of &amp;theta; already as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &amp;theta; = [-4 1 1]' &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;So our hypothesis function becomes (based on our previously noted equation above) to predict:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 1&quot; if -4 + x1 + x2 &amp;gt;= 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 1&quot; if x1 + x2 &amp;gt;= 4 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Similarly,&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &quot;y = 0&quot; if x1 + x2 &amp;lt; 4 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;![]() &lt;/p&gt;&lt;p&gt;The line in the above graph that separates the 2 binary classes &quot;y = 1&quot; and &quot;y = 0&quot; is called the decision boundary. It is extremely important to note that the decision boundary is the property of the hypothesis function and is parameterized by &amp;theta;, and is not dependent on the dataset.&lt;/p&gt;&lt;p&gt;I think it has been a bit heavy today, so let's stop here. In the next post, we will take a look at some other, more interesting, decision boundaries and understand more about them. So stay tuned!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 09 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-08-ml-501-logistic-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-08-ml-501-logistic-regression.html
</link>
<title>
ml-501: Logistic Regression
</title>
<description>
&lt;p&gt;Hello :) I have a good news today! If you have not noticed, we have moved from the 40x series into the 50x series of ML posts today. Yay! This means that we will start learning a new section, or type of ML algorithms. But wait, isn't that logistic &quot;regression&quot; there in the title? Haven't we been learning regression algorithms already in the 40x series &lt;a href='http://www.golb.in/ml-306-multivariate-linear-regression-44.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-403-feature-scaling-47.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-404-feature-choice-and-polynomial-regression-48.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-405-normal-equation-part-1-49.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-406-normal-equation-part-2-50.html'&gt;here&lt;/a&gt;. The reason this is so is because we will look at a different type of ML problem, viz &quot;classification&quot; in this 50x series, but for historical reasons these algorithms are called logistic regression. Note the &quot;logistic&quot; part, which means &quot;logical&quot;, or &quot;classes&quot;. Ready to start? Here we go!&lt;/p&gt;&lt;p&gt;Today we will be learning the &quot;classification&quot; type of ML problem, where the output variable &quot;y&quot; is a set of &quot;discrete&quot; values. If you remember, we had given a brief introduction to these problems in this 10x introductory &lt;a href='http://www.golb.in/ml-102-supervised-learning-34.html'&gt;post&lt;/a&gt;. But now, let's take a deeper look using some examples:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;email spam: In the email spam classification, we have a spam classifier ML implementation which, given a text of email, classifies it as either &quot;yes&quot; (spam), or &quot;no&quot; (not-spam)&lt;/li&gt;&lt;li&gt;online transactions: In this case, the problem statement is to classify a particular online transaction as either &quot;fraudulent&quot; or &quot;not-fraudulent&quot;&lt;/li&gt;&lt;li&gt;tumor: In this problem, the ML implementation should tell whether a given tumor is &quot;benign&quot; (not-harmful), or malignant (harmful)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In all of the above cases, the output variable &quot;y&quot; can take one of 2 values, &quot;yes&quot; or &quot;no&quot;. Traditionally, this has been represented as &quot;class-1&quot; (presence of something), or &quot;class-0&quot; (absence of something), but this assignment is mostly arbitrary. Also, in the above cases, there are only 2 values, and hence the classification is also known as a binary-class classification; whereas when there are more than 2 values, then the algorithm is known as a multi-class classification.&lt;/p&gt;&lt;p&gt;Since this is the first post in the &quot;classification&quot; domain, we will keep it simple and look at only the binary-class classification type of problems today. So let's move onto the problem statement for today's example.&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;In this example:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;X: tumor size&lt;/li&gt;&lt;li&gt;y: 1 (malignant), 0 (benign)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That is, given just the tumor size, we need to predict whether the tumor is malignant or benign. The sample data looks like shown above. Notice that all data points are either at &quot;y = 0&quot;, or &quot;y = 1&quot;, signifying that this is a classification problem.&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;Let's say that, since we have not learnt the logistic regression ML algorithm yet, we will apply the linear regression ML algorithm, that we already know, and see how it performs. After running linear regression, we get our hypothesis line as shown in the above figure. In this case, if we define a threshold as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; if h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) &amp;gt;= 0.5, predict &quot;y = 1&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; if h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) &amp;lt; 0.5, predict &quot;y = 0&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;then it looks like our linear regression implementation has done a pretty good job of classifying the benign and malignant examples correctly, and is in fact working as a logistic regression algorithm.&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;However, if we get another data point, an outlier actually, then we get a different hypothesis function as shown in the above figure. But this time, we see that our new hypothesis function performs terribly on the data, and is in fact mis-classifying the malignant examples as benign. What really happened is that the first time, our linear regression implementation turned out to be lucky. In general, the linear regression does not work well for classification problems.&lt;/p&gt;&lt;p&gt;In the next post we will look at the linear regression algorithm itself and how it is used to solve a classification ML problem. So stay tuned!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 08 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-07-ml-406-normal-equation-part-2.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-07-ml-406-normal-equation-part-2.html
</link>
<title>
ml-406: Normal Equation -- part - 2
</title>
<description>
&lt;p&gt;Hello there! Welcome back. In the last &lt;a href='http://www.golb.in/ml-405-normal-equation-part-1-49.html'&gt;post&lt;/a&gt; we took a look at another algorithm, viz &quot;normal equation&quot; to solve the linear regression ML problem. However that was, maybe, too theoretical in nature. Today we will take a concrete example and see how to implement the solution for the same in octave. Are you ready? If so, lets move on&amp;hellip;&lt;/p&gt;&lt;p&gt;The concrete example for today is as follows (including the pivot feature x&lt;sub&gt;0&lt;/sub&gt;):&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;size(feet&lt;sup&gt;2&lt;/sup&gt;)&lt;/td&gt;&lt;td&gt;#bedrooms&lt;/td&gt;&lt;td&gt;#floors&lt;/td&gt;&lt;td&gt;age (years)&lt;/td&gt;&lt;td&gt;price($1000)&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;x&lt;sub&gt;0&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;x&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;x&lt;sub&gt;4&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2104&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;45&lt;/td&gt;&lt;td&gt;460&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1416&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;232&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1534&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;30&lt;/td&gt;&lt;td&gt;315&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;852&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;178&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;In matrix forms, it gives rise to the following X (of size mx(n+1)):&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2104&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;45&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1416&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1534&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;30&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;852&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;36&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;and y (of size mx1):&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;460&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;232&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;315&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;178&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;If that is the case, then by implementing matrix algebra, theta can be solved as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; theta = (X&lt;sup&gt;T&lt;/sup&gt; &lt;em&gt; X)&lt;sup&gt;-1&lt;/sup&gt; &lt;/em&gt; X&lt;sup&gt;T&lt;/sup&gt; * y; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In octave, the command to solve the above is&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; theta = pinv(X' &lt;em&gt; X) &lt;/em&gt; X' * y; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;That's it! Ain't it cool how such a complex concept can be expressed in just 1 line-of-code in a high level language like octave? Hence, as I had mentioned &lt;a href='http://www.golb.in/ml-201-octave-1-36.html'&gt;here&lt;/a&gt;, it is extremely important (and useful) to first write solutions for any ML problem in a high level language (like octave, matlab, python, etc) and only when we are sure that we have the right solution, to port them to other faster languages if need be.&lt;/p&gt;&lt;p&gt;Coming back to our problem at hand. Aren't you wondering that if it is so simple to solve for the values of theta, then why did we learn the &quot;gradient descent&quot; algorithm in the first place? Well, obviously, because there are scenarios where the normal equation method does not work. To understand this, lets compare the 2 methods, viz gradient descent and normal equation, and see their relative advantages and disadvantages.&lt;/p&gt;&lt;p&gt;Lets look at the advantages of normal equation method over gradient descent:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;there is no need to do feature scaling&lt;/li&gt;&lt;li&gt;there is no need to choose the learning rate &quot;&amp;alpha;&quot;&lt;/li&gt;&lt;li&gt;there is no need iterate&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Now lets look at scenarios where normal equation method does not work:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;when the number of features (n) is very large, then the cost to compute the inverse of the nxn matrix (X&lt;sup&gt;T&lt;/sup&gt; * X), is extremely large (order of O(n&lt;sup&gt;3&lt;/sup&gt;)). On modern computers, for n &lt; 10000, the implementation will work (speed will decrease as n increases), but for values of n above 10000, the processing might never complete.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Although, at this stage of our study, n of the order of 10000 looks extremely big, do note that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;in real world problems, it is possible to have these many (and more) features&lt;/li&gt;&lt;li&gt;when you consider the &quot;polynomial regression&quot; algorithm from &lt;a href='http://www.golb.in/ml-404-feature-choice-and-polynomial-regression-48.html'&gt;this&lt;/a&gt; post, one can easily see how n can grow very fast&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Finally, there is 1 more special case when the normal equation algorithm might get stuck. This is when the matrix (X&lt;sup&gt;T&lt;/sup&gt; * X) is non-invertible, that it does not have an inverse. This should happen in only the following 2 cases:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;some features x&lt;sub&gt;i&lt;/sub&gt; and x&lt;sub&gt;j&lt;/sub&gt; are linearly dependent on each other (for example, x&lt;sub&gt;1&lt;/sub&gt; = size in feet, and x&lt;sub&gt;2&lt;/sub&gt; = size in meters). in such a case we need to find out such features and remove one of them&lt;/li&gt;&lt;li&gt;&amp;#35;training-examples (m) &lt;= #features (n). that is, there is not enough data to fit all features and find the optimal theta. in this case we need to&lt;/li&gt;&lt;li&gt;either reduce features&lt;/li&gt;&lt;li&gt;or add more data&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both the above situations are easily avoidable, which leaves us with the only limitation of &quot;n &lt; 10000&quot;, thus making the normal equation algorithm a very attractive choice while solving linear regression ML problems.&lt;/p&gt;&lt;p&gt;This also brings us to the end of the 40x series. In the next post we will start looking at other ML problems and algorithms to solve them. So don't go away. Keep watching this space :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 07 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-06-ml-405-normal-equation-part-1.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-06-ml-405-normal-equation-part-1.html
</link>
<title>
ml-405: Normal Equation -- part - 1
</title>
<description>
&lt;p&gt;Hello! Today we will learn an amazing ML algorithm that allows us to find the hypothesis function for a regression problem in just 1 step. Sounds exciting? So lets move forward with full force!&lt;/p&gt;&lt;/p&gt;&lt;p&gt;As we already know from &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;here&lt;/a&gt;, the steps to solve a regression problem are:&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;hypothesis function&lt;/b&gt; is defined as:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Sigma;&lt;sub&gt;j&lt;/sub&gt;=0&lt;sup&gt;n&lt;/sup&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; x&lt;sub&gt;j&lt;/sub&gt;&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;cost function&lt;/b&gt; is defined as:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, &amp;Theta;&lt;sub&gt;n&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;=1&lt;sup&gt;m&lt;/sup&gt; &amp;Sigma;&lt;sub&gt;j&lt;/sub&gt;=1&lt;sup&gt;n&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m)&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;While the &lt;b&gt;gradient descent&lt;/b&gt; steps are:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;=1&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;)), for j = 0 to n &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; }&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Notice that the last step, viz the &quot;gradient descent&quot; is a loop, that is it takes an &quot;iterative&quot; approach to minimize the cost function, and thus find the optimal values for &amp;Theta; (that is &amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;, &amp;Theta;&lt;sub&gt;n&lt;/sub&gt;).&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Wouldn't it be great if we could skip this whole iteration step, and find the optimal values for &amp;Theta; directly? Well, it is certainly possible; and that is exactly what we will learn today. This method of mathematically finding the optimal values of &amp;Theta; directly (in just 1 step) is known as the &quot;normal equation&quot; method. Let us try to understand how it works by first solving a simple problem and then going on to the more general but difficult problem definition. So for now, let us assume that &amp;Theta; is a scalar (real number), instead of a vector (&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;, &amp;Theta;&lt;sub&gt;n&lt;/sub&gt;). So our cost function is defined as:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; j(&amp;Theta;) = a &lt;em&gt; &amp;Theta;&lt;sup&gt;2&lt;/sup&gt; + b &lt;/em&gt; &amp;Theta; + c;&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The mathematical way to find the optimal value of &amp;Theta; that minimizes the above cost function is to find the derivative of the cost function and set it to 0 as follows&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; 2 &lt;em&gt; a &lt;/em&gt; &amp;Theta; + b = 0;&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This is an equation with a single variable (&amp;Theta;) and can be easily solved. The resulting value of &amp;Theta; is the optimal value that we are looking for! Ain't that cool?&lt;/p&gt;&lt;/p&gt;&lt;p&gt;To generalize, lets look at our original cost function again:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, &amp;Theta;&lt;sub&gt;n&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;=1&lt;sup&gt;m&lt;/sup&gt; &amp;Sigma;&lt;sub&gt;j&lt;/sub&gt;=1&lt;sup&gt;n&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m)&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;According to calculus theory, if we take the &quot;partial derivatives&quot; of the above cost function with respect to each of the &amp;Theta;s (&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;., &amp;Theta;&lt;sub&gt;n&lt;/sub&gt;), set them to 0 (zero) and solve, we will get the optimal values for each of our &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; (j = 0, 1, &amp;hellip;, n)! Now these values can be easily substituted in the hypothesis function and used to predict values for &quot;y&quot;.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Ain't this cool? If it is, keep watching this space for the next post where we will look at the implementation of this method in octave, and also look at what advantages and disadvantages does this method have when compared with the gradient descent algorithm.&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 06 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-05-ml-404-feature-choice-and-polynomial-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-05-ml-404-feature-choice-and-polynomial-regression.html
</link>
<title>
ml-404: Feature Choice and Polynomial Regression
</title>
<description>
&lt;p&gt;Hi. It's good to see you once more. Today we will learn a particular technique to choose features that might improve the performance (in terms of prediction accuracy) of the regression algorithm, and in turn look at a subset type of regression that results from it.&lt;/p&gt;&lt;p&gt;In one of the initial &lt;a href='http://www.golb.in/ml-301-linear-regression-with-one-variable-38.html'&gt;posts&lt;/a&gt;&amp;nbsp;when we started learning the linear regression ML algorithm, we had used the following data&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; size in ft&lt;sup&gt;2&lt;/sup&gt; (x)&lt;/td&gt; &lt;td&gt; price ($) in 1000's (y)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 2104&lt;/td&gt; &lt;td&gt; 460&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 1416&lt;/td&gt; &lt;td&gt; 232&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 1534&lt;/td&gt; &lt;td&gt; 315&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 852&lt;/td&gt; &lt;td&gt; 178&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &amp;hellip;&lt;/td&gt; &lt;td&gt; &amp;hellip;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;and called our problem as the &quot;housing prediction&quot; problem. This particular problem is the &quot;univariate&quot; type of linear regression because we have only 1 input feature, viz the size of the house in sq-ft.&lt;/p&gt;&lt;p&gt;However, it is very easy to assume that we might have got our input features in the form of &quot;length&quot; and &quot;depth&quot;, that is as x&lt;sub&gt;1&lt;/sub&gt; and x&lt;sub&gt;2&lt;/sub&gt; instead. But, maybe, we had some background domain knowledge to know that the price of the house is &lt;i&gt;more&lt;/i&gt; dependent on the area (length * depth), instead of the individual features of length and depth themselves. So in this case, it's like we created a new feature x&lt;sub&gt;3&lt;/sub&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; x&lt;sub&gt;3&lt;/sub&gt; = x&lt;sub&gt;1&lt;/sub&gt; * x&lt;sub&gt;2&lt;/sub&gt;; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and used it in place of x&lt;sub&gt;1&lt;/sub&gt; and x&lt;sub&gt;2&lt;/sub&gt;. Maybe, we also knew, due to domain knowledge, that given the area (x&lt;sub&gt;3&lt;/sub&gt;), the features of length (x&lt;sub&gt;1&lt;/sub&gt;) and depth (x&lt;sub&gt;2&lt;/sub&gt;) do not play any significant role in the prediction of the price (y), and hence can be dropped; thus making y dependent only on x&lt;sub&gt;3&lt;/sub&gt;.&lt;/p&gt;&lt;p&gt;The point to note here is that x&lt;sub&gt;3&lt;/sub&gt; is no longer a simple variable/feature, but in fact is the product of 2 (x&lt;sub&gt;1&lt;/sub&gt; and x&lt;sub&gt;2&lt;/sub&gt;) simpler features. Thus, while the graph of x&lt;sub&gt;1&lt;/sub&gt; versus y might have been linear, the graph of x&lt;sub&gt;3&lt;/sub&gt; versus y would appear to have a quadratic shape (having some sort of a curved nature).&lt;/p&gt;&lt;p&gt;Similar effect (in terms of the graph plot) would appear if we would have used a polynomial power of the base variable itself. For example, if&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; y = x&lt;sub&gt;1&lt;/sub&gt; ^ 2; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; y = x&lt;sub&gt;2&lt;/sub&gt; ^ 3; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;etc, would give the curve of the input versus output variable a different shape.&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;Looking at the plots of your input versus output variable, you might get an idea a polynomial feature might be needed. In such cases, armed with some domain knowledge, if one can play with choosing the power of the input feature variables, it is possible to radically improve the performance (in terms of prediction accuracy, not speed) of our implementation.&lt;/p&gt;&lt;p&gt;Since, in these cases, we use a polynomial power of our input variable(s), these algorithms are also known as &quot;polynomial regression&quot; type of ML algorithms. Of course, if one has domain knowledge and already knows some relationship (although not exactly) between the input and output variables, then it can be extremely helpful.&lt;/p&gt;&lt;p&gt;In future posts we will take a look at some advanced algorithms which can automatically find out such relationships, and other algorithms which can automatically choose only those features which improve the performance of the implementation significantly and drop the others. So keep watching this space to learn about them :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 05 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-04-ml-403-feature-scaling.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-04-ml-403-feature-scaling.html
</link>
<title>
ml-403: Feature Scaling
</title>
<description>
&lt;p&gt;Hello. Welcome back! I hope that you are enjoying this series on machine learning, especially the last 2 posts &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-203-vectorization-in-octave-46.html'&gt;here&lt;/a&gt; where we implemented the &quot;multivariate linear regression&quot; ML algorithm and even improved its performance. In this post we will look at one more technique which should be applied to the multivariate version of the linear regression ML algorithm to improve its performance, viz &quot;feature scaling&quot;. The reason why I say that this technique should be used for the &quot;multivariate&quot; version, is because by using this technique we solve a certain problem from which the &quot;univariate&quot; version does not suffer.&lt;/p&gt;&lt;p&gt;Lets take a concrete example to throw more light on what I am trying to say. For this, let's consider this &lt;a href='http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'&gt;data&lt;/a&gt;, a small snippet of which is shown below:&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 307.0&lt;/td&gt; &lt;td&gt; 130.0&lt;/td&gt; &lt;td&gt; 3504.&lt;/td&gt; &lt;td&gt; 12.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;chevrolet chevelle malibu&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 350.0&lt;/td&gt; &lt;td&gt; 165.0&lt;/td&gt; &lt;td&gt; 3693.&lt;/td&gt; &lt;td&gt; 11.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;buick skylark 320&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 318.0&lt;/td&gt; &lt;td&gt; 150.0&lt;/td&gt; &lt;td&gt; 3436.&lt;/td&gt; &lt;td&gt; 11.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;plymouth satellite&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 16.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 304.0&lt;/td&gt; &lt;td&gt; 150.0&lt;/td&gt; &lt;td&gt; 3433.&lt;/td&gt; &lt;td&gt; 12.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;amc rebel sst&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 17.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 302.0&lt;/td&gt; &lt;td&gt; 140.0&lt;/td&gt; &lt;td&gt; 3449.&lt;/td&gt; &lt;td&gt; 10.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;ford torino&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 429.0&lt;/td&gt; &lt;td&gt; 198.0&lt;/td&gt; &lt;td&gt; 4341.&lt;/td&gt; &lt;td&gt; 10.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;ford galaxie 500&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 454.0&lt;/td&gt; &lt;td&gt; 220.0&lt;/td&gt; &lt;td&gt; 4354.&lt;/td&gt; &lt;td&gt; 9.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;chevrolet impala&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 440.0&lt;/td&gt; &lt;td&gt; 215.0&lt;/td&gt; &lt;td&gt; 4312.&lt;/td&gt; &lt;td&gt; 8.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;plymouth fury iii&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 455.0&lt;/td&gt; &lt;td&gt; 225.0&lt;/td&gt; &lt;td&gt; 4425.&lt;/td&gt; &lt;td&gt; 10.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;pontiac catalina&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 390.0&lt;/td&gt; &lt;td&gt; 190.0&lt;/td&gt; &lt;td&gt; 3850.&lt;/td&gt; &lt;td&gt; 8.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;amc ambassador dpl&quot;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;If you remember, we have seen this dataset earlier &lt;a href='http://www.golb.in/ml-301-linear-regression-with-one-variable-38.html'&gt;here&lt;/a&gt;. Let's ignore the first column for now, because that is the target variable, so that leaves us with 8 features. We notice that (in our limited dataset snippet):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;1st feature ranges between 4 and 8 (in the complete dataset)&lt;/li&gt;&lt;li&gt;2nd feature ranges between 300 and 500 (approx)&lt;/li&gt;&lt;li&gt;3rd feature ranges between 100 and 250 (approx)&lt;/li&gt;&lt;li&gt;4th feature ranges between 3400 and 4500 (approx)&lt;/li&gt;&lt;li&gt;5th feature ranges between 8 and 12 (approx)&lt;/li&gt;&lt;li&gt;6th feature ranges between 70 and 85 (in the complete dataset)&lt;/li&gt;&lt;li&gt;7th feature ranges between 1 and 3 (in the complete dataset)&lt;/li&gt;&lt;li&gt;8th feature is a string (so we'll ignore it for this discussion)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reason that we are interested in the &quot;range&quot; of values that our features take is because depending on the relative ranges that the feature variables take the shape of the cost function will differ. And this will impact the performance of our implementation.&lt;/p&gt;&lt;p&gt;I hope that you remember from &lt;a href='http://www.golb.in/ml-302-intuition-for-univariate-linear-regression-39.html'&gt;this&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-303-gradient-descent-41.html'&gt;this&lt;/a&gt; posts that our cost function &quot;J(&amp;theta;)&quot; always has a convex (like a bowl) shape. When we see a bowl from the top, we see a circle. But for our cost function, it can vary between being circular to an extremely thin &lt;a href='http://en.wikipedia.org/wiki/Ellipse'&gt;ellipse&lt;/a&gt;. Whether it will be a circle or an ellipse is determined by the relative ranges of the features.&lt;/p&gt;&lt;p&gt;Also, the more circular the shape of our cost function, the better is the performance of the gradient descent implementation. That is because in this case the gradient descent needs much lesser number of steps (iterations) to reach the global optima. As against this, if the shape is narrow, then the cost can oscillate around the major axis of the ellipse and take lot of iterations to reach the global optima.&lt;/p&gt;&lt;p&gt;In order for the shape of our cost function to be as circular as possible, we must modify our input variables such that their ranges are similar. This process is known as &quot;feature-scaling&quot;. Although, there are various methods to achieve this, the most commonly used technique is the &quot;mean normalization&quot; and is defined as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; x&lt;sub&gt;j&lt;/sub&gt; = (x&lt;sub&gt;j&lt;/sub&gt; - m&lt;sub&gt;j&lt;/sub&gt;) / s&lt;sub&gt;j&lt;/sub&gt;; where &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; m&lt;sub&gt;j&lt;/sub&gt; = average (or mean) value of x&lt;sub&gt;j&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; s&lt;sub&gt;j&lt;/sub&gt; = range (max - min) OR std. deviation of x&lt;sub&gt;j&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; j = j&lt;sup&gt;th&lt;/sup&gt; feature &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;By implementing the above on all our input variables, we make the data of the form&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; -1 &lt;= X(i, j) &lt;= 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;One thing to note though is that we need not follow the above rule very religiously; approximate values are ok too. For example,&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; -1 &lt;= X(:, 1) &lt;= 1           % 1st feature &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; 5 &lt;= X(:, 2) &lt;= 10           % 2nd feature &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; -100 &lt;= X(:3) &lt;= -50         % 3rd feature &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;is fine. But the below is not&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; -1 &lt;= X(:, 1) &lt;= 1           % 1st feature &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; 5 &lt;= X(:, 2) &lt;= &lt;b&gt;1000&lt;/b&gt;         % 2nd feature &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; -100 &lt;= X(:3) &lt;= -50         % 3rd feature &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Of course, even with the above data the implementation will work, but it might take a very long time to converge.&lt;/p&gt;&lt;p&gt;Another very important thing to remember is that we should &lt;b&gt;not&lt;/b&gt; scale the pivot feature x&lt;sub&gt;0&lt;/sub&gt;.&lt;/p&gt;&lt;p&gt;I think that's it for today. In the upcoming posts we will study other ML algorithms and also understand which to use when. So stay tuned!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 04 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-03-ml-203-vectorization-in-octave.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-03-ml-203-vectorization-in-octave.html
</link>
<title>
ml-203: Vectorization in Octave
</title>
<description>
&lt;p&gt;I hope you have noticed that we have jumped back to the 20x series from the last &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;post&lt;/a&gt; which was in the 30x series, and wondered why are we moving backwards ;). Well, that is because in the last 20x &lt;a href='http://www.golb.in/ml-202-octave-2-37.html'&gt;post&lt;/a&gt; I had said that there is still one thing left about octave that we have not studied yet, but which we will study after completing one ML algorithm. So lets get onto that.&lt;/p&gt;&lt;p&gt;But before we move ahead, I urge you to take a look at our final program for &quot;multivariate linear regression&quot; ML algorithm implementation in the previous &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;post&lt;/a&gt;, because our today's discussion is all about how we can improve it's performance by making use of the &quot;vectorization&quot; property in octave.&lt;/p&gt;&lt;p&gt;The basic meaning of vectorization is that instead of running a (for) loop over a vector, we use matrix theory to do the computations. This is because in most of the programming languages that support matrix manipulations the libraries for doing them are heavily optimized and can run orders of magnitude faster than the corresponding loop implementation.&lt;/p&gt;&lt;p&gt;Lets take a concrete example to understand better. Lets look at the following snippet from our complete multivariate linear regression implementation (see previous &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;post&lt;/a&gt;):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;function y = hypothesis&amp;#40;x, theta&amp;#41;
  N = length&amp;#40;x&amp;#41;;
  y = 0;
  for j in 1:N
    y = y + x&amp;#40;j&amp;#41; &amp;#42; theta&amp;#40;j&amp;#41;;
  end;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Remember that, the input variables &quot;x&quot; and &quot;theta&quot; in the above function are &quot;1xN&quot; and &quot;Nx1&quot; vectors respectively, so if we do the following vector multiplication&lt;/p&gt;&lt;pre&gt;&lt;code&gt;x &amp;#42; theta;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;then the result is exactly the same as that of our previous implementation of the hypothesis function. So by implementing our hypothesis function as the following:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;function y = hypothesis&amp;#40;x, theta&amp;#41;
  y = x &amp;#42; theta;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;we get the following benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;our LOC (lines of code) reduces; and the less code we write, the less bugs it will have ;)&lt;/li&gt;&lt;li&gt;more importantly, we have moved from a loop structure to matrix algebra, so that we are making use of the highly optimized libraries of octave. thus our overall program performance will improve tremendously&lt;/li&gt;&lt;li&gt;the implementation becomes generic and will work for all of scalars, vectors and matrices&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Similarly, we should take benefit from octave functions that will work on vectors and matrices and avoid loops completely. Taking cue from the above, lets re-implement our multivariate linear regression algorithm as follows:&lt;div id=&quot;i-ads&quot;&gt;&lt;div class=&quot;g-ad&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:inline-block;width:234px;height:60px&quot; data-ad-client=&quot;ca-pub-7627106577670276&quot; data-ad-slot=&quot;9310803587&quot;&gt;&lt;/ins&gt;&lt;/div&gt;&lt;div class=&quot;g-ad&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:inline-block;width:234px;height:60px&quot; data-ad-client=&quot;ca-pub-7627106577670276&quot; data-ad-slot=&quot;1787536781&quot;&gt;&lt;/ins&gt;&lt;/div&gt;&lt;/div&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% helper functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function y = hypothesis&amp;#40;x, theta&amp;#41;
  y = x' &amp;#42; theta;
end;

function J = cost&amp;#40;X, y, theta&amp;#41;
  m = size&amp;#40;X, 1&amp;#41;;
  X = &amp;#91;ones&amp;#40;m, 1&amp;#41;, X&amp;#93;; % add the x0 feature column
  J = sum&amp;#40;&amp;#40;hypothesis&amp;#40;X, theta&amp;#41; - y&amp;#41; .&amp;#94; 2&amp;#41; / &amp;#40;2 &amp;#42; m&amp;#41;;
end;

function &amp;#91;J, theta&amp;#93; = gradientDescent&amp;#40;X, y, theta, alpha, num\&amp;#95;iters&amp;#41;
  m = size&amp;#40;X, 1&amp;#41;;
  n = size&amp;#40;X, 2&amp;#41;;
  for iter in 1:num\&amp;#95;iters
    diff = hypothesis&amp;#40;X, theta&amp;#41; - y;
    sum = zeros&amp;#40;n, 1&amp;#41;;
    for j = 1:n
      sum&amp;#40;j&amp;#41; = sum&amp;#40;diff&amp;#40;j&amp;#41; .\&amp;#42; X'&amp;#40;:, j&amp;#41;&amp;#41;;
    endfor;
    theta = theta - &amp;#40;alpha &amp;#42; &amp;#40;sum / m&amp;#41;&amp;#41;;
  endfor;
  J = cost&amp;#40;X, y, theta&amp;#41;;
end;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% main program remains unchanged
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Do you see the reduction in complexity? Still there is an improvement in the generality of the program! And to top it all this implementation runs super fast as compared to the previous. Ain't that great?&lt;/p&gt;&lt;p&gt;Ensure that you understand what is going on here. Take help from the previous 20x octave posts &lt;a href='http://www.golb.in/ml-201-octave-1-36.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-202-octave-2-37.html'&gt;here&lt;/a&gt;. Remember, the easiest trick to know that you are doing the correct thing is to check the ranks of the matrices that you are doing computation on. For example, in the first example above,&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rank&amp;#40;x&amp;#41; =&amp;gt; 1, n
rank&amp;#40;theta&amp;#41; =&amp;gt; n, 1

expected rank of h =&amp;gt; 1, 1

so it should be that =&amp;gt; h = x &amp;#42; theta
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you remember the above rule-of-thumb, it should be difficult to make mistakes ;) But if you still have any doubts, don't hesitate to ask them using the comments, and I'll try my best to answer them. So what are you waiting for? Try implementing the above in octave on your system and try experimenting to improve your own understanding!&lt;/p&gt;&lt;p&gt;ps: In the coursera ML course, all octave programs are really programming assignments and have credits. I have broken the promise by giving (partially) answers here and in the previous &lt;a href='http://www.golb.in/ml-402-multivariate-linear-regression-in-octave-45.html'&gt;post&lt;/a&gt;. I will not show anymore octave programs continuing forward, but only the functions and intuitions behind them. I hope that you will understand. Thanks :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 03 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-02-ml-402-multivariate-linear-regression-in-octave.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-02-ml-402-multivariate-linear-regression-in-octave.html
</link>
<title>
ml-402: Multivariate Linear Regression in Octave
</title>
<description>
&lt;p&gt;Hi! I hope that you have been following the recent posts in the ML series where we have understood in much detail the various aspects of the linear regression ML algorithm, and also intuition about how it works. We have also formulated all the mathematical formulae, but have not seen how to implement them in octave yet. So that's what we are going to do today. Lets start with a small recap (of only the functions) of the multivariate linear regression ML algorithm:&lt;/p&gt;&lt;p&gt;&lt;b&gt;hypothesis function&lt;/b&gt; is defined as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;Sigma;&lt;sub&gt;j=0&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; &amp;theta;&lt;sub&gt;j&lt;/sub&gt; x&lt;sub&gt;j&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;cost function&lt;/b&gt; is defined as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J&lt;sub&gt;(&amp;theta;0, &amp;theta;1, &amp;theta;2, &amp;hellip;, &amp;theta;n)&lt;/sub&gt; = (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; &amp;Sigma;&lt;sub&gt;j=1&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; (h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;While the &lt;b&gt;gradient descent&lt;/b&gt; steps are:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;)), for j = 0 to n &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Before we move onto octave, lets define some variables:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;b&gt;X&lt;/b&gt; =&amp;gt; input feature &quot;matrix&quot; of size &quot;mxn&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &lt;b&gt;x&lt;/b&gt; =&amp;gt; &quot;one&quot; example &quot;vector&quot; of size &quot;(n+1)x1&quot; (including the pivot feature x0 (which is always = 1)) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &lt;b&gt;&amp;theta;&lt;/b&gt; =&amp;gt; vector of size &quot;(n+1)x1&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &lt;b&gt;y&lt;/b&gt; =&amp;gt; output target variable &quot;vector&quot; of size &quot;mx1&quot; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Now for the actual octave program&lt;/p&gt;&lt;pre&gt;&lt;code&gt;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% helper functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function y = hypothesis&amp;#40;x, theta&amp;#41;
  N = length&amp;#40;x&amp;#41;; % this is actually inclusive of pivot x0, so this N = n + 1, where n = #features
  y = 0;
  for j in 1:N
    y = y + x&amp;#40;j&amp;#41; &amp;#42; theta&amp;#40;j&amp;#41;;
  end;
end;

function J = cost&amp;#40;X, y, theta&amp;#41;
  m = size&amp;#40;X, 1&amp;#41;;
  X = &amp;#91;ones&amp;#40;m, 1&amp;#41;, X&amp;#93;; % add the x0 feature column
  n = size&amp;#40;X, 2&amp;#41;; % note that this n includes the x0 feature too
  J = 0;
  for i in 1:m
    J = J + &amp;#40;hypothesis&amp;#40;X&amp;#40;i, :&amp;#41;, theta&amp;#41; - y&amp;#40;i&amp;#41;&amp;#41; &amp;#94; 2; % the inner &amp;quot;j&amp;quot; for-loop is inside the &amp;quot;hypothesis&amp;quot; function
  end;
  J = J / &amp;#40;2 &amp;#42; m&amp;#41;;
end;

function &amp;#91;J, theta&amp;#93; = gradientDescent&amp;#40;X, y, theta, alpha, num\&amp;#95;iters&amp;#41;
  % ideally we should let gradientDescent run until convergence,
  % but for simplicity we will run it for a fixed number of iterations &amp;#40;num\&amp;#95;iters&amp;#41;
  
  m = size&amp;#40;X, 1&amp;#41;;
  n = size&amp;#40;X, 2&amp;#41;;
  
  for iter in 1:num\&amp;#95;iters
    sum = zeros&amp;#40;n, 1&amp;#41;;
    for i = 1:m
      diff = hypothesis&amp;#40;X&amp;#40;i, :&amp;#41;, theta&amp;#41; - y&amp;#40;i&amp;#41;;
      for j = 1:n
        sum&amp;#40;j&amp;#41; = sum&amp;#40;j&amp;#41; + diff &amp;#42; X&amp;#40;i, j&amp;#41;;
      end;
    end;
    for j = 1:n
      theta&amp;#40;j&amp;#41; = theta&amp;#40;j&amp;#41; - &amp;#40;alpha &amp;#42; &amp;#40;sum&amp;#40;j&amp;#41; / m&amp;#41;&amp;#41;;
    end;
  end;

  J = cost&amp;#40;X, y, theta&amp;#41;;
end;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% main program starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

&amp;#91;X, y&amp;#93; = load&amp;#40;&amp;quot;data&amp;quot;&amp;#41;;        % load some data

%%%%% initialize some basic variables
m = size&amp;#40;X, 1&amp;#41;;               % #training examples
X = &amp;#91;ones&amp;#40;m, 1&amp;#41;, X&amp;#93;;          % add pivot feature vector x0
n = size&amp;#40;X, 2&amp;#41;;               % #features + 1
alpha = 0.01;                 % &amp;quot;learning rate&amp;quot;; actual value needs to be determined experimentally
theta = zeros&amp;#40;n, 1&amp;#41;;          % initialize to some arbitrary value
num\&amp;#95;iters = 1000;             % initialize to some arbitrary value

%%%%% run gradientDescent
&amp;#91;J, theta&amp;#93; = gradientDescent&amp;#40;X, y, theta, alpha, num\&amp;#95;iters&amp;#41;;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That's it! Wasn't that easy?&lt;/p&gt;&lt;p&gt;Once we have found out the optimal values for &amp;theta; (theta), we can find the value for any new &quot;y&quot;, by simply calling the hypothesis function using the new (given) value for &quot;x&quot;.&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 02 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-02-01-ml-401-multivariate-linear-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-02-01-ml-401-multivariate-linear-regression.html
</link>
<title>
ml-401: Multivariate Linear Regression
</title>
<description>
&lt;p&gt;Hi! In the last &lt;a href=&quot;http://www.golb.in/ml-305-univariate-linear-regression-43.html&quot;&gt;post&lt;/a&gt;, we completed learning our very first machine learning algorithm, viz &quot;linear regression&quot;. As you might remember from &lt;a href=&quot;http://www.golb.in/ml-102-supervised-learning-34.html&quot;&gt;here&lt;/a&gt;, this is a type of &quot;supervised learning&quot; ML algorithm. However, during our study, we had only 1 feature (or input variable), so that what we learnt was really the &quot;univariate linear regression&quot;. Today we will take a look at how to handle a more real life situation, that is when we have more than one feature. Such an algorithm is called the &quot;multivariate linear regression&quot; ML algorithm.&lt;/p&gt;&lt;p&gt;To recap, for univariate linear regression, the steps were:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;x&lt;/b&gt; =&amp;gt; input variable/feature (in &quot;univariate&quot;, we have only 1 feature)&lt;/li&gt;&lt;li&gt;&lt;b&gt;y&lt;/b&gt; =&amp;gt; output variable/target (this is the value that we need to predict)&lt;/li&gt;&lt;li&gt;&lt;b&gt;m&lt;/b&gt; =&amp;gt; #training examples&lt;/li&gt;&lt;li&gt;&lt;b&gt;h(x)&lt;/b&gt; =&amp;gt; hypothesis function, defined as&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; x &lt;/p&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;cost function&lt;/b&gt; =&amp;gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;=1&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m)&lt;/li&gt;&lt;li&gt;minimization of the cost function was done using the &quot;gradient descent&quot; algorithm which was implemented as&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)), for j = 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;)), for j = 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;For the &quot;multivariate&quot; version of &quot;linear regression&quot; ML algorithm, we will have multiple features x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;, etc. We define one more notation&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;n&lt;/b&gt; =&amp;gt; number of features&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In this case our hypothesis function &quot;h(x)&quot; will change as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; x&lt;sub&gt;1&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;2&lt;/sub&gt; x&lt;sub&gt;2&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;3&lt;/sub&gt; x&lt;sub&gt;3&lt;/sub&gt; + &amp;hellip; + &amp;Theta;&lt;sub&gt;n&lt;/sub&gt; x&lt;sub&gt;n&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;Sigma;&lt;sub&gt;j=1&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; x&lt;sub&gt;j&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;It is customary (makes later maths easier) to add an extra feature x&lt;sub&gt;0&lt;/sub&gt; whose value is always &quot;1&quot;, so that h(x) becomes&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Sigma;&lt;sub&gt;j=0&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; x&lt;sub&gt;j&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Our cost function too will change as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, &amp;Theta;&lt;sub&gt;n&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; &amp;Sigma;&lt;sub&gt;j=1&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;And so will the gradient descent:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)), for j = 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;)), for j = 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;2&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;2&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;2&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;2&lt;/sub&gt;)), for j = 2 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;hellip; &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;n&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;n&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;n&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;n&lt;/sub&gt;)), for j = n &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or, re-written simply:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha;&amp;#42;(1/m)&amp;#42; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;) * (x&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;)), for j = 0 to n &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;As always, remember that the above updates to all &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; (for j = 0 to n) should happen simultaneously for a correct implementation (see &lt;a href=&quot;http://www.golb.in/ml-305-univariate-linear-regression-43.html&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;Once we compute our hypothesis function using the above implementation, it is then trivial to predict &quot;y&quot; for any given input set of features&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; y = h(x) = &amp;Sigma;&lt;sub&gt;j=0&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; x&lt;sub&gt;j&lt;/sub&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;That's it for today. In the next post we will learn how to implement these algorithms using &quot;vectorization&quot; for faster performance. So stay tuned!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 01 Feb 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-30-ml-305-univariate-linear-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-30-ml-305-univariate-linear-regression.html
</link>
<title>
ml-305: Univariate Linear Regression
</title>
<description>
&lt;p&gt;In the last posts &lt;a href='http://www.golb.in/ml-303-gradient-descent-41.html'&gt;here&lt;/a&gt;&amp;nbsp;and &lt;a href='http://www.golb.in/ml-304-gradient-descent-and-learning-rate-%CE%B1-42.html'&gt;here&lt;/a&gt;, we looked in much depth at the &quot;gradient descent&quot; minimization algorithm. Today we will complete the puzzle by fitting that last piece in the whole jigsaw of the &quot;univariate linear regression&quot; ML algorithm that we have been studying in this 30x series.&lt;/p&gt;&lt;p&gt;To do that, lets recap what we have learnt so far:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;x =&amp;gt; input variable/feature (in &quot;univariate&quot;, we have only 1 feature)&lt;/li&gt;&lt;li&gt;y =&amp;gt; output variable/target (this is the value that we need to predict)&lt;/li&gt;&lt;li&gt;m =&amp;gt; #training examples&lt;/li&gt;&lt;li&gt;h(x) =&amp;gt; hypothesis function, defined as&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; x &lt;/p&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;=1&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m)&lt;/li&gt;&lt;li&gt;Goal is to minimize the cost function, to find optimal values of &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; (to be substituted in the hypothesis function). This is achieved using gradient descent algorithm, defined as&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) (for j = 0, 1) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;where,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&amp;alpha;&lt;/b&gt;: is called the &quot;learning rate&quot;&lt;/li&gt;&lt;li&gt;&lt;b&gt;&amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&lt;/b&gt;: is the partial derivative of the cost function (J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)) with respect to &amp;Theta;&lt;sub&gt;j&lt;/sub&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The only thing left is to substitute the real value of J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) inside the gradient descent algorithm. To do that we need to know the partial derivative of J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) with respect to &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;. Without going into derivation of the partial derivatives (because&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it is too complex and mathematically involved&lt;/li&gt;&lt;li&gt;i don't know it ;) and&lt;/li&gt;&lt;li&gt;knowing the derivation is not needed to correctly implement gradient descent&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;), the gradient descent steps are defined (for our cost function for univariate linear regression) as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; - &amp;alpha;&lt;em&gt;(1/m)&lt;/em&gt; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x(i)) - y(i))), for j = 0 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; - &amp;alpha;&lt;em&gt;(1/m)&lt;/em&gt; (&amp;Sigma;&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x(i)) - y(i)) * (x(i))), for j = 1 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; } &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;That is we are inserting the actual values of the partial derivatives of J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;), with respect to &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; respectively in the gradient descent implementation. As mentioned previously, while implementing this step in octave (or any other software), it is extremely important to make both updates simultaneously.&lt;/p&gt;&lt;p&gt;With this we have completed learning our first ML algorithm, viz the &quot;univariate linear regression&quot;. I hope you have had as much fun learning it as I have teaching it. For me, a lot of my concepts have become clear while explaining things over here in so much detail! This alone has made the whole effort worthwhile. It has also given me motivation to continue this series. So in the upcoming posts I will try to explain other more advanced ML algorithms.&lt;/p&gt;&lt;p&gt;Do leave comments below to share with me your experience here, especially (constructive) criticism about how I can strive to make these posts better. I am eagerly looking forward to hearing from you :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 30 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-29-ml-304-gradient-descent-and-learning-rate.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-29-ml-304-gradient-descent-and-learning-rate.html
</link>
<title>
ml-304: Gradient Descent and Learning Rate
</title>
<description>
&lt;p&gt;Hi. In the last &lt;a href=&quot;http://www.golb.in/ml-303-gradient-descent-41.html&quot;&gt;post&lt;/a&gt;, we learnt about the &quot;gradient descent&quot; algorithm that is used to minimize our cost function &quot;J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&quot;. Gradient descent algorithm was defined as:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;repeat until convergence {
  j = j -  /j J&amp;#40;0, 1&amp;#41; &amp;#40;for j = 0, 1&amp;#41;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&amp;alpha;&lt;/b&gt;: is called the &quot;learning rate&quot;&lt;/li&gt;&lt;li&gt;&lt;b&gt;&amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&lt;/b&gt;: is the partial derivative of the cost function (J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)) with respect to &amp;Theta;&lt;sub&gt;j&lt;/sub&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Today we will try to understand the working of the gradient descent algorithm itself. We will also learn about the role of the learning rate &quot;&amp;alpha;&quot;. Let us start by understanding a single step of the algorithm, viz&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;j = &amp;Theta;j - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;j J(&amp;Theta;0, &amp;Theta;1) (for j = 0, 1) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;As we have mentioned above, the term &quot;&amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&quot; is the partial derivative of the cost function (J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)) with respect to &amp;Theta;&lt;sub&gt;j&lt;/sub&gt;. The partial derivative of a function defines the &quot;slope&quot; of that function at a given point. That is, in this case, this term defines the slope of our cost function at a given point.&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;We also know, from this &lt;a href=&quot;http://www.golb.in/ml-302-intuition-for-univariate-linear-regression-39.html&quot;&gt;post&lt;/a&gt;, that the term &amp;Theta; defines the slope of our hypothesis function &quot;h(x)&quot;. So in a single step of the gradient descent algorithm, we are trying to change the slope &amp;Theta; by a small amount (given by &amp;alpha; times the slope of J).&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;How &quot;small&quot; is the change, is what is determined by the learning rate &quot;&amp;alpha;&quot;. If &quot;&amp;alpha;&quot; is too small, then the changes will be very small and our gradient descent implementation will take a long time to converge (see the green points in the graph below). Whereas if &quot;&amp;alpha;&quot; is too large, then gradient descent will take very big steps, and it is even possible for it to overshoot the minimum value and even diverge. But if &quot;&amp;alpha;&quot; is just right (see red points in graph), then the implementation will take just the right steps and converge soon to the local minimum.&lt;/p&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;As such, the gradient descent algorithm can be used to find any minima (local or global). But because we have defined our cost function in such a way, that it is always convex and always has a global minima, our gradient descent implementation will always find the global minima!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 29 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-28-ml-303-gradient-descent.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-28-ml-303-gradient-descent.html
</link>
<title>
ml-303: Gradient Descent
</title>
<description>
&lt;p&gt;Hi, welcome back! I hope you have been following our machine learning algorithm series in the last few posts. If you have, then you know that till now we have developed the basic framework for a particular algorithm called &quot;univariate linear regression&quot; and the only step pending is to minimize our cost function &quot;J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&quot; so that we can find the optimal values of &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;. Once we have these values, we can substitute them and arrive at the definition of our hypothesis function &quot;h(x)&quot; which will let us predict the values for our target variable &quot;y&quot;.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;I hope that things are clear till here. If they are, then lets move on to our last and final step of the algorithm, that is to minimize our cost function &quot;J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&quot;. The general outline of the steps that we will follow to achieve this are:&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;start with some value of &amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; (a common choice is &quot;0, 0&quot;)&lt;/li&gt;&lt;/li&gt;&lt;li&gt;keep changing &amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; to reduce J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) until we hopefully end up at a minimum&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/static/photos/3f253e13ef98f0f1eab3f715986f0bc81_600.png&quot; style=&quot;width: 600px; height: 332px;&quot; /&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;There are various minimization algorithms available, but the most commonly used (and the one that we will study now) is the &quot;gradient descent&quot; minimization algorithm. It is defined as below&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; repeat until convergence { &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; = &amp;Theta;&lt;sub&gt;j&lt;/sub&gt; - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) (for j = 0, 1) }&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;where,&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&amp;alpha;&lt;/b&gt;: is called the &quot;learning rate&quot;&lt;/li&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;&amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;j&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&lt;/b&gt;: is the partial derivative of the cost function (J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)) with respect to &amp;Theta;&lt;sub&gt;j&lt;/sub&gt;&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Note that in the above algorithm, in any 1 step, &lt;span style=&quot;text-decoration:underline;&quot;&gt;all&lt;/span&gt; of the &amp;Theta;'s should be updated simultaneously. So,&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; temp0 = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;0&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; temp1 = &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;1&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = temp0 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; = temp1&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;is correct, but the following is not&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; temp0 = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;0&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = temp0 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; temp1 = &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; - &amp;alpha; &amp;delta;/&amp;delta;&amp;Theta;&lt;sub&gt;1&lt;/sub&gt; J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; = temp1&lt;/pre&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;That is because in the second set of commands, when we are calculating temp1, we use the &lt;span style=&quot;text-decoration:underline;&quot;&gt;new&lt;/span&gt; value of &amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, instead of the old one. This is a very important step and missing this can lead to a lot of difficult to find errors, so be careful about implementing this step.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;That's it for today. In the next post we will dig deeper to get an understanding of how gradient descent works to find the minimal value of the cost function (J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)) and in turn help us to find the optimal values of &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;. We will also see what is the learning rate &quot;&amp;alpha;&quot; and how it's value will have an impact on the performance of our gradient descent implementation.&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 28 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-27-golbin-updates-bug-fixes-and-new-feature.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-27-golbin-updates-bug-fixes-and-new-feature.html
</link>
<title>
Golbin updates: bug fixes and new features
</title>
<description>
&lt;p&gt;Hi! We hope that you are liking the various stories that you see here on &lt;a href='http://www.golb.in/'&gt;Golbin&lt;/a&gt;. We are always striving not only to bring you quality content that will interest you, but are also adding new features to the site. As we have mentioned previously &lt;a href='http://www.golb.in/lazyload-google-ads-without-jquery-lazyload-ad-plugin-27.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/more-exciting-new-features-for-golbin-25.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/speed-your-website-100-times-11.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/exciting-new-features-on-golbin-9.html'&gt;here&lt;/a&gt;, we have regularly added new features, and we sincerely hope that you like them :)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;In today's release we have made the following changes:&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;CSS fix&lt;/b&gt;: We have fixed some css issues (especially related to 'table', and 'li' elements) that made some of the articles look bad&lt;/li&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;image size&lt;/b&gt;: In the recent articles (&lt;a href='http://www.golb.in/org-mode-32.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/vimperator-part-2-31.html'&gt;here&lt;/a&gt;, etc), the images inside the article were of very small size (width = 300px) due to which they were not able to deliver the content that they were supposed to. This irritating issue has been fixed&lt;/li&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;future publish&lt;/b&gt;: We have added a new feature to the editorial site so that an author can enter any date-time in the future and publish the article. The article will get saved in the &quot;draft&quot; mode and will automatically be published on the said date-time. This will allow the authors to write stories in advance for an event and be rid of the worry of needing to publish it on the desired date-time. Ain't it cool?&lt;/li&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;auto archival&lt;/b&gt;: This is another great feature that we have added to the editorial site. Actually, to be honest, it always existed, but was not visible to authors due to a bug that has now been fixed. The feature is that every time an author publishes his article, it gets saved on the &lt;a href='http://web.archive.org'&gt;web.archive.org&lt;/a&gt; website. For example the last &lt;a href='http://www.golb.in/ml-302-intuition-for-univariate-linear-regression-39.html'&gt;article&lt;/a&gt; got archived &lt;a href='http://web.archive.org/web/20140124145929/http://www.golb.in/ml-302-intuition-for-univariate-linear-regression-39.html'&gt;here&lt;/a&gt;. This is an important feature because it will allow resolution of issues in case there is a plagiarism issue recorded (by or against &lt;a href='http://www.golb.in/'&gt;Golbin&lt;/a&gt;)&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If all this sounds great to you, do share it with others, and surely leave your comments/feedback. Better still, why don't you join us share your stories; after all that is why this website exists, doesn't it?&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 27 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-27-ml-302-intuition-for-univariate-linear-regression.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-27-ml-302-intuition-for-univariate-linear-regression.html
</link>
<title>
ml-302: Intuition for Linear Regression
</title>
<description>
&lt;p&gt;Hello! Welcome back. Last time we studied the &quot;univariate linear regression&quot; ML algorithm &lt;a href='http://www.golb.in/ml-301-linear-regression-with-one-variable-38.html'&gt;here&lt;/a&gt;. That time we took in a lot of new concepts, and not everything might be clear to all. So today we will dig a bit deeper into the various components and try to understand how and why they work correctly. This will help us analyze if our implementation is working correctly and also to debug things if it is not.&lt;/p&gt;&lt;p&gt;So lets begin with a quick review. First, the terminology&lt;/p&gt;&lt;ul&gt;&lt;li&gt;m =&gt; #examples&lt;/li&gt;&lt;li&gt;x =&gt; input variable/feature&lt;/li&gt;&lt;li&gt;y =&gt; output variable/target&lt;/li&gt;&lt;li&gt;x&lt;sup&gt;(i)&lt;/sup&gt;, y&lt;sup&gt;(i)&lt;/sup&gt; =&gt; i&lt;sup&gt;th&lt;/sup&gt; training example&lt;/li&gt;&lt;li&gt;h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; x =&gt; hypothesis function; it maps from 'x' to 'y'&lt;/li&gt;&lt;li&gt;J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m) =&gt; cost function; it is a function of &amp;Theta; (not 'x')&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The algorithm flow is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;define the hypothesis function (univariate for now)&lt;/li&gt;&lt;li&gt;find optimal values for &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; by minimizing cost function&lt;/li&gt;&lt;li&gt;substitute &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; in h, to find the hypothesis function&lt;/li&gt;&lt;li&gt;now using this hypothesis function, we can predict value of 'y' for any given 'x'I remember that we have still not figured out how to do the minimization step, but we will get there, don't worry :) Today we will look in detail at the first few steps to improve our understanding of them to such an extent so as to&lt;/li&gt;&lt;li&gt;ensure that our implementation is correct&lt;/li&gt;&lt;li&gt;fix it if it is not&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So lets begin! To keep things simple, and our explanation easier to understand, lets assume that &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = 0, so that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; x&lt;/li&gt;&lt;li&gt;J(&amp;Theta;&lt;sub&gt;1&lt;/sub&gt;) = (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If we take the following data&lt;/p&gt;&lt;pre&gt;&lt;code&gt;x = &amp;#91;0:10&amp;#93;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and plot the following data&lt;/p&gt;&lt;pre&gt;&lt;code&gt;hold on
plot&amp;#40;x, x &amp;#42; 2&amp;#41;
plot&amp;#40;x, x &amp;#42; 3&amp;#41;
plot&amp;#40;x, x &amp;#42; 4&amp;#41;
hold off
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;p&gt;that is, we take different values of &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; (2, 3 and 4 respectively), then we see that we get 3 different lines. All of them pass through the origin (because &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; = 0), but have different angles. This means that our variable &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; defines the angle (or slope) of our line. Also note that this line is the representation of our hypothesis function. It is a straight line because we our hypothesis function is a function of a single variable (x); it is &quot;univariate&quot; linear regression.&lt;/p&gt;&lt;p&gt;Lets move on to our cost function J. As before, visualization will help us understand it better. But to get there, we should first define some data. So lets borrow the data from the last &lt;a href='http://www.golb.in/ml-301-linear-regression-with-one-variable-38.html'&gt;post&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; x&lt;/td&gt; &lt;td&gt; y&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 307.0&lt;/td&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 350.0&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 318.0&lt;/td&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 304.0&lt;/td&gt; &lt;td&gt; 16.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 302.0&lt;/td&gt; &lt;td&gt; 17.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 429.0&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 454.0&lt;/td&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 440.0&lt;/td&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 455.0&lt;/td&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 390.0&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;x = &amp;#91;307.0; 350.0; 318.0; 304.0; 302.0; 429.0; 454.0; 440.0; 455.0; 390.0&amp;#93;;
y = &amp;#91;18.0; 15.0; 18.0; 16.0; 17.0; 15.0; 14.0; 14.0; 14.0; 15.0&amp;#93;;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Also, for easy implementation, lets define our hypothesis to be function of both '&amp;Theta;&lt;sub&gt;1'&lt;/sub&gt; and 'x' as follows&lt;/p&gt;&lt;pre&gt;&lt;code&gt;function h = hypothesis&amp;#40;theta, x&amp;#41;
  h = theta &amp;#42; x;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Similarly, let us define our cost function as follows&lt;/p&gt;&lt;pre&gt;&lt;code&gt;function J = cost&amp;#40;theta, x, y&amp;#41;
  m = length&amp;#40;x&amp;#41;;
  J = 0;
  for i = 1:m
    J = J + &amp;#40;&amp;#40;&amp;#40;hypothesis&amp;#40;theta, x&amp;#40;i&amp;#41;&amp;#41;&amp;#41; - y&amp;#40;i&amp;#41;&amp;#41; &amp;#94; 2&amp;#41;;
  endfor;
  J = J / &amp;#40;2 &amp;#42; m&amp;#41;;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To get an understanding of how the cost varies with theta, we will take some theta values (say between -10 and 10 with a step of 0.5) and calculate the cost for these values. Then we can plot theta against these values to see how they interact with each other&lt;/p&gt;&lt;pre&gt;&lt;code&gt;theta = &amp;#91;-10:0.5:10&amp;#93;;
theta = theta&amp;#40;:&amp;#41;;
costs = zeros&amp;#40;length&amp;#40;theta&amp;#41;, 1&amp;#41;;

for i = 1:length&amp;#40;theta&amp;#41;
  costs&amp;#40;i&amp;#41; = cost&amp;#40;theta&amp;#40;i&amp;#41;, x, y&amp;#41;;
endfor;

plot&amp;#40;theta, costs&amp;#41;;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;![]()&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#91;a, b&amp;#93; = min&amp;#40;costs&amp;#41;
theta&amp;#40;b&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Our cost-function (J) is minimum (122.80) when theta = 0; but that is not the point. The point is that the above curve is a 'convex' curve, and has a global minimum. In the next post we are going to learn techniques to find this global minimum value, so keep watching this space for that!&lt;/p&gt;&lt;p&gt;I hope that the above explanation convinces you that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the cost function is defined in such a way that it has a global minimum (that needs to be found)&lt;/li&gt;&lt;li&gt;once we find the values of theta which minimize the cost (or error), we would get a hypothesis that will allow us to predict the value of y, given x&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So lets now meet in the next post in which we will see how to implement logic to minimize the cost-function J.&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 27 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-26-ml-301-linear-regression-with-one-variable.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-26-ml-301-linear-regression-with-one-variable.html
</link>
<title>
ml-301: Linear Regression with one variable
</title>
<description>
&lt;p&gt;Welcome back! After having gone through some posts on introduction to ML (&lt;a href='http://www.golb.in/ml-101-introduction-to-machine-learning-33.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-102-supervised-learning-34.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-103-unsupervised-learning-and-recommender-systems-35.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-201-octave-1-36.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-202-octave-2-37.html'&gt;here&lt;/a&gt;), lets now start learning a real ML algorithm. The algorithm that we will study today is a &lt;a href='http://www.golb.in/ml-102-supervised-learning-34.html'&gt;supervised learning&lt;/a&gt; regression algorithm.&lt;/p&gt;&lt;p&gt;First lets define the problem statement. Today we will use a subset of &lt;a href='http://archive.ics.uci.edu/ml/datasets/Auto+MPG'&gt;this&lt;/a&gt; &lt;a href='http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'&gt;data&lt;/a&gt;. The first few training examples in the data are:&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 307.0&lt;/td&gt; &lt;td&gt; 130.0&lt;/td&gt; &lt;td&gt; 3504.&lt;/td&gt; &lt;td&gt; 12.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;chevrolet chevelle malibu&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 350.0&lt;/td&gt; &lt;td&gt; 165.0&lt;/td&gt; &lt;td&gt; 3693.&lt;/td&gt; &lt;td&gt; 11.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;buick skylark 320&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 318.0&lt;/td&gt; &lt;td&gt; 150.0&lt;/td&gt; &lt;td&gt; 3436.&lt;/td&gt; &lt;td&gt; 11.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;plymouth satellite&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 16.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 304.0&lt;/td&gt; &lt;td&gt; 150.0&lt;/td&gt; &lt;td&gt; 3433.&lt;/td&gt; &lt;td&gt; 12.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;amc rebel sst&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 17.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 302.0&lt;/td&gt; &lt;td&gt; 140.0&lt;/td&gt; &lt;td&gt; 3449.&lt;/td&gt; &lt;td&gt; 10.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;ford torino&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 429.0&lt;/td&gt; &lt;td&gt; 198.0&lt;/td&gt; &lt;td&gt; 4341.&lt;/td&gt; &lt;td&gt; 10.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;ford galaxie 500&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 454.0&lt;/td&gt; &lt;td&gt; 220.0&lt;/td&gt; &lt;td&gt; 4354.&lt;/td&gt; &lt;td&gt; 9.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;chevrolet impala&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 440.0&lt;/td&gt; &lt;td&gt; 215.0&lt;/td&gt; &lt;td&gt; 4312.&lt;/td&gt; &lt;td&gt; 8.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;plymouth fury iii&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 455.0&lt;/td&gt; &lt;td&gt; 225.0&lt;/td&gt; &lt;td&gt; 4425.&lt;/td&gt; &lt;td&gt; 10.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;pontiac catalina&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 390.0&lt;/td&gt; &lt;td&gt; 190.0&lt;/td&gt; &lt;td&gt; 3850.&lt;/td&gt; &lt;td&gt; 8.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;amc ambassador dpl&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 383.0&lt;/td&gt; &lt;td&gt; 170.0&lt;/td&gt; &lt;td&gt; 3563.&lt;/td&gt; &lt;td&gt; 10.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;dodge challenger se&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 340.0&lt;/td&gt; &lt;td&gt; 160.0&lt;/td&gt; &lt;td&gt; 3609.&lt;/td&gt; &lt;td&gt; 8.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;plymouth 'cuda 340&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 400.0&lt;/td&gt; &lt;td&gt; 150.0&lt;/td&gt; &lt;td&gt; 3761.&lt;/td&gt; &lt;td&gt; 9.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;chevrolet monte carlo&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;td&gt; 8&lt;/td&gt; &lt;td&gt; 455.0&lt;/td&gt; &lt;td&gt; 225.0&lt;/td&gt; &lt;td&gt; 3086.&lt;/td&gt; &lt;td&gt; 10.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;buick estate wagon (sw)&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 24.0&lt;/td&gt; &lt;td&gt; 4&lt;/td&gt; &lt;td&gt; 113.0&lt;/td&gt; &lt;td&gt; 95.00&lt;/td&gt; &lt;td&gt; 2372.&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 3&lt;/td&gt; &lt;td&gt; &quot;toyota corona mark ii&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 22.0&lt;/td&gt; &lt;td&gt; 6&lt;/td&gt; &lt;td&gt; 198.0&lt;/td&gt; &lt;td&gt; 95.00&lt;/td&gt; &lt;td&gt; 2833.&lt;/td&gt; &lt;td&gt; 15.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;plymouth duster&quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;td&gt; 6&lt;/td&gt; &lt;td&gt; 199.0&lt;/td&gt; &lt;td&gt; 97.00&lt;/td&gt; &lt;td&gt; 2774.&lt;/td&gt; &lt;td&gt; 15.5&lt;/td&gt; &lt;td&gt; 70&lt;/td&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; &quot;amc hornet&quot;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;In this data, the 1st column is the &quot;mpg&quot; target variable, or the parameter that we have to predict, while the rest of the columns are the features or input variables. But today we will go simple and will use only 1 input variable, that is the 3rd column &quot;displacement&quot;. To keep the algorithm simple, we will assume that our target variable &quot;mpg&quot; is someway dependent or related to only &quot;displacement&quot;, our input variable.&lt;/p&gt;&lt;p&gt;So now the first 10 training examples look like (notice that I have swapped the order of the data Note that the first column is x (input variable(s)), and second column is y (target variable))&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; x&lt;/td&gt; &lt;td&gt; y&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 307.0&lt;/td&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 350.0&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 318.0&lt;/td&gt; &lt;td&gt; 18.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 304.0&lt;/td&gt; &lt;td&gt; 16.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 302.0&lt;/td&gt; &lt;td&gt; 17.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 429.0&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 454.0&lt;/td&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 440.0&lt;/td&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 455.0&lt;/td&gt; &lt;td&gt; 14.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 390.0&lt;/td&gt; &lt;td&gt; 15.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;p&gt;Thus the problem definition is given a new &quot;displacement&quot; (x), we need to predict the value for &quot;mpg&quot; (y). To do this we need to a function &quot;h&quot; that maps the input &quot;x&quot; to the output &quot;y&quot;. In ML world, this function &quot;h&quot; is called the hypothesis function, and is represented as&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) = &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; + &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; x&lt;/blockquote&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In the above example &quot;h&quot; is also called 'univariate linear regression', because we are using only 1 variable (or feature).&lt;/p&gt;&lt;p&gt;Before we move ahead, lets get some terminology noted&lt;/p&gt;&lt;ul&gt;&lt;li&gt;m =&gt; #examples&lt;/li&gt;&lt;li&gt;x =&gt; input variable/feature&lt;/li&gt;&lt;li&gt;y =&gt; output variable/target&lt;/li&gt;&lt;li&gt;x&lt;sup&gt;(i)&lt;/sup&gt;, y&lt;sup&gt;(i)&lt;/sup&gt; =&gt; i&lt;sup&gt;th&lt;/sup&gt; training example, so for example, in the above data, x&lt;sup&gt;(1)&lt;/sup&gt; = 307.0, x&lt;sup&gt;(5)&lt;/sup&gt; = 302.0, y&lt;sup&gt;(7)&lt;/sup&gt; = 14.0&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;What we need to do next is to chose &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; so that h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) is as close to y for our training examples (x, y). I hope you understand why we are doing this. This is because, if the hypothesis function that we find out (as the end goal of our ML problem) is perfect, then the value of h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) should be exactly equal to the corresponding y&lt;sup&gt;(i)&lt;/sup&gt;, for any example i.&lt;/p&gt;&lt;p&gt;So, obviously, our goal would be to do the following&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; minimize&lt;sub&gt;(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;,&amp;Theta;&lt;sub&gt;1&lt;/sub&gt;)&lt;/sub&gt; (&amp;Sigma;&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; (h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x&lt;sup&gt;(i)&lt;/sup&gt;) - y&lt;sup&gt;(i)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;)/(2&amp;#42;m)&lt;/blockquote&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The division by 2m is simply taking an average of the squared error and is done to make later math easier. This function is called the cost function J(&amp;Theta;&lt;sub&gt;0&lt;/sub&gt;, &amp;Theta;&lt;sub&gt;1&lt;/sub&gt;), and also called squared error function, or squared error cost function. This is the most most commonly used error function in ML world.&lt;/p&gt;&lt;p&gt;By implementing the above we would find the optimum values of &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; that will minimize our cost function J. Then all we need to do is to insert these &amp;Theta;&lt;sub&gt;0&lt;/sub&gt; and &amp;Theta;&lt;sub&gt;1&lt;/sub&gt; values into our hypothesis function h&lt;sub&gt;&amp;Theta;&lt;/sub&gt;(x) and our ML problem is solved. Then given any new value for x, we can simply put that x in the hypothesis function to get the y.&lt;/p&gt;&lt;p&gt;Ain't that cool? We have already started solving ML problems and making predictions!&lt;/p&gt;&lt;p&gt;Well, not so fast stud ;) We still have to figure out how to perform the last step of the puzzle, viz &quot;minimize&quot;, don't we? But I think this much study is good for today, and we will jump to the minimization problem in the next post. So keep watching this space :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 26 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-25-ml-202-octave-2.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-25-ml-202-octave-2.html
</link>
<title>
ml-202: octave -- part - 2
</title>
<description>
&lt;p&gt;Hi there, welcome back! In the last &lt;a href='http://www.golb.in/ml-201-octave-1-36.html'&gt;post&lt;/a&gt;, we looked at some basic &lt;a href='http://www.gnu.org/software/octave/doc/interpreter/index.html'&gt;octave&lt;/a&gt; commands. Today we will dig deeper into octave and understand more concepts and commands. I hope you are eager and excited, so lets begin!&lt;/p&gt;&lt;p&gt;&lt;b&gt;Plotting&lt;/b&gt;: One of the most important things that we do during implementing ML algorithms is to plot data. This is done to&lt;/p&gt;&lt;ul&gt;&lt;li&gt;understand the nature of our input/training data which gives us insight into what approach to take to solve the problem&lt;/li&gt;&lt;li&gt;examine our implementation and ensure that it is indeed working correctly, and/or improve it&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Some of the important commands from the plotting world are:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;plot&amp;#40;A, B&amp;#41; % if A, B are both vectors, then it will plot them against the X and Y axes
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;hold on % print a new graph on top of the previous graph
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;xlabel&amp;#40;'&amp;amp;amp;lt;label-name&amp;amp;amp;gt;'&amp;#41; % label the x axis &amp;#40;similarly, ylabel&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;axis&amp;#40;&amp;#91;0.5 1 -1 1&amp;#93;&amp;#41; % change X axis range to be between 0.5 and 1, and Y axis between -1 and 1
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;print -dpng '&amp;amp;amp;lt;file-name&amp;amp;amp;gt;.png' % save graph as png file
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;close % close the graph
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;subplot&amp;#40;1, 2, 1&amp;#41; % divide figure into 2 parts, and access the first element
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;Computing on data&lt;/b&gt;: Finally, the operations that you have been waiting for. Doing stuff to all the data that we have created and/or loaded earlier:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;E = D&amp;#40;:&amp;#41; % will create a column vector from matrix D
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;sum&amp;#40;C&amp;#41; % will do a column-wise sum of matrix C and produce a 1x3 row vector
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;sum&amp;#40;sum&amp;#40;C&amp;#41;&amp;#41; % will sum all the elements of matrix C and produce a single number
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;some operators work at matrix level (example A &amp;#42; B is matrix multiplication), where as some operators (the ones beginning with the '.' character, like .* for example) work at element level (example A .&amp;#42; B is multiplication of corresponding elements of matrices A and B (assuming both A and B have the same ranks))&lt;/p&gt;&lt;pre&gt;&lt;code&gt;size&amp;#40;A&amp;#41; % shows the rank of matrix A
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;F = &amp;#91;B B2&amp;#93; % place B and B2 next to each other &amp;#40;horizontally&amp;#41; and save them in F
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;G = &amp;#91;B; B2&amp;#93; % place B and B2 one below the other &amp;#40;vertically&amp;#41; and save them in G
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;Control statements&lt;/b&gt;: Like any other programming language, octave too has its own set of control statements, and they are as follows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;for loop&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;v = zeros&amp;#40;10, 1&amp;#41;;
for i in 1:10
  v&amp;#40;i&amp;#41; = 2 .&amp;#94; i;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;while loop&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;i = 1;
while i &amp;amp;amp;lt;= 5
  v&amp;#40;i&amp;#41; = v&amp;#40;i&amp;#41; &amp;#42; 2;
  i = i + 1;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;if and break (to achieve the same thing as point-2 above)&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;i = 1;
while true
  v&amp;#40;i&amp;#41; = v&amp;#40;i&amp;#41; &amp;#42; 2;
  i = i + 1;
  if i == 6
    break;
  end;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;if-elseif-else&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;v&amp;#40;1&amp;#41; = 2;
if v&amp;#40;1&amp;#41; == 1
  disp&amp;#40;'The value is one'&amp;#41;;
elseif v&amp;#40;1&amp;#41; == 2
  disp&amp;#40;'The value is two'&amp;#41;;
else
  disp&amp;#40;'The value is not one or two'&amp;#41;;
end;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;functions&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;function &amp;#91;y1, y2&amp;#93; = squareAndCubeThisNumber&amp;#40;x&amp;#41;
  y1 = x &amp;#94; 2;
  y2 = x &amp;#94; 3;
end;
squareAndCubeThisNumber&amp;#40;3&amp;#41; % will return and print 9 and 27
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;One most important thing that I have not mentioned yet, but hope that you have noticed is that the array (vector and matrix) subscripts start with '1' and not '0' in octave. This information is very important for those of you who come from a programming background (I guess that would be all of you who are reading this post ;) )&lt;/p&gt;&lt;p&gt;Do note that the above (and in the previous post) is &lt;i&gt;not&lt;/i&gt; an exhaustive list of all capabilities of octave; and neither are they the only operations that we will be using during implementing ML algorithms. So I urge you all to please&lt;/p&gt;&lt;ul&gt;&lt;li&gt;install octave on your local system&lt;/li&gt;&lt;li&gt;Read further documentation &lt;a href='http://www.gnu.org/software/octave/doc/interpreter/index.html'&gt;here&lt;/a&gt; and play around with the help command&lt;/li&gt;&lt;li&gt;Try out the above commands (at the least) and feel a bit comfortable using them&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With the basics now out of our way, lets move with full speed ahead to really implementing some ML algorithms. God speed!&lt;/p&gt;&lt;p&gt;ps: There is one more part about octave (vectorization), but we will get to that after studying atleast 1 more real algorithm first!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 25 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-24-ml-201-octave-1.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-24-ml-201-octave-1.html
</link>
<title>
ml-201: octave -- part - 1
</title>
<description>
&lt;p&gt;Hi again! In the previous posts &lt;a href='http://www.golb.in/ml-101-introduction-to-machine-learning-33.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/ml-102-supervised-learning-34.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/ml-103-unsupervised-learning-and-recommender-systems-35.html'&gt;here&lt;/a&gt;, we did pretty theoretical and very basic introduction. But starting now (notice the &lt;b&gt;201&lt;/b&gt; in the title of the article?), we will be digging deeper into specific algorithms, their working, intuition and implementation. But before we go there, we need to get familiar with some background setup.&lt;/p&gt;&lt;p&gt;Firstly, you should have atleast some basic knowledge of the following disciplines of mathematics:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href='http://en.wikipedia.org/wiki/Linear_algebra'&gt;linear algebra&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='http://en.wikipedia.org/wiki/Matrix_(mathematics'&gt;matrix theory&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;&lt;a href='http://en.wikipedia.org/wiki/Calculus'&gt;calculus&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Don't worry, you don't need to be gurus, just basic knowledge is good enough to understand the intuition of and implement ML algorithms correctly.&lt;/p&gt;&lt;p&gt;Furthermore, for this series I will use the &lt;a href='https://www.gnu.org/software/octave/'&gt;octave&lt;/a&gt; programming language because:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;that is what was used in the &lt;a href='https://www.coursera.org/'&gt;coursera&lt;/a&gt; course that I recently completed (which is what prompted this series)&lt;/li&gt;&lt;li&gt;it is extremely good for rapid prototyping (most algorithms can be implemented in just a few lines of code)&lt;/li&gt;&lt;li&gt;the smartest thing that you can do when implementing a ML algorithm is &quot;rapid prototyping&quot; to avoid getting trapped in a black hole of wrong effort which does not lead to a effective solution, but simply wastes timeSo without further ado, lets start with getting a little bit familiar with the octave programming language:&lt;/li&gt;&lt;li&gt;Octave is a &lt;a href='http://www.gnu.org/'&gt;gnu&lt;/a&gt; software and its installers are available for OS X, windoZZZe, and linux &lt;a href='http://www.gnu.org/software/octave/download.html'&gt;here&lt;/a&gt;&lt;/li&gt;&lt;li&gt;It is a programming language made specifically for ML and has good library support&lt;/li&gt;&lt;li&gt;However, it is very slow and hence might not be suitable for production programs&lt;/li&gt;&lt;li&gt;You can execute octave commands both from the command line, or even a GUI&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Octave is very well documented and the complete documentation is available &lt;a href='http://www.gnu.org/software/octave/doc/interpreter/index.html'&gt;here&lt;/a&gt;, but we will go through some of the most commonly used commands (and more during the actual algorithm discussion). I have only used the command line version during the course, and that is what I will explain here. Maybe the GUI would be much easier for you, YMMV.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Basic&lt;/b&gt;: The most frequently used command in octave (for us beginners) is, obviously the &quot;help&quot; command ;). The basic format of the help command is&lt;/p&gt;&lt;pre&gt;&lt;code&gt;help &amp;lt;command-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For example &quot;help plot&quot; shows the help section for the &quot;plot&quot; command&lt;/p&gt;&lt;p&gt;Some other basic commands are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the '%' character starts a comment. If it comes in the middle of a line, then the part after the '%' is the comment. eg&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;% this whole line is a command
a = 5; % a is assigned the value 5
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;a ';' suppresses the output. so for eg&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;a = 5 % this will print 'a = 5'
a = 5; % this will not print anything
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;inside a matrix definition, the ';' character takes the remaining numbers onto the next row of the matrix. eg&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;A = &amp;#91;1 2 3; 2 3 4; 3 4 5&amp;#93;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;is a 3x3 matrix of the form&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;table table-bordered&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; 1&lt;/td&gt; &lt;td&gt; 2&lt;/td&gt; &lt;td&gt; 3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 2&lt;/td&gt; &lt;td&gt; 3&lt;/td&gt; &lt;td&gt; 4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 3&lt;/td&gt; &lt;td&gt; 4&lt;/td&gt; &lt;td&gt; 5&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Matrices (and vectors (single dimension matrices)) are the &quot;the&quot; way of representing data and performing computations on them during the implementation of ML algorithms in octave. So obviously, octave has a great support for matrix creation and manipulation. Some of the frequently used operations are:&lt;ul&gt;&lt;li&gt;A = zeros(2, 3) % will create a 2x3 matrix filled with the element zero&lt;/li&gt;&lt;li&gt;B = ones(3, 2) % will create a 3x2 matrix filled with the element one&lt;/li&gt;&lt;li&gt;B2 = 2 * ones(3, 2) % will create a 3x2 matrix filled with the element two&lt;/li&gt;&lt;li&gt;C = rand(3, 3) % will create a 3x3 matrix filled with numbers between 0 and 1 with a uniform distribution&lt;/li&gt;&lt;li&gt;D = eye(5) % will create a 5x5 &lt;a href='http://en.wikipedia.org/wiki/Identity_matrix'&gt;identity matrix&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Loading, saving and plotting data&lt;/b&gt;: Of course the next most common operation that you would do while implementing a ML algorithm would be loading, saving and plotting of data. Octave supports these operations via:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;pwd: shows the current working directory&lt;/li&gt;&lt;li&gt;cd &lt;new-directory&gt;: change to another directory (where data and/or code exists, so that you can load them)&lt;/li&gt;&lt;li&gt;data = load(&lt;file-name&gt;): load data from file into variable named &quot;data&quot;&lt;/li&gt;&lt;li&gt;who: shows what variables are present in the memory in the current scope&lt;/li&gt;&lt;li&gt;whos: shows what variables are present in the memory, and show their size (and other information)&lt;/li&gt;&lt;li&gt;clear [&lt;variable-name&gt;+] : remove variable(s) from memory&lt;/li&gt;&lt;li&gt;save &lt;file-name&gt; &lt;variable-name&gt; : save value of variable (may be a vector or a matrix) into the named file&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I think that is a big enough bite for today. We will continue with some more octave in the next post, so keep watching this space :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 24 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-23-ml-103-unsupervised-learning-and-recommender-systems.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-23-ml-103-unsupervised-learning-and-recommender-systems.html
</link>
<title>
ml-103: Unsupervised learning and Recommender systems
</title>
<description>
&lt;p&gt;Hello :) In the last 2 posts &lt;a href='http://www.golb.in/ml-101-introduction-to-machine-learning-33.html'&gt;here&lt;/a&gt; and &lt;a target=&quot;_blank&quot;href=&quot;http://www.golb.in/ml-102-supervised-learning-34.html&quot;&gt;here&lt;/a&gt;, we got a general overview of what Machine learning is and also looked at a particular category ML algorithms, viz the &quot;supervised learning&quot; type.&lt;/p&gt;&lt;p&gt;Today we will briefly look at 2 more types of ML algorithms, viz the &quot;unsupervised learning&quot; and &quot;recommender systems&quot;. I hope you remember that in the supervised learning types of ML algorithms, we get the &quot;right answers&quot; embedded in the training data. As against this, in the &quot;unsupervised learning&quot; algorithms, we do &lt;strong&gt;not&lt;/strong&gt; get the right answers in the training data. In fact sometimes, there is no right answer at all, as I will show below.&lt;/p&gt;&lt;p&gt;The above explanation seems to be very vague, so lets dig deeper with the help of an example.&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;461&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/giladlotan/5151742603/player/0c0134107b&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;The above image is a snippet of the facebook social graph; it's a graph of how a user is connected to other users. It also shows clusters of users; groups of users who are very strongly/closely connected to each other. In such a dataset, as you would rightly guess, there is no &quot;right answer&quot;. Basically the objective is to find this cluster/closeness attribute.&lt;/p&gt;&lt;p&gt;Similarly, if you see &lt;a href='http://archive.ics.uci.edu/ml/datasets/3D+Road+Network+(North+Jutland,+Denmark'&gt;this&lt;/a&gt;) dataset, it's about the road network in North Jutland, Denmark in 3d (longitude, latitude, altitude). If the dataset is used to find information/knowledge which will be used to perform accurate routing for eco-routing, cyclist routes, etc, then there is no right answer which is present inside the dataset. The task is for the program to learn the structure of the data itself and find out interesting knowledge from it.&lt;/p&gt;&lt;p&gt;Some other examples of unsupervised learning are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;google news: it puts similar news, from various channels/websites, into similar (correct) sections&lt;/li&gt;&lt;li&gt;organizing computer clusters in a data-center: based on network activity between them, for example&lt;/li&gt;&lt;li&gt;market segmentation&lt;/li&gt;&lt;li&gt;social network analysis&lt;/li&gt;&lt;li&gt;astronomical data analysis&lt;/li&gt;&lt;li&gt;etc&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Lets move on to the next type of ML algorithm, which is &quot;recommender systems&quot;. The easiest way to imagine this algorithm is to understand an example that we have all experienced at some time or the other &amp;amp;ndash; e-commerce websites. I am sure you all would have noticed that when we buy something online, we are recommended other objects, for which there is a higher probability of us buying; for example, books from same author or on same topic, or movies from same genre, etc.&lt;/p&gt;&lt;p&gt;In ML, there are certain ways in which these systems are implemented. But don't you worry. We will be looking into all of these algorithms in much more depth in the forthcoming posts. So keep tuned :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 23 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-22-ml-102-supervised-learning.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-22-ml-102-supervised-learning.html
</link>
<title>
ml-102: Supervised Learning
</title>
<description>
&lt;p&gt;Hi! I hope you enjoyed the last &lt;a href='http://www.golb.in/ml-101-introduction-to-machine-learning-33.html'&gt;post&lt;/a&gt; on introduction to ML. Today we will dig just a little bit deeper and understand the various types of ML algorithms. Although not complete, some of the different types of ML algorithms are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Supervised learning&lt;/li&gt;&lt;li&gt;Unsupervised learning&lt;/li&gt;&lt;li&gt;Recommender systems&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The first 2, viz supervised and unsupervised techniques are related to each other and are differentiated based on the input data and the problem statement. If the (training) data contains both the features and the output (value to be found), then algorithms that work on such data are called supervised learning algorithms. Let me explain with the help of an example.&lt;/p&gt;&lt;p&gt;Lets say we have some data about houses like &quot;size in feet-square&quot; and &quot;price&quot;. We want our ML algorithm to observe this data and if we give it size data about some new house, then we want it to be able to predict the price for that new house. In this problem, then thing we want to predict (that is the output of the program) is the &quot;price&quot;. However, do notice that the training data contains both the &quot;size&quot; feature and the &quot;price&quot; parameter. Hence in this problem, we call the technique as a supervised learning algorithm. To put another way, the training dataset contains &quot;right answers&quot;.&lt;/p&gt;&lt;p&gt;Some other examples of supervised learning are datasets like&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href='http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset'&gt;Bike Sharing Dataset Data Set&lt;/a&gt;: From this dataset, if we want to predict how many bikes would be rented during the next month, then that problem would use a supervised learning ML algorithm&lt;/li&gt;&lt;li&gt;&lt;a href='http://archive.ics.uci.edu/ml/datasets/Thoracic+Surgery+Data'&gt;Thoracic Surgery Data Data Set&lt;/a&gt;: Using this dataset we might want to predict whether the post-operative life expectancy of a certain patient&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In both of the above examples, the dataset contains the &quot;right answers&quot; for the data provided. Based on this information, our program needs to learn the mapping between the features and the output. Then given a new problem instance (set of feature values), we need to predict the output value.&lt;/p&gt;&lt;p&gt;Supervised learning can be further divided into 2 broad categories based on the problem statement:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Regression&lt;/strong&gt;: This is the case when the output value to be predicted can take a wide range (mostly continuous) values. For example, in the &quot;house price prediction&quot; problem the predicted value is &quot;price&quot; which can take on a very wide range of values. Similarly, in the &quot;bike sharing&quot; problem, the value to be predicted is &quot;number of bikes rented&quot; which will also have a numerical value&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;: On the other hand, when the value to be predicted can have a value from within a fixed set of numbers, then it is called as a classification problem. For example, if our objective in the &quot;Thoracic Surgery&quot; dataset above would have been to find whether a patient died within the first year after surgery, then the output can have only 2 valid values &quot;yes&quot; or &quot;no&quot;; thus it would be a classification problem. The values are generally called &quot;class&quot; and it can be 2 or more. For example, in a handwriting recognition dataset, using the english alphabet, we would have 62 classes (A-Z, a-z, 0-9)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That's it for today. In the next post we will look at the remaining 2 ML algorithm types; so stay tuned :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 22 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-21-ml-101-introduction-to-machine-learning.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-21-ml-101-introduction-to-machine-learning.html
</link>
<title>
ml-101: Introduction to Machine Learning
</title>
<description>
&lt;p&gt;Facts:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;As some of my friends on twitter and facebook know, I recently completed the &quot;Machine Learning&quot; course on &lt;a href='https://www.coursera.org/'&gt;coursera&lt;/a&gt;, taught by &quot;Andrew Ng&quot; from Stanford university. I managed to get a 100% score on the programming assignments and the review tests (mean attempt of 2.22 on the tests), but did not get a completion certificate because I missed the &quot;completion survey&quot; before the deadline&lt;/li&gt;&lt;li&gt;While we teach, we learn.  &amp;ndash; Roman philosopher Seneca (see &lt;a href='http://en.wikipedia.org/wiki/Learning_by_teaching'&gt;this&lt;/a&gt; &lt;a href='http://ideas.time.com/2011/11/30/the-protege-effect/'&gt;this&lt;/a&gt;, &lt;a href='http://ezinearticles.com/?The-Best-Way-to-Learn-is-to-Teach&amp;amp;amp;id=560127'&gt;this&lt;/a&gt;, and lots more)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Combining these 2 facts I thought that it would be great to try and explain what I have learned during the last few weeks to all of you. This has the advantages of:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;My understanding of the subject will improve&lt;/li&gt;&lt;li&gt;I've been having the &quot;writers block&quot; for the last 2-3 days, and was not able to think of what to write here on &lt;a href='http://www.golb.in/'&gt;Golbin&lt;/a&gt;. Problem solved ;)&lt;/li&gt;&lt;li&gt;Maybe someone will benefit from this and will get inspired to try it out. Maybe I will get someone to collaborate on a project with, who knows?So lets start! Wait! Before starting, let me put down a few notes:&lt;/li&gt;&lt;li&gt;I have signed the &quot;code of honor&quot; on coursera, so do NOT expect to find assignment questions and answers to them here&lt;/li&gt;&lt;li&gt;I will try to use different examples (than in the course) to explain. This is mostly so that by applying the concepts to different things, I believe I will be able to improve my understanding better&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Ok, lets start now. Really start! First some definitions:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; [Machine Learning is a] field of study that gives computers the ability to learn without being explicitly programmed. &amp;amp;ndash;Arthur Samuel (1959) &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; A computer program is said to learn from experience E with respect to some task T and some performance measure P, if it's performance on T, as measured by P, improves with experience E. &amp;amp;ndash; Tom Mitchell (1998) &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The 2nd definition is a mathematically more well-posed learning problem, but maybe a bit more difficult to understand. The best way to understand the concept is through the following example: In 1959 Arthur Samuel created a checkers program even though he himself was not a good checkers player. What he did was for the program to play tens of thousands of games against itself, and learn from the various board positions that it encountered during the games. That is, when a program is not told the rules of the game, but learns them itself by observation, we can call it a machine learning program.&lt;/p&gt;&lt;p&gt;Machine learning is not a new field, but has been gaining a lot of interest and popularity in the last few years. It has grown out of &quot;Artificial Intelligence&quot; stream of computer science, but heavily uses other subjects like statistics for example.&lt;/p&gt;&lt;p&gt;In fact, today, machine learning algorithms are very pervasive, especially on the web. You are surrounded by them, although you might not notice them. Take the following for example:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Every time you use &lt;a href='http://www.google.com/'&gt;google&lt;/a&gt; search, you are using a algorithm that has been automatically learned by machine(s) to show relevant results to you&lt;/li&gt;&lt;li&gt;When you use email, there is a spam classifier algorithm working in the background to automatically send spam messages to the junk folder&lt;/li&gt;&lt;li&gt;Even the various ads that you see on web pages; some machine has learnt which ads it should show to you so as to increase the probability of you clicking on themSo what other kinds of programs can we write using ML?&lt;/li&gt;&lt;li&gt;ML is very good for programs that we cannot write by hand; either because we don't understand the concept totally ourselves, or because it is too difficult to map the problem into the computer domain. For example, autonomous helicopter navigation, handwriting recognition, NLP (natural language processing), computer vision, etc&lt;/li&gt;&lt;li&gt;Self customizing programs: When &lt;a href='http://www.amazon.com/'&gt;amazon&lt;/a&gt; (or &lt;a href='https://www.netflix.com/'&gt;netflix&lt;/a&gt;, etc) shows you recommendations of what to buy next, this program should be different for every user. Obviously it is not possible to make a million different copies (with different behavior and hence algorithm) of the recommender program. ML shines beautifully when solving such problems&lt;/li&gt;&lt;li&gt;Database mining: These days companies and organizations collect a huge amount of data (in terabytes), and in most cases we don't know what gems are hidden inside. This is where ML clustering algorithms can be used&lt;/li&gt;&lt;li&gt;and the list goes on and on&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So that's it for today. We will slowly dig deeper into the subject in the upcoming posts. So keep watching this space :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 21 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-20-org-mode.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-20-org-mode.html
</link>
<title>
Emacs: org-mode
</title>
<description>
&lt;p&gt;Looks like I really have you hooked onto the concept of not only improving productivity, but changing the way you think by &quot;killing the rat!&quot; (see &lt;a href='http://www.golb.in/kill-that-rat-28.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/stumpwm-29.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/vimperator-30.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/vimperator-part-2-31.html'&gt;here&lt;/a&gt; for previous post on the &quot;kill that rat&quot; series). Good for both of us! For you because you will pave your way onto a completely different level, and for me because I am benefiting through this series too&lt;/p&gt;&lt;ul&gt;&lt;li&gt;improving my writing skills&lt;/li&gt;&lt;li&gt;writing/blogging improves cognitive skills (see &lt;a href='http://www.golb.in/to-blog-or-not-to-blog-5.html'&gt;here&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;teaching is the best form of learning&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So lets continue the series and learn about the super-awesome &lt;a href='http://orgmode.org/'&gt;org-mode&lt;/a&gt; in &lt;a href='http://www.gnu.org/software/emacs/tour/'&gt;emacs&lt;/a&gt;. If emacs was not as awesome as it is, but had just this 1 feature, even then it is worthwhile to undergo the steep learning curve and start using emacs. Is that claim a bit too far fetched? Not at all, allow me to explain.&lt;/p&gt;&lt;p&gt;In short org-mode = notes + todo + planner + reminders + tables (including calculator) + export (to html, pdf, tex, etc) + etc&lt;/p&gt;&lt;p&gt;Basically, if you have ever used a mind-mapping tool (if you have not, and are not on emacs yet, I would surely suggest that you try one out (see &lt;a href='http://lifehacker.com/5188833/hive-five-five-best-mind-mapping-applications'&gt;here&lt;/a&gt;) before starting on the steep learning curve of emacs), then org-mode is everything that your (favorite) mind-mapping tool did not offer, and not all the bloat that you wanted it to not have.&lt;/p&gt;&lt;p&gt;Ok, enough round-about stories. Lets get to the real point with some concrete features:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Plain Text Files&lt;/strong&gt;: org-mode uses plain text files to store all information, which means that&lt;/p&gt;&lt;ul&gt;&lt;li&gt;you are never tied up to any proprietary format, and can move to some other tool if you wish&lt;/li&gt;&lt;li&gt;you can version-control your data files (visit the git-week series &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-4-16.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-5-17.html'&gt;here&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Section-ify&lt;/strong&gt;: it is super easy to split your content into sections, sub-sections, sub-sub-sections, and so on. The best part about splitting your content into sections is that you can collapse all sections except the one that you are interested in reading and/or editing right now. This is the same reason why people use mind-mapping tools; to be able to zoom into just 1 sub-part at a time and concentrate on it. Thus you can see the whole picture by zooming out, or concentrate on a small section by zooming in at whatever level you want to. Let me explain this the above image. As you can see, I am currently writing this very article in the golbin.org file. But all other sections, namely &quot;backlog&quot;, &quot;notes&quot;, etc and sub-sections, namely &quot;wrote (no published)...&quot;, etc are all collapsed. Thus at this time, I am looking only at this article, and nothing else is distracting me.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Agendas&lt;/strong&gt;: Each headline can be turned into a task, and you can assign a schedule (start date-time) and/or a deadline (end date-time) to it. Org-mode will then keep a track of it, and you will be able to see all of your todos in the agenda view (on a day/week/month level). This is really the killer feature of org-mode.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://orgmode.org/img/planning.jpg&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://orgmode.org/img/agenda.jpg&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Clocking&lt;/strong&gt;: &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; You cannot manage what you cannot measure. &amp;ndash;F. John Reh &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Well, we have been talking about improving productivity, but how can we do that if we don't measure it? Wouldn't it be great if we could know exactly how much time we are spending on every (non-trivial) activity? The clocking feature of org-mode allows one to do exactly that. All you have to do is to mention the task as a headline (doesn't matter what level it is at). Then you can simply take the cursor to that task and press 'C-c C-x C-i' to clock-in and 'C-c C-x C-o' to clock-out of the task. With practice it becomes second nature. And what's more org-mode even shows you a cool summary view&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://orgmode.org/img/clocking.jpg&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tables&lt;/strong&gt;: A lot of times we need to have some tabular data in our notes, but even our favorite note-taking (or mind-mapping) software does not allow us to do that; except org-mode that is ;). Org-mode allows not only creating and editing tables, but also allows us to import and export them from/to .csv and .tsv formats.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://orgmode.org/img/table2.jpg&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Exporting&lt;/strong&gt;: Org mode allows one to easily export notes into various formats like html, pdf, latex, etc. A lot of people even use it for blogging directly from within emacs.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://orgmode.org/img/export.jpg&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;I hope I have given you enough reasons to atleast give it a try and see how you feel about it. But then, as I had said previously, if you are not in a mood (or can't right now due to other constraints) to go through the steep learning curve that emacs has, you should atleast use a mind-mapping tool. It will allow you to organize your thoughts better!&lt;/p&gt;&lt;p&gt;Finally, as always, do share your experiences and don't feel shy/afraid to ask for help :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 20 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-19-vimperator-part-2.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-19-vimperator-part-2.html
</link>
<title>
Vimperator: part - 2
</title>
<description>
&lt;p&gt;Hello there. Welcome back! I hope you have already read my first post on vimperator &lt;a href='http://www.golb.in/vimperator-30.html'&gt;here&lt;/a&gt;, if not, I urge you to read that first and then come back here. Go on, I'll wait :)&lt;/p&gt;&lt;p&gt;Today I will show you some advanced, but even more awesome features of vimperator &amp;ndash; the tool that takes web browsing to another level! The first is &quot;modes&quot;. If you have used the fantastic &lt;a href='http://www.vim.org/'&gt;vim&lt;/a&gt; editor before, you will know what I am talking about; and you will feel very much at home with vimperator (in fact, I am sure you would have guessed that the name &quot;vimperator&quot; stems from &quot;vim&quot;). For those of you who have not used vim, worry not; that is why I am here, to help you out :)&lt;/p&gt;&lt;p&gt;Anyways, so vimperator has different modes of operation. The most commonly used are the &quot;insert&quot; and the &quot;normal&quot; modes. The &quot;insert&quot; mode is where the focus is inside some inputbox or textarea of a form (for example when you are filling up the username/password in a login form, or even when you are typing inside the search box on some website). In this mode your keyboard shortcuts like h, j, k, l, etc (see my previous post &lt;a href='http://www.golb.in/vimperator-30.html'&gt;here&lt;/a&gt; for details) will NOT work. Instead whatever you type on the keyboard will get filled in the box.&lt;/p&gt;&lt;p&gt;As against this, the &quot;normal&quot; mode is where you really use the keyboard shortcuts to navigate on the page, or between tabs, etc. The easiest way to go from &quot;normal&quot; mode to &quot;input&quot; mode is&lt;/p&gt;&lt;ul&gt;&lt;li&gt;either press &quot;f&quot;, and select the number corresponding to the input box you want to type into&lt;/li&gt;&lt;li&gt;or click with mouse inside the input box&lt;/li&gt;&lt;li&gt;or press &quot;gi&quot; to go to the first (or last visited) input box&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Similarly, to go from &quot;input&quot; mode to &quot;normal&quot; mode is&lt;/p&gt;&lt;ul&gt;&lt;li&gt;either left-click your mouse anywhere on the page (except inside an input box)&lt;/li&gt;&lt;li&gt;press the &quot;Esc&quot; key on your keyboard&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The next feature, which is my favorite, that I am going to show you is &quot;macros&quot;. Again, if you have used a real editor (vim and/or emacs), then you know what editor macros are. Basically they are a recorded set of commands that you can repeat with just 1 key press. Understanding macros can make your life super simple! So lets say you want to repeat a set of tasks a few number of times; for example, you want to register 10 accounts on a website that you are developing/testing. Also lets say, for the sake of this discussion, that the registration page is quite long (has say 20 fields). However, for the testing accounts you would want to only have the username to be different (something like &quot;test1&quot;, &quot;test2&quot;, &quot;test3&quot;, and so on), and all other fields to be the same. Lets also assume filling 1 form takes about 2 minutes &amp;ndash; that is 20 minutes for 10 accounts. So how do we do the same task in like 2.5 mins; or maybe 20 accounts in 3 mins?&lt;/p&gt;&lt;p&gt;The first thing you do, obviously, is to open the page ;). Now, before you start filling the form, press 'qa'. The key 'q' will start the recording and save it in a buffer 'a'. You should now see the word &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Recording &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; in your status bar. Then you fill up the form content. After finishing, press the 'q' key again, and the whole sequence (including form content) will be saved into the 'a' buffer. You should now see a message in the status bar like:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Recorded macro 'a' &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Remember to NOT press the &quot;submit&quot; button of the form while the recording is still going on, because that will use the same username all the 10 (or 20) times, and will simply result in an error. After you have pressed 'q' for the second time (and completed the macro recording), you should press the submit button. Now that we have this macro recorded, we want to repeat it 10 times. For that all you need to do is to open the registration page, and press '@a'. The '@' key tells vimperator that we want to invoke a previously saved macro, and the 'a' tells the macro name. In fact this can be done in an even better way by saving the &quot;page open&quot; also inside the macro (using the &quot;t &amp;amp;lt;page-url&amp;amp;gt;&quot; command (see my previous post &lt;a href='http://www.golb.in/vimperator-30.html'&gt;here&lt;/a&gt;)); this is left as an excercise for the user ;)&lt;/p&gt;&lt;p&gt;The above was just 1 scenario where one can use macros. But once you know that you have this powerful tool at your disposal, the opportunities of using it are endless. I have even successfully used it for developer testing of web frontend (something similar to what selenium does).&lt;/p&gt;&lt;p&gt;Finally, let me show you some configurations that you can put in your ~/.vimperatorrc file, with the help of a snippet of my config file&lt;/p&gt;&lt;pre&gt;&lt;code&gt;set ds=yahoo

map &amp;amp;amp;lt;C-.&amp;amp;amp;gt; gt
map &amp;amp;amp;lt;C-,&amp;amp;amp;gt; gT
map &amp;amp;amp;lt;C-s&amp;amp;amp;gt; :sav

ia Pra Pradnyesh
ia Sa Sawant
ia sp spradnyesh
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;&quot;set ds=yahoo&quot; sets Yahoo! as my default search engine. so if i say &quot;t &amp;amp;lt;some space separated text&amp;amp;gt;&quot;, then this is not a URL, and instead, vimperator will search for this text on the Yahoo! search engine&lt;/li&gt;&lt;li&gt;the &quot;map&quot; command maps a key combination to a particular command. For example, I personally find the &quot;gt&quot; and &quot;gT&quot; combos extremely unwieldy for changing tabs. So I have mapped the &quot;control+.&quot; and &quot;control+,&quot;, respectively, which are much more convenient for me. Similarly, &quot;control+s&quot; saves the page ;)&lt;/li&gt;&lt;li&gt;the &quot;ia&quot; command expands the shortform into the longform when the first is entered into some form input box. For example when I press &quot;Pra&quot; into an input box, it automatically gets expanded into &quot;Pradnyesh&quot;. Ain't that neat?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I sincerely hope that I have been able to show you what a great tool vimperator is and how it can turn web-browsing into a completely different ball game. I also hope that you have enjoyed reading this as much as I have writing it.&lt;/p&gt;&lt;p&gt;Finally, as always, do share your experiences and don't feel shy/afraid to ask for help :)&lt;/p&gt;&lt;p&gt;ps: I have tried similar plugins for the chrome browser, but they don't come anywhere near to what vimperator can do.&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 19 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-18-vimperator.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-18-vimperator.html
</link>
<title>
Vimperator: part - 1
</title>
<description>
&lt;p&gt;Continuing with &lt;a href='http://www.golb.in/kill-that-rat-28.html'&gt;this &lt;/a&gt; and &lt;a href='http://www.golb.in/stumpwm-29.html'&gt;this&lt;/a&gt;, let me introduce yet another of my favorite tools to you: &lt;a href='http://www.vimperator.org/vimperator'&gt;vimperator&lt;/a&gt;. Vimperator is essentially a &lt;a href='http://www.mozilla.org/en-US/'&gt;firefox&lt;/a&gt; plugin, but it is so awesome that it takes web browsing to a whole different level. And I am not exaggerating it at all; allow me to present the case in front of you.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;The most basic (but awesome, nonetheless) feature is mouseless browsing. Press the &quot;f&quot; key so that all links on the page get highlighted with a number.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;Screenshot&quot; src=&quot;../img/2015-11-05-153511&amp;#95;958x1061&amp;#95;scrot.png&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;Now you can press the number of the link you want to open, and that link will be opened in the current tab. Instead do the same with &quot;F&quot; and the link will be opened in a new tab. Similarly, the following keys can be used for navigation&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;h, j, k, l: moving left, down, up, right (similar to vim) on the page&lt;/li&gt;&lt;/li&gt;&lt;li&gt;gt: switch to next tab&lt;/li&gt;&lt;/li&gt;&lt;li&gt;gT: switch to previous tab&lt;/li&gt;&lt;/li&gt;&lt;li&gt;o: enter URL and open in current tab&lt;/li&gt;&lt;/li&gt;&lt;li&gt;O: edit current URL and open page in current tab&lt;/li&gt;&lt;/li&gt;&lt;li&gt;t: enter URL to open page in a new tab&lt;/li&gt;&lt;/li&gt;&lt;li&gt;T: edit current URL and open page in new tab&lt;/li&gt;&lt;/li&gt;&lt;li&gt;f: find on page&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Another feature that I use regularly (atleast 10-20 times in a single day) is dynamic bookmarks. I am sure that online search (google, bing, wikipedia, etc) is an extremely frequent usecase in any persons browsing session. Most browsers provide a dedicated search-bar next to the location bar. And also, this search-bar is customizable, that is the browser provides a dropdown of the various search engines that you want to search on. This is definitely more convenient than first going to the search engine page and then searching; you save 1 step. But:&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;see the above image again. Vimperator removes both the location bar and the search bar as well&lt;/li&gt;&lt;/li&gt;&lt;li&gt;more importantly, navigating the mouse to the search bar and optionally changing the search engine is not the best way to do it at all&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So how do we solve this problem efficiently using vimperator? This is where dynamic bookmarks come in. Lets take an example of using the google search engine. You need to do this only the 1st time:&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;go to google.com and search for &quot;golb.in&quot;&lt;/li&gt;&lt;/li&gt;&lt;li&gt;press &quot;a&quot; to see a &quot;bmark&quot; command at the bottom of the page&lt;/li&gt;&lt;/li&gt;&lt;li&gt;edit the URL to look similar to https://www.google.com/search?q=%s&lt;/li&gt;&lt;/li&gt;&lt;li&gt;leave a space after the URL and enter &quot;-keyword google&quot;&lt;/li&gt;&lt;/li&gt;&lt;li&gt;press enter&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;And you are done! Now every time you want to search something on google, simply press &quot;t google &amp;amp;lt;your search query&amp;amp;gt;&quot; and you will be taken to the corresponding google search page. You can do similarly with other search engines, and create bookmarks like &quot;bing&quot;, &quot;wiki&quot;, etc. Heck, this technique can be used on any website that gives you a search functionality. Who needs browser supported search engines anymore? Ain't that cool?&lt;/p&gt;&lt;/p&gt;&lt;p&gt;I hope this (and my previous &lt;a href='http://www.golb.in/kill-that-rat-28.html'&gt;post&lt;/a&gt; on going mouseless) has convinced you to atleast give vimperator a try. If not, don't worry. I still have a few more, advanced but even more awesome, features of vimperator. I will leave them for the next post ;)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;But I won't leave you before telling you that you can reach the locally available help via the &quot;:help&quot; and &quot;:helpall&quot; commands&lt;/p&gt;&lt;/p&gt;&lt;p&gt;If you did give vimperator a try, do let me know your experience using it, via comments. And don't feel shy or afraid to ask for help if you find it difficult&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 18 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-17-stumpwm.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-17-stumpwm.html
</link>
<title>
Stumpwm
</title>
<description>
&lt;p&gt;Yesterday, I mentioned &lt;a href='http://www.golb.in/kill-that-rat-28.html'&gt;here&lt;/a&gt; about how I am using different tools and techniques to go mouseless and improve not only my productivity, but also the way I think. After writing it, I felt that each of those tools deserved a more detailed description than just a mention. This would also allow me to show you some of the cool things that I am able to do with them, and how I do them. Maybe they will act as a better motivating factor for you to make that switch :)&lt;/p&gt;&lt;p&gt;Today I will talk about &lt;a href='http://www.nongnu.org/stumpwm/'&gt;stumpwm&lt;/a&gt;, the best (according to me) of them all. As a &lt;a href='http://en.wikipedia.org/wiki/Desktop_Window_Manager'&gt;DWM&lt;/a&gt; it is extremely minimal in terms of look-and-feel, but highly configurable. Since a picture is worth a 1000 words, I'll let the below screenshots do the talking of the look-and-feel of stumpwm:&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;Screenshot&quot; src=&quot;../img/screenshot&amp;#95;2015-11-05&amp;#95;15-23-15.png&quot; width=600 /&gt;&lt;/p&gt;&lt;p&gt;Even though the image is not clear, I hope you have noted that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;windows do not have any borders; there are no close/minimize buttons&lt;/li&gt;&lt;li&gt;all windows take up equal window space. tiling is a better word here, because every window is a tile. you can increase or decrease the size of a tile, and hence of the application that it contains&lt;/li&gt;&lt;li&gt;due to the tiling nature, you can arrange the tiles (extremely easily) in any manner that you want&lt;/li&gt;&lt;li&gt;the taskbar is at the top and is very minimal&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;My favorite tiling structure, when I am coding is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;emacs: left top&lt;/li&gt;&lt;li&gt;terminal: left bottom&lt;/li&gt;&lt;li&gt;firefox: right&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;with this structure the only time I have to ever touch the mouse is when I want to browse through the DOM structure in firefox!&lt;/p&gt;&lt;p&gt;Enough about tiling, lets talk about some other features, and then we will move onto configuration. As I had mentioned &lt;a href='http://www.golb.in/kill-that-rat-28.html'&gt;previously&lt;/a&gt;, my favorite feature is &quot;open-or-raise&quot;. What this means is that when you hit a (configured) key combination, stumpwm will try to switch to the (configured for that key combination) application if it is already running; if not, then stumpwm will first execute/start it and switch the focus to it. When I say &quot;switch to&quot;, it does not mean that it will pull the app into the tile that is currently into focus, but will instead move focus to the tile which is hosting the application.&lt;/p&gt;&lt;p&gt;This is better explained with help of another feature called &quot;groups&quot;. Groups is the same as the &quot;virtual desktop&quot; concept that linux has had for so many years (and windoZZZe still does not have it). Virtual desktops are an extremely convenient way to manage multiple applications. Let me explain this with an example. I generally have the following 4 groups (this predates my use of stumpwm):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;coding: emacs + terminal + firefox(+vimperator)&lt;/li&gt;&lt;li&gt;email client&lt;/li&gt;&lt;li&gt;messenger windows&lt;/li&gt;&lt;li&gt;general browsing: firefox, rss-reader, etc&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Coming back to open-or-raise. Lets say i am currently on group-2 and reading email. Then I can press super+e (super = the windoZZZe &quot;start&quot; key) and switch immediately to group-1 with focus in emacs. Similarly, I can press super+f once to go to firefox instance on group-1 and press again to go to instance on group-4. For comparison, imagine the other scenario (that most people use today):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;if you are a mouse user, move your hand from keyboard to mouse, navigate to the correct position on the taskbar, click, assume you have multiple instances of the application running, then navigate mouse to correct instance, click&lt;/li&gt;&lt;li&gt;if you are a keyboard user, then press alt+tab (like 10000 times :p) to finally come to the correct application&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;On the other hand, on stumpwm it's just 1 (or few) click(s). And configuring the keyboard binding is as simple as putting the below lines in your ~/.stumpwmrc file&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#40;defcommand emacs &amp;#40;&amp;#41; &amp;#40;&amp;#41; &amp;#40;run-or-raise &amp;quot;emacs&amp;quot; '&amp;#40;:class &amp;quot;Emacs&amp;quot;&amp;#41;&amp;#41;&amp;#41;
&amp;#40;define-key &amp;#42;top-map&amp;#42; &amp;#40;kbd &amp;quot;s-e&amp;quot;&amp;#41; &amp;quot;emacs&amp;quot;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Don't get scared of the brackets. The above lines are simply defining a command &quot;emacs&quot; and assigning it to the &quot;super+e&quot; key.&lt;/p&gt;&lt;p&gt;Personally, a setup like this makes the DWM invisible and makes me focus on the (really) important things; making it a &quot;best&quot; technology :)&lt;/p&gt;&lt;p&gt;I think the best technologies disappear. &amp;ndash; Jack Dorsey&lt;/p&gt;&lt;p&gt;It is really awesome, and there's no turning back once you get used to it. Of course, there are a lot more things that you can configure. But I would rather urge you to give it a try. It's as easy as running this command&lt;/p&gt;&lt;pre&gt;&lt;code&gt;sudo apt-get install stumpwm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;on Ubuntu (other linuxes should have similar commands), and re-logging in with stumpwm selected as your DWM.&lt;/p&gt;&lt;p&gt;Don't forget to share what your experience is. And don't feel shy to ask for help (there are a lot of good tutorials and documentation online, especially &lt;a href='http://www.nongnu.org/stumpwm/manual/stumpwm.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.mygooglest.com/fni/stumpwm.html'&gt;here&lt;/a&gt; and a sample (extremely detailed but well documented) stumpwmrc &lt;a href='http://deftsp-dotfiles.googlecode.com/svn/trunk/.stumpwmrc'&gt;here&lt;/a&gt;)!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 17 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-16-kill-that-rat.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-16-kill-that-rat.html
</link>
<title>
Kill that rat!
</title>
<description>
&lt;p&gt;I have been using &lt;a href='http://www.linux.org/'&gt;Linux&lt;/a&gt; for atleast the last 9 years, trying out different distributions like &lt;a href='http://www.redhat.com/'&gt;RedHat&lt;/a&gt;, &lt;a href='http://fedoraproject.org/'&gt;Fedora&lt;/a&gt;, &lt;a href='http://www.debian.org/'&gt;Debian&lt;/a&gt;, &lt;a href='http://www.ubuntu.com/'&gt;Ubuntu&lt;/a&gt;, &lt;a href='http://www.gentoo.org/'&gt;Gentoo&lt;/a&gt; and finally settling down on &lt;a href='http://www.sabayon.org/'&gt;Sabayon&lt;/a&gt; (for the last 2 years). Similarly, on the desktop manager side, I have used &lt;a href='http://kde.org/'&gt;KDE&lt;/a&gt; for a long long time, but have been using &lt;a href='http://www.nongnu.org/stumpwm/'&gt;Stumpwm&lt;/a&gt; for the last 3+ years. On the browser front, I use the awesome &lt;a href='http://www.vimperator.org/'&gt;vimperator&lt;/a&gt; plugin for &lt;a href='http://www.mozilla.org/en-US/'&gt;firefox&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;You might think, and correctly at that, &quot;what is this guy talking about? what has all this to do with a rat, or even with productivity (if you read the description of this article on one of the Golbin! index pages)&quot;. Well, let me start with the following definition:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; mouse, n: A device for pointing at the xterm in which you want to type. &amp;ndash;A.S.R. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;If you don't get the above, I am basically talking about the PC &quot;mouse&quot;. I feel that the mouse is only there to slow you down. Based on my personal experience, I can say very confidently that the less mouse I use, the faster I work. Agreed, that it takes a bit of practice, and maybe making some (initially difficult) choices too, but in the long term you will be a completely different person. Notice, that I don't say &quot;a more productive person&quot;, but a &quot;completely different person&quot;. That is because, using the keyboard changes you in more than 1 ways.&lt;/p&gt;&lt;p&gt;Of course your typing speed will increase, which will make regular things (coding, typing emails, blogging, etc) faster. This alone will help you think faster, because our system throughput is limited by the I/O (typing). Even though we are capable of thinking faster, we end up thinking slowly because our typing puts pauses to our thinking, breaking the flow.&lt;/p&gt;&lt;p&gt;Also, more importantly, your choice of tools/systems will change too. For example, I use the following tools/systems:&lt;/p&gt;&lt;p&gt;Linux: I am a software developer, and strongly believe that every software developer should use a Linux desktop to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;learn something more than clicking &quot;next, next, next, finish&quot; :p&lt;/li&gt;&lt;li&gt;getting hands dirty on the terminal. That itself will teach you so many tools (find, grep, etc) and techniques (regular expressions, bash scripting, etc) to make you a better developer&lt;/li&gt;&lt;li&gt;huge number choice of desktop managers (among other things)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Stumpwm&lt;/strong&gt;: I always liked KDE better than &lt;a href='http://www.gnome.org/'&gt;Gnome&lt;/a&gt;, simply because KDE allowed a &lt;i&gt;lot&lt;/i&gt; more keyboard customization than gnome (about 5-9 years ago). But in my search for a mouseless &lt;a href='http://en.wikipedia.org/wiki/Desktop_Window_Manager'&gt;DWM&lt;/a&gt; (desktop window manager), I stumbled upon &lt;a href='http://www.nongnu.org/ratpoison/'&gt;Ratpoison&lt;/a&gt;, &lt;a href='http://en.wikipedia.org/wiki/Wmii'&gt;wmii&lt;/a&gt; and Stumpwm. At that time, one of my colleagues was using wmii and showed it to me and I was impressed. However, since Stumpwm is written in common-lisp, and I was trying to learn it at that time, I tried using it, and have never looked back. Stumpwm is a &lt;a href='http://en.wikipedia.org/wiki/Tiling_window_manager'&gt;tiling window manager&lt;/a&gt; (unlike KDE, gnome, etc which are &lt;a href='http://en.wikipedia.org/wiki/Stacking_window_manager'&gt;stacking window managers&lt;/a&gt;). It allows me to have absolute control of where and how to place my windows, and all that without touching the mouse. But the feature I like the most in Stumpwm is to be able to bind keyboard shortcuts to applications (switch to a certain app, or start it if it's not already running and then switch to it). Agreed that recently KDE (and other DWMs) support this functionality, but they did not about 4+ years ago. And, just using this feature made me so much more productive, and gave me a adrenaline rush that I have never bothered looking at DWM advancements since; I have achieved DWM nirvana ;)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Vim and Emacs&lt;/strong&gt;: I had been a (pure) &lt;a href='http://www.vim.org/'&gt;vim&lt;/a&gt; guy for almost 5 years, when I switched to Emacs. The move was motivated largely because of common-lisp (&lt;a href='http://www.gnu.org/software/emacs/'&gt;emacs&lt;/a&gt; + &lt;a href='http://common-lisp.net/project/slime/'&gt;slime&lt;/a&gt; is the best way to code in common-lisp) and the uber awesome &lt;a href='http://orgmode.org/'&gt;org-mode&lt;/a&gt; (which I use for note taking, reminders, organization, planning and almost everything). To some people, these might be simply editors, but people who use them know that they will change the way you think and do things. If you are a developer, you should definitely use them!&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Vimperator&lt;/strong&gt;: Being a web developer, browser is my &quot;the&quot; tool. I tend to use both &lt;a href='http://www.google.com/chrome/'&gt;Chrome&lt;/a&gt; (&lt;a href='http://www.chromium.org/'&gt;chromium&lt;/a&gt;) and firefox during development. But for development I simply cannot do without the firefox+vimperator setup because browsing is such an easy (nay, great!) experience. Not only do i not need to move my hands (from the keyboard) to open links (in same or background tabs), but I can even do cool stuff like record a macro and play it back over and over again. This makes automated testing super easy!&lt;/p&gt;&lt;p&gt;As Eric Raymond said in &quot;How to Become a Hacker&quot;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Lisp is worth learning for the profound enlightenment experience you will have when you finally get it; that experience will make you a better programmer for the rest of your days, even if you never actually use Lisp itself a lot. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Similarly, going mouse-less is an enlightening way to learn something that will open the door to a wonderful new world. And if you will believe me, let me tell you, the grass is certainly greener on this side of the wall :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 16 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-15-lazyload-google-ads-without-jquery-lazyload-ad-plugin.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-15-lazyload-google-ads-without-jquery-lazyload-ad-plugin.html
</link>
<title>
Lazyload Google ads without jQuery LazyLoad ad plugin
</title>
<description>
&lt;p&gt;I had mentioned in one of my previous articles &lt;a href='http://www.golb.in/more-exciting-new-features-for-golbin-25.html'&gt;here&lt;/a&gt;, that we had changed our lazyloading technique for google-ads on &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt; pages. Today I will show you how we did it.&lt;/p&gt;&lt;p&gt;Previously, we were using the awesome jQuery lazyload Ad plugin as shown in their documentation &lt;a href='http://jqueryad.web2ajax.fr/'&gt;here&lt;/a&gt;. However, the plugin itself added about 8.4Kb to the page-weight. Even though Golbin! was using measures to lazyload the plugin itself, it was still additional page-weight, especially for first time (with browser cache empty) users.&lt;/p&gt;&lt;p&gt;Then, recently, we noticed that google had made some changes to their &lt;a href='https://support.google.com/adsense/answer/3221666'&gt;adsense&lt;/a&gt; code because of which the ads would be rendered asynchronously, thus removing the need for the jQuery lazyload Ad plugin. Sample code looks like below:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;script async src=&amp;quot;http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;ins class=&amp;quot;adsbygoogle&amp;quot;
    style=&amp;quot;display:inline-block;width:300px;height:250px&amp;quot;
    data-ad-client=&amp;quot;ca-pub-xxxxxxxxxxxxxxxx&amp;quot;
    data-ad-slot=&amp;quot;6440411535&amp;quot;&amp;gt;
&amp;lt;/ins&amp;gt;
&amp;lt;script&amp;gt; &amp;#40;adsbygoogle = window.adsbygoogle || &amp;#91;&amp;#93;&amp;#41;.push&amp;#40;{}&amp;#41;; &amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So far so good. However, we wanted to go a step further than this because:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In our experience, if we put the script in the bottom of the page (just before the ending &quot;&quot; tag), then the ads did not get rendered at all. Putting the js script at page bottom is good performance practice (see &lt;a href='http://developer.yahoo.com/performance/rules.html'&gt;here&lt;/a&gt; and &lt;a href='https://developers.google.com/speed/docs/insights/rules'&gt;here&lt;/a&gt; for best practices for website frontend performance).&lt;/li&gt;&lt;li&gt;Golbin! was already using a technique to lazyload the javascript files needed by the page itself. And we wanted to continue using this technique for the new script too.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Fortunately, one needs to include the script tag only once in the page irrespective of how many ad units were present on the page; this went well with our technique, because then we could lazyload the adsense script and use some JS to invoke some function to show the ads in the correct place.&lt;/p&gt;&lt;p&gt;Our initial HTML markup snippet looks something similar to:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;g-ad&amp;quot;&amp;gt;
    &amp;lt;ins class=&amp;quot;adsbygoogle&amp;quot;
        style=&amp;quot;display:inline-block;width:300px;height:250px&amp;quot;
        data-ad-client=&amp;quot;ca-pub-xxxxxxxxxxxxxxxx&amp;quot;
        data-ad-slot=&amp;quot;6440411535&amp;quot;&amp;gt;&amp;lt;/ins&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and the supporting javascript code looks like:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;$.getScript&amp;#40;&amp;quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&amp;quot;,
    function &amp;#40;data, textStatus, jqxhr&amp;#41; {
        var script = document.createElement&amp;#40;&amp;quot;script&amp;quot;&amp;#41;;
        script.type = &amp;quot;text/javascript&amp;quot;;
        script.text = &amp;quot;&amp;#40;adsbygoogle = window.adsbygoogle || &amp;#91;&amp;#93;&amp;#41;.push&amp;#40;{}&amp;#41;&amp;quot;;
        for &amp;#40;var a = null, arr = $&amp;#40;&amp;quot;div.g-ad&amp;quot;&amp;#41;, i = 0; i &amp;lt; arr.length; i += 1&amp;#41; {
            a = arr&amp;#91;i&amp;#93;;
            $&amp;#40;a&amp;#41;.append&amp;#40;script&amp;#41;;
       };
    }
&amp;#41;;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you have ever used jQuery, you will notice that what the above javascript snippet does is that it lazyloads the adsense javascript via the getScript function. And when the adsense javascript is loaded, the anonymous function is called which does the following steps&lt;/p&gt;&lt;ul&gt;&lt;li&gt;creates a script tag having the code shown in the adsense documentation&lt;/li&gt;&lt;li&gt;finds all &quot;div&quot; elements having the class &quot;g-ad&quot;&lt;/li&gt;&lt;li&gt;loops through the divs found in the above step and adds the script tag to the div&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;And voila, we're done! We have now successfully reduced 8.4Kb of page-weight, which means faster pages for our readers :)&lt;/p&gt;&lt;p&gt;The only drawback of this method is that it is impossible to debug it in the javascript debugger (in both firefox and chrome browsers), because the newly added script does not show up in the DOM :(&lt;/p&gt;&lt;p&gt;But don't worry. If this technique has worked for us, it should for you too. So, go ahead and give it a try! And don't forget to share your experiences back :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 15 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-14-busan-week-day-5-eulsukdo-eco-center.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-14-busan-week-day-5-eulsukdo-eco-center.html
</link>
<title>
Busan week: day 5 -- Eulsukdo Eco Center
</title>
<description>
&lt;p&gt;Since today was the last day of tour, we decided to go for the &quot;&lt;a href='http://busan.for91days.com/2012/07/06/the-eulsukdo-island-bird-sanctuary/'&gt;Eulsukdo eco-center&lt;/a&gt;&quot; themed tour that we had missed on the unfortunate 3rd day. So we got up early and went to Busan station. But it looks like the Eulsukdo tour was jinxed. When I called the Busan city tour help-desk (+82-51-464-9898), they told me that both the morning and afternoon tours were fully reserved. Anyways, we were already in the bus and went to Busan station hoping that someone might skip (like we had on our 3rd day). We reached the station just a few minutes before the departure only to find out that there was just 1 empty seat. Dejected, we tried to decide what to do next. We had 3 options:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Visit Beomeosa (&lt;a href='http://en.wikipedia.org/wiki/Beomeosa'&gt;[1&lt;/a&gt;], &lt;a href='http://koreantemples.com/?p%3D241'&gt;[2&lt;/a&gt;], &lt;a href='http://www.tripadvisor.com/Attraction_Review-g297884-d615908-Reviews-Beomeosa_Temple-Busan.html'&gt;[3&lt;/a&gt;]) temple by ourselves&lt;/li&gt;&lt;/li&gt;&lt;li&gt;Visit &lt;a href='http://www.busanhaps.com/article/history-hills-busans-gamcheon-culture-village'&gt;Gamcheon&lt;/a&gt; &lt;a href='http://www.tripadvisor.com/Attraction_Review-g297884-d3901349-Reviews-Gamcheon_Culture_Village-Busan.html'&gt;culture&lt;/a&gt; village by ourselves&lt;/li&gt;&lt;/li&gt;&lt;li&gt;Go back to hotel and come back for afternoon tour&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We decided to do the third because we did not want to wander out on our own with a baby in our hands. We visited the Haeundae beach on our way back to the hotel, where we saw a lot of &lt;a href='http://en.wikipedia.org/wiki/Black-legged_Kittiwake'&gt;black-legged seagulls&lt;/a&gt; up close&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11624357126/player/b40239cac8&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11623801213/player/8e465e2903&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;In the afternoon, we left early only to reach 40 mins before time for the Eulsukdo tour. This time our wait was fruitful because we were the 1st people to board the bus and got a good seat too. Unfortunately, there weren't any birds (other than ducks, which was not exciting at all) at the eco-center :(&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11623740303/player/363e907ca3&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;This marked the end of our tour, and the next day we traveled back from Busan to Suwon on the KTX. Although the train reached top speeds of 290+ kmph, it always did so when inside tunnels, so we weren't able to see outside. But we did notice our ears getting jammed, and could feel the speeds. Also, this time again, I tried to do the photography technique that I had mentioned in the day-1 post &lt;a href='http://www.golb.in/busan-week-day-1-haeundae-beach-20.html'&gt;here&lt;/a&gt;. This time I had more success, but none of the photos turned out to be good enough. Even then, it was a good learning experience.&lt;/p&gt;&lt;p&gt;I hope you all loved the travelogue (older posts are &lt;a href='http://www.golb.in/busan-week-day-1-haeundae-beach-20.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/busan-week-day-2-yeonggung-sa-21.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/busan-week-day-3-taejongdae-22.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/busan-week-day-4-busan-aquarium-and-shinsegae-mall-23.html'&gt;here&lt;/a&gt;)! Do share your thoughts :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 14 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-13-busan-week-day-4-busan-aquarium-and-shinsegae-mall.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-13-busan-week-day-4-busan-aquarium-and-shinsegae-mall.html
</link>
<title>
Busan week: day 4 -- Busan aquarium and Shinsaegae mall
</title>
<description>
&lt;p&gt;Today I wasn't in a mood to travel all the way to Busan station (it's about 35-50 mins from Haeundae beach, depending on the traffic; also the Haeundae -&gt; Busan (bus #1003) route is longer than the other way around). So we decided to do some local sightseeing, which included &lt;a href='http://www.busanaquarium.com/en/'&gt;Busan aquarium&lt;/a&gt; and &lt;a href='http://en.wikipedia.org/wiki/Shinsegae'&gt;Shinsegae&lt;/a&gt; mall.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;As I have mentioned &lt;a href='http://www.golb.in/busan-week-day-1-haeundae-beach-20.html'&gt;here&lt;/a&gt;, I was super excited to visit the Busan aquarium because it offered the once in a lifetime opportunity to swim with the sharks. Unfortunately, my hopes were shattered because when I inquired about it, I was sent from one desk to another and then asked to call some phone number where the man only spoke Korean. I spent 30 mins running from desk-to-desk, by which time my wife was pissed off because my daughter being difficult to be managed by her alone. We also did not get to sit in the glass-bottomed boat because my daughter was too young. The actual fishes weren't that exciting since the variety was almost the same as what we had already seen in &lt;a href='http://english.visitkorea.or.kr/enu/SI/SI_EN_3_1_1_1.jsp?cid%3D736274'&gt;COEX mall aquarium&lt;/a&gt; in &lt;a href='http://en.wikipedia.org/wiki/Seoul'&gt;Seoul&lt;/a&gt;. I did manage to get 1 macro image that I liked a lot&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11622415516/player/7a2993574a&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Finally, we got a cartoon-painting made of the 3 of us and went back to the hotel for lunch&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621846323/player/b24a375893&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;In the evening we visited Shinsegae which has its name in the Guinness book of world records as the worlds largest shopping mall. This time we traveled by the subway because&lt;/p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Haeundae subway station was pretty close from our hotel&lt;/li&gt;&lt;/li&gt;&lt;li&gt;Shinsegae is directly reachable from the &quot;City Centum&quot; subway station&lt;/li&gt;&lt;/li&gt;&lt;li&gt;City centum station is just 3 stations away from Haeundae station&lt;/li&gt;&lt;/li&gt;&lt;li&gt;We would be able to avoid the heavy evening traffic&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Although we had seen the mall from outside a lot of times now (because it is on the way when traveling Busan &lt;-&gt; Haeundae), this was the first time we were inside it. And although it was not immediately apparent how huge it was, it was fascinating to see an ice-rink inside it. I even gave ice-skating a try but gave up within just 20 minutes after falling hard twice ;)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;But the most weird and disappointingly surprising thing about the world's largest mall was that it closes at 8pm! Yes you heard it right, it shuts down at 8pm. This was completely unexpected for us, and very inconvenient too. Because we had to rush out and found ourselves trapped in the wrong part of the mall. The elevators would either take us to a road-side exit (we didn't know which side of the mall), or to a do-not-enter red-tape. Eventually, with a baby in my hands, I did what I had to do. Entered the no-entry zone, found my way to the part that I knew we had entered from (the part that was connected to the subway station) and took the elevator from there. All this crazy running took us about 15 minutes, just to come from the 7th floor to the basement. It was one of the weirdest experience ever!&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11622653554/player/3e3669eef3&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11623049916/player/c73456e243&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11622642884/player/0197de3a84&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11623046456/player/13d55a990c&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 13 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-12-busan-week-day-3-taejongdae.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-12-busan-week-day-3-taejongdae.html
</link>
<title>
Busan week: day 3 -- Taejongdae
</title>
<description>
&lt;p&gt;The first half of our third day (see the first 2 &lt;a href='http://www.golb.in/busan-week-day-1-haeundae-beach-20.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/busan-week-day-2-yeonggung-sa-21.html'&gt;here&lt;/a&gt;) in Busan was the lowest point of the whole week. We had a reservation for the &quot;&lt;a href='http://busan.for91days.com/2012/07/06/the-eulsukdo-island-bird-sanctuary/'&gt;Eulsukdo eco&lt;/a&gt;&quot; tour in the morning, but my daughter vomited just when we were about to reach Busan station. We had no choice but to head back to the hotel.&lt;/p&gt;&lt;p&gt;Once we were ready again, We took the looped city tour bus and visited &lt;a href='http://english.visitkorea.or.kr/enu/SI/SI_EN_3_1_1_1.jsp?cid%3D264167'&gt;Taejongdae&lt;/a&gt;. It is a hill which hosts the Taejongdae resort. This time, we took the Haeundae loop tour bus to go from Haeundae beach to busan and it passed via the famous &lt;a href='http://english.visitkorea.or.kr/enu/SI/SI_EN_3_1_1_1.jsp?cid%3D1064834'&gt;Gwangan bridge&lt;/a&gt;:&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621884866/player/3b9d96dfda&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621888096/player/29ba2542ac&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621887146/player/6b25b0f486&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;Unfortunately, we made the bad decision of getting down at the &quot;Busan art museum&quot; stop. The decision was bad because after getting down, we went in the wrong direction and spent about 15 minutes just searching for the place. We were already pissed off because of the bad start, so we came back to the bus stop and waited another 20 mins for the bus. But this time we were laughing at our stupidity, because the art museum was just 1 min away from the stop. Anyways, we weren't in a mood to go there after this. Fortunately, the same bus got converted into the Taejongdae loop tour bus. So we did not have to get down of the bus, but also were able to catch a good window seat ;) On the way to Taejongdae from Busan, we passed the 75-square stop (which we did not get down at). The sea-meets-the-sky view was simply amazing. Here are some ok-ok shots from inside the moving bus&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621065865/player/63d2263e5b&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621842166/player/24a8924eb8&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;On reaching Taejongdae we boarded the mini tram and saw the place; we did not get down from the tram at any of the stops that it made, including the light-house:&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621081583/player/4ae57fe096&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11620855015/player/54234f668b&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;From Taejongdae we took the loop tour bus back to Busan; but this time we got a bus with an open top. This was the first time for both my wife and me to sit in an open roof double-decker bus, and it was an amazing experience. On the way we passed &lt;a href='http://english.visitkorea.or.kr/enu/SI/SI_EN_3_1_1_1.jsp?cid%3D1054924'&gt;Songdo&lt;/a&gt; &lt;a href='http://www.tripadvisor.com/Attraction_Review-g297884-d3420565-Reviews-Songdo_Beach-Busan.html'&gt;beach&lt;/a&gt; and the world famous &lt;a href='http://www.tripadvisor.com/Attraction_Review-g297884-d1372972-Reviews-Jagalchi_Market-Busan.html'&gt;Jagalchi&lt;/a&gt; &lt;a href='http://english.visitkorea.or.kr/enu/SI/SI_EN_3_1_1_1.jsp?cid%3D264168'&gt;fish&lt;/a&gt; &lt;a href='http://en.wikipedia.org/wiki/Jagalchi_Market'&gt;market&lt;/a&gt;, but did not get down at either&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11622017636/player/01fa200002&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621997836/player/caff767910&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621196655/player/06f6030ca3&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621410203/player/aeffe8d89a&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621546594/player/006bc526a9&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 12 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-11-busan-week-day-2-yeonggung-sa.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-11-busan-week-day-2-yeonggung-sa.html
</link>
<title>
Busan week: day 2 -- YeongGungsa temple
</title>
<description>
&lt;p&gt;Before coming to Busan I had done some basic research about the places to visit in Busan. We had found that there are some city tours available. These tours were of 2 different types: looped and themed. The ticket for either was 10,000 won (approx. 10$). The difference between the 2 is that in the looped tour, there are buses that go around Busan in a circular fashion, and you can buy the ticket once (from the driver), and get in and down at any of the places (tourist spots) that the bus stops at. There is 1 bus every 30 mins (between 9.30am and 5pm starting from Busan station), so you can practically go to place A -&gt; visit it for as long as you wish -&gt; hop into the next bus and go to place B -&gt; and so on. This mode of travel is extremely convenient and cheap; especially if you like traveling on your own and don't need a guide (have done some basic homework of what places interest you and you want to visit). There are 2 different routes of the looped tours, called the &quot;&lt;a href='http://en.wikipedia.org/wiki/Haeundae_District'&gt;Haeundae&lt;/a&gt;&quot; and &quot;&lt;a href='http://en.wikipedia.org/wiki/Taejongdae'&gt;Taejongdae&lt;/a&gt;&quot; loops, and the same ticket works on both.&lt;/p&gt;&lt;p&gt;The 2nd type of city tour buses are the &quot;themed&quot; tours: &quot;History and Culture&quot;, &quot;&lt;a href='http://yongkungsa.or.kr/en/'&gt;Haedong YeongGung temple&lt;/a&gt;&quot;, &quot;&lt;a href='http://busan.for91days.com/2012/07/06/the-eulsukdo-island-bird-sanctuary/'&gt;Eulsukdo Eco&lt;/a&gt;&quot; and &quot;Night view&quot;. These tours are about 3hrs 40 mins each, and the buses leave twice a day; except for the &quot;Night view&quot; which is about 2.5hrs long, and runs only once in the evening. One good thing about themed Busan city tours is that you can call up their online help-desk (+82-51-464-9898) and do a reservation but actually purchase the ticket from the driver (at the start of the tour). The bad thing is that the help-desk (in my experience) is open only for about 2 hours between 9-11am in the morning; and some of the tours get full about 10 days in advance.&lt;/p&gt;&lt;p&gt;Anyways, we called up the help-desk in the morning, and were able to reserve 2 seats for the &quot;Haedong YeongGung temple&quot; tour. Since we reached a bit before the tour start time, I clicked a few photos in the Busan station area&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621680976/player/5547ccdaac&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;YeongGung-sa (sa means temple) was undoubtedly the best place that I visited in Busan. The temple is located on a cliff and the view is extremely serene and beautiful. It is the place that receives the 1st sun light in Korea.&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621397824/player/b3505ff0da&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621255003/player/a1206dcd7e&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621793556/player/145870c31a&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11620950435/player/436175ddb9&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621385194/player/37686de243&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621239643/player/b9813d77b8&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621367824/player/300795d223&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621365494/player/9b1e3a8d7e&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 11 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-10-busan-week-day-1-haeundae-beach.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-10-busan-week-day-1-haeundae-beach.html
</link>
<title>
Busan week: day 1 -- Haeundae beach
</title>
<description>
&lt;p&gt;First of all, let me wish you all a very happy and prosperous 2014! Second, let me pray for Golbin! to get bigger, both in terms of readers and storytellers :)&lt;/p&gt;&lt;p&gt;This and the next few of my stories are going to be about my week-long travel to &lt;a target=&quot;_blank&quot;href=&quot;http://en.wikipedia.org/wiki/Busan&quot;&gt;Busan&lt;/a&gt;, South Korea. I went there with my wife and 1 year old daughter in the week of Christmas (23-28 Dec), because of different reasons&lt;/p&gt;&lt;ul&gt;&lt;li&gt;23rd was my wife's birthday&lt;/li&gt;&lt;/li&gt;&lt;li&gt;I had holidays in my office on 24 and 25, so only 3 days of leaves need be taken&lt;/li&gt;&lt;/li&gt;&lt;li&gt;Busan, I was told, is much warmer (without any snow), but a lot windy, during this time of the year, than &lt;a target=&quot;_blank&quot;href=&quot;http://en.wikipedia.org/wiki/Suwon&quot;&gt;Suwon&lt;/a&gt; (where I reside)&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Anyways, so we decided to go to Busan for a week long vacation. We had to prepare for a long time, because this was the 1st time that we were going out for such a long time taking our baby (who is just 1 year old) with us. The preparation revolved mostly around baby clothes, for the windy weather, and home-made baby food. The only thing we had actually reserved before going to Busan were the hotel (we stayed at the &lt;a target=&quot;_blank&quot;href=&quot;http://www.rivierahotel.co.kr/EN/&quot;&gt;Riviera&lt;/a&gt; hotel, which was very close to Haeundae beach) and the to and fro train tickets. We did not want to book/reserve other things mostly because there was a huge amount of uncertainty regarding how my daughter would react to a new place. Fortunately, not only did she not give us much trouble, but she also enjoyed the place a lot&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11624256564/player/f8243639c2&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;Considering that we had our baby with us, we had set low expectations of travel in a day. If we could visit 1-2 places in 1 day, that would be good enough for us. That is also 1 reason for the week long tour, even though some of my (bachelor / newly-married) friends had said that just 1-2 days were enough to visit Busan.&lt;/p&gt;&lt;p&gt;We traveled from Suwon to Busan in the &lt;a target=&quot;&lt;i&gt;blank&quot;href=&quot;http://www.korail.com/en/&quot;&gt;KTX&lt;/a&gt; train, which runs at a max speed of about 280 kmph. The idea of traveling so fast itself was enough reason for me to visit Busan. The other major attraction being the Busan aquarium, about which I had read and heard a &lt;a target=&quot;&lt;/i&gt;blank&quot;href=&quot;http://www.tripadvisor.com/Attraction&lt;i&gt;Review-g297884-d669986-Reviews-Busan&lt;/i&gt;Aquarium-Busan.html&quot;&gt;few good things&lt;/a&gt;, and had especially liked their &lt;a target=&quot;&lt;i&gt;blank&quot;href=&quot;http://www.busanaquarium.com/en/&quot;&gt;website&lt;/a&gt;. I was also looking forward to the journey itself because I wanted to replicate the photography techniques that this guy (&lt;a target=&quot;&lt;/i&gt;blank&quot;href=&quot;http://www.qiranger.com/2011/06/10/suwon-to-busan/&quot;&gt;here&lt;/a&gt;) had used; my try was disastrous :(&lt;/p&gt;&lt;p&gt;We started from Suwon station on 23rd morning 10.50am, and reached Busan at around 1.40pm and to our hotel (bus #1003) in &lt;a target=&quot;&lt;i&gt;blank&quot;href=&quot;http://en.wikipedia.org/wiki/Haeundae&lt;/i&gt;District&quot;&gt;Haeundae&lt;/a&gt; at about 2.30pm. Then we rested a bit, had some lunch, and visited Haeundae beach in the evening. Although my wife loved the beautiful beach, we had to return soon to the warm and cozy hotel room because it was a bit too windy and chilly for my daughter to handle&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621602166/player/66efefa17a&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621601406/player/e76fa5d334&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621192974/player/0c3f21d1ce&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;500&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621191764/player/83e3b443b9&quot; webkitallowfullscreen=&quot;&quot; width=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;375&quot; mozallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; src=&quot;https://www.flickr.com/photos/spradnyesh/11621030403/player/e7a678e028&quot; webkitallowfullscreen=&quot;&quot; width=&quot;500&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;Finally, we ate dinner at &lt;a target=&quot;&lt;i&gt;blank&quot;href=&quot;http://www.tripadvisor.com/Restaurant&lt;/i&gt;Review-g297884-d4166622-Reviews-Namaste-Busan.html&quot;&gt;Namaste India&lt;/a&gt;; the food was very tasty. Since it was my wife's birthday, we had some ice-cream and called it a day!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 10 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-09-how-to-include-environment-variables-in-crontab?.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-09-how-to-include-environment-variables-in-crontab?.html
</link>
<title>
How to include environment variables in crontab
</title>
<description>
&lt;p&gt;As you all probably already know (from &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;), I am currently working on a project that needs me to collect data from various online resources. Since this data needs to be collected at regular intervals (due to updates happening to the data continuously), and not in just one go, I am using cronjob scripts to collect this data.&lt;/p&gt;&lt;p&gt;While writing this script I faced a lot of problems, and that is what I will discuss today. In the hope that someone out there will find something important from my mistakes and learnings, and that this post may be helpful in solving similar problems for him/her too.&lt;/p&gt;&lt;p&gt;As always, let me first start with some background. When developing the script, my desktop was behind a proxy. Although both curl and wget have inbuilt proxy support, both of them refused to work for me. With wget, i was getting the following error:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Connecting to 168.219.61.252:8080... connected.
Proxy request sent, awaiting response... 403 Forbidden
2013-12-18 13:39:36 ERROR 403: Forbidden.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;My first instinct was that the proxy is blocking the request. But I was able to download the file perfectly from browser (which also goes via the same proxy). I hit a dead end and did not know what to do next. So I searched for command line based download managers instead and found &lt;a href='http://aria2.sourceforge.net/'&gt;aria2&lt;/a&gt; that worked fine for me. YMMV.&lt;/p&gt;&lt;p&gt;Before going on, let me also mention that I had the following environment variables set&lt;/p&gt;&lt;pre&gt;&lt;code&gt;13:47:17 &amp;#126; $ env | grep proxy
http&amp;#95;proxy=http://168.219.61.252:8080/
ftp&amp;#95;proxy=http://168.219.61.252:8080/
all&amp;#95;proxy=socks://http://168.219.61.250:8080/
https&amp;#95;proxy=http://168.219.61.252:8080/
proxy=168.219.61.252:8080
no&amp;#95;proxy=localhost,127.0.0.0/8,::1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Back to the script now. The reason I needed a script (instead of just a simple wget/aria2 command inside crontab was simply because i needed to do some pre and post-processing to the downloaded content). Anyways, that's besides the point. So lets continue.&lt;/p&gt;&lt;p&gt;I was ready with the script and tested it extensively, both for the download part and post-processing, and ensured that things were working fine. Then i setup crontab as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;13:47:19 &amp;#126; $ crontab -l
# DO NOT EDIT THIS FILE - edit the master and reinstall.
# &amp;#40;/tmp/crontab.XXXX4oqDI8 installed on Wed Dec 11 19:42:26 2013&amp;#41;
# &amp;#40;Cron version V5.0 -- $Id: crontab.c,v 1.12 2004/01/23 18:56:42 vixie Exp $&amp;#41;
HOME=/home/spradnyesh 0 &amp;#42;/5 &amp;#42; &amp;#42; 1,2,3,4,5 $HOME/project/scripts/download-daily.sh 2&amp;amp;amp;gt;&amp;amp;amp;amp;1 &amp;amp;amp;gt; $HOME/dd.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I am sure you all know what the parameters of the cronjob line mean. But I will explain them briefly just for the sake of completeness:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;1. 0: run at 00 minutes
2. &amp;#42;/5: run every 5 hours
3. &amp;#42;: run on every day of the month
4. &amp;#42;: run in every month
5. 1,2,3,4,5: run only on weekdays &amp;#40;mon,tue,wed,thurs,fri&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Having explained that, lets move to the script itself. It looked something like this&lt;/p&gt;&lt;pre&gt;&lt;code&gt;#!/bin/bash

##### initial setup
cd $HOME/project/data/
toDate=`date +%F--%H`
prefix='http://www.example.com/'
suffix='&amp;amp;amp;amp;a=b'
outfile='download-daily.txt'

##### download data
rm $outfile
touch $outfile
for i in `cat ../list`
do
    echo &amp;quot;${prefix}${i}${suffix}&amp;quot; &amp;amp;amp;gt;&amp;amp;amp;gt; $outfile
    echo &amp;quot; out=${toDate}-${i}.html&amp;quot; &amp;amp;amp;gt;&amp;amp;amp;gt; $outfile
done
aria2c -j 10 --allow-overwrite=true --remove-control-file=true --remove-control-file=false -i $outfile

##### do some post-processing

git add $toDate.html; git commit -am &amp;quot;added data for $toDate&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The actual details of the script don't matter. Basically, I am simply doing some initial setup, downloading the data, doing post processing including saving the new file to git (read about my previous git experiences &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-4-16.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-5-17.html'&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;Unfortunately (&lt;a href='http://en.wikipedia.org/wiki/Murphy%2527s_law'&gt;Murphy's law&lt;/a&gt;), the script failed to download any data. On examining closely, I found that the $http&lt;i&gt;proxy environment variable was not visible inside the script. Researching online on this issue I learnt (to learn about the difference between &quot;learnt&quot; and &quot;learned&quot;, visit my previous story &lt;a href='http://www.golb.in/difference-between-learnt-and-learned-18.html'&gt;here&lt;/a&gt;) that the environment available to a script run via cron is very limited (mostly for security reasons), which is why the $http&lt;/i&gt;proxy variable was invisible inside the script.&lt;/p&gt;&lt;p&gt;To deal with this issue, i found a few solutions &lt;a href='http://stackoverflow.com/a/2229861'&gt;here&lt;/a&gt;, &lt;a href='http://stackoverflow.com/a/10657111'&gt;here&lt;/a&gt;, &lt;a href='http://stackoverflow.com/a/13579786'&gt;here&lt;/a&gt; (this one (and similarly &lt;a href='http://stackoverflow.com/a/18647100'&gt;this&lt;/a&gt; one and &lt;a href='http://stackoverflow.com/a/18647100'&gt;this&lt;/a&gt; one) is a big NO NO for security reasons, and also due to limitations as explained &lt;a href='http://stackoverflow.com/questions/5934554/crontab-source-file#'&gt;here&lt;/a&gt; and &lt;a href='http://stackoverflow.com/a/5935335'&gt;here&lt;/a&gt;), but none of them seemed to work for me. What eventually worked for me was setting up my cronjob rule as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;0 &amp;#42;/5 &amp;#42; &amp;#42; 1,2,3,4,5 env http&amp;#95;proxy=http://168.219.61.252:8080/ $HOME/project/scripts/download-daily.sh 2&amp;amp;amp;gt;&amp;amp;amp;amp;1 &amp;amp;amp;gt; $HOME/dd.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Do share your experiences with setting environment variables inside of cronjob scripts. Maybe you found some other (easier) way that worked for you. I would really love to hear about it!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 09 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-08-difference-between-learnt-and-learned.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-08-difference-between-learnt-and-learned.html
</link>
<title>
Difference between 'learned' and 'learnt'
</title>
<description>
&lt;p&gt;Hi there! Although I sincerely hope that you have been reading my recent git-week posts &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-4-16.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-5-17.html'&gt;here&lt;/a&gt;, frankly, I myself am happy that the week is over. No, no, don't misunderstand me. I am happy that I learned and was able to share these git gems with you all. But, I am currently undergoing some stress, due to&lt;/p&gt;&lt;ul&gt;&lt;li&gt;things not going well at work (read &lt;a href='http://www.golb.in/svn-unmerge-10.html'&gt;this&lt;/a&gt; to know why)&lt;/li&gt;&lt;li&gt;I am starting to feel the pressure of &quot;don't break the chain&quot; (read about my attempt at it &lt;a href='http://www.golb.in/dont-break-the-chain-12.html'&gt;here&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;Being a newbie to blogging, I am having some difficulty finding good ideas/matter to write about&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Anyways, I will do an honest attempt to not only not break the chain, but also to not give under-cooked material to read. Otherwise you might feel bored and go away. Now, we don't want that to happen, do we? But at the same time I will keep the topic for today light enough so that I am able to achieve my objectives&lt;/p&gt;&lt;ul&gt;&lt;li&gt;provide useful/entertaining information to you&lt;/li&gt;&lt;li&gt;improve my &lt;a href='http://en.wikipedia.org/wiki/Cognition'&gt;cognitive&lt;/a&gt; and writing skills&lt;/li&gt;&lt;li&gt;not break the chain&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So the topic for today is &quot;difference between learnt and learned&quot;. Before I go on to the formal differences, let me tell you my version of the story. I used to think that &quot;learnt&quot; was something that had happened very very long time ago, like &quot;I had learnt to ride a bicycle when I was 5 years old&quot;, whereas &quot;learned&quot; was something that happened very recently, or just now, like &quot;I just learned what this means&quot;. Also, I noticed (when saying sentences containing the words in my mind) that I would lean towards using &quot;learnt&quot; almost always instead of using &quot;learned&quot;.&lt;/p&gt;&lt;p&gt;Now I am not really sure if this is correct (along with the other definitions that I will show below) or not. What do you think? Probably you too thought similarly.&lt;/p&gt;&lt;p&gt;But 1 important question that I have not said anything about is &quot;why&quot;. As in, &quot;why&quot; did i get interested in the difference between &quot;learnt&quot; and &quot;learned&quot; in the first place. Why does it matter to me at all? The story behind this is that when i was writing the post about &lt;a href='http://www.golb.in/speed-your-website-100-times-11.html'&gt;speed your website 100x times&lt;/a&gt;, my editor showed the word &quot;learnt&quot; in yellow color grabbing my attention. I went into spell-check and the only option it showed was &quot;learned&quot;; &quot;learnt&quot; was not there at all in it's suggestions. This confused me because I knew for sure that &quot;learnt&quot; is a valid word. This piqued my curiosity enough for me to quickly search online for meanings and differences between the 2 words.&lt;/p&gt;&lt;p&gt;Anyways, coming back to the actual differences. Unexpectedly I found 2 &lt;i&gt;unrelated&lt;/i&gt; differences &lt;a href='http://www.tellmewhyfacts.com/2007/02/what-is-difference-between-learnt-and.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.differencebetween.net/language/difference-between-learned-and-learnt/'&gt;here&lt;/a&gt;. The first one says that they are the same words, but pronounced (and spelled) differently in British and American English. Well, that atleast explains&lt;/p&gt;&lt;ul&gt;&lt;li&gt;why I would lean towards using &quot;learnt&quot; instead of using &quot;learned&quot; almost every time (since I am Indian and learn British English. Actually what Indians use is our very own dialect of English, if I may call it that. But lets be content by saying that since India was ruled by the British for more than 150 years, what we use is British English)&lt;/li&gt;&lt;li&gt;why my editor did not show &quot;learnt&quot; in its suggestions; because I am using the American (en-US) language pack for spelling check/correction&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The second definition &lt;a href='http://www.differencebetween.net/language/difference-between-learned-and-learnt/'&gt;here&lt;/a&gt; mentioned &quot;learned&quot; as an adjective, like &quot;he is a very learned man&quot;, whereas &quot;learnt&quot; as a verb, like &quot;he has learnt many languages&quot;. Looking back, I remember having learnt something like this when I was in school. But I have never been very good with languages (and history and geography and ... (the list goes on, heh heh!)). But hey, I did manage to use the correct word in my previous sentence. Does that mean that I am a learned man now?&lt;/p&gt;&lt;p&gt;What do you think?&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 08 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-07-git-week-part-5.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-07-git-week-part-5.html
</link>
<title>
Git week: part - 5
</title>
<description>
&lt;p&gt;Hello again. Welcome to the 5th post in the &quot;git week&quot; series; visit the previous 4 posts &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-4-16.html'&gt;here&lt;/a&gt;. Today I will share another extremely helpful git feature with you all: &quot;git rebase -i&quot;.&lt;/p&gt;&lt;p&gt;If you remember, We have already used git-rebase once in the &lt;a href='http://www.golb.in/svn-unmerge-10.html'&gt;svn unmerge&lt;/a&gt; post. In that post I already showed how one could use git-rebase to easily handle things that are either too difficult or, at times, impossible to do in SVN. Today we will see an advanced use of git-rebase, that is &quot;git rebase -i&quot;. This command allows us to reorder commits. Yes, you read that right! One can actually reorder commits in git. Ain't that cool?&lt;/p&gt;&lt;p&gt;One can easily imagine certain obvious uses of this feature. For example, lets say I always push to central repository from my master branch, but locally use feature branches. Also, I may be working in parallel on 1 or more features and may also fixing bugs directly in the master branch. And I may be merging (after feature completion) from some feature branch into master. In this case, I would want to reorder the commits in a more meaningful order (one that makes more sense and shows a well thought flow to an outsider) on the master before pushing it to the central repository.&lt;/p&gt;&lt;p&gt;In fact, this use case happened to me last week. I was working on a certain feature, and had merged the incomplete feature to master (I know, bad boy ;) ); but, fortunately, not pushed it. Then I noticed a critical bug in the production system which had to be fixed immediately. So now this bug fix is at HEAD of master, and the unfinished feature is at &quot;HEAD~1&quot; location. Obviously I cannot push this state to production. This is where &quot;git rebase -i&quot; saves the day!&lt;/p&gt;&lt;p&gt;I do the following&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git rebase -i HEAD&amp;#126;1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here, git opens up an editor asking you to change the order of the commits (newest is at bottom). So I swap the last and one-but-last commit and save and quit the editor. Then I did&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;to see that the last 2 commits (incomplete-feature and critical-bug-fix) placed in the correct (as desired) order! Now I pushed the critical-bug-fix to central repository and created a build and deployed to my production server. This time, I took the care to not push the HEAD of master, but HEAD~1 to central repository.&lt;/p&gt;&lt;p&gt;As an added bonus, I am going to tell you a special feature of &quot;git rebase -i&quot;. That is once git opens the editor for you to reorder your commits, you will notice the following statements at the end of the editor&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Rebase 3a40bd2..ba8573d onto 3a40bd2
#
# Commands:
#  p, pick = use commit
#  r, reword = use commit, but edit the commit message
#  e, edit = use commit, but stop for amending
#  s, squash = use commit, but meld into previous commit
#  f, fixup = like &amp;quot;squash&amp;quot;, but discard this commit's log message
#  x, exec = run command &amp;#40;the rest of the line&amp;#41; using shell
#
# These lines can be re-ordered; they are executed from top to bottom.
#
# If you remove a line here THAT COMMIT WILL BE LOST.
#
# However, if you remove everything, the rebase will be aborted.
#
# Note that empty commits are commented out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you read these statements carefully, you will notice that &quot;git rebase -i&quot; is allowing you to not only reorder your commits, but to delete, edit, etc your existing commits. This is awesome knowledge and will surely come in handy in the future. Yay!&lt;/p&gt;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Do remember, that this command makes very low level modifications to your repository and you can easily screw things up. So you should always make changes like these to a cloned repository. Also, as always, do NOT make these changes to the part of history that you have already published.&lt;/p&gt;&lt;p&gt;I am going to keep this git-week series like a 5 day work-week and end this series here. I hope you found this (and previous posts &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-4-16.html'&gt;here&lt;/a&gt;) useful.&lt;/p&gt;&lt;p&gt;Do let me know if they did, and how. Also let me know if you like my writing and would like me to write more such articles on other topics as well. Your comments will give me the confidence and motivation to &quot;not break the chain&quot; (see my previous article &lt;a href='http://www.golb.in/dont-break-the-chain-12.html'&gt;don't break the chain&lt;/a&gt;)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 07 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2014-01-06-git-week-part-4.html
</guid>
<link>
http://spradnyesh.github.io/posts/2014-01-06-git-week-part-4.html
</link>
<title>
Git week: part - 4
</title>
<description>
&lt;p&gt;Hello again. Welcome back to the 4th post in the &quot;git week&quot; series; visit the previous 3 &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt;. Today I will share another extremely helpful git feature with you all: &quot;git subtree&quot;.&lt;/p&gt;&lt;p&gt;But before i begin with the actual feature, let me give you the background story about how I came to discover it. I find it useful to explain something by first explaining the situation where I needed to use it. I hope that you find it helpful too :)&lt;/p&gt;&lt;p&gt;Anyways, back to the background story. As I have hinted in my previous stories, I am working on a &lt;a href='http://en.wiktionary.org/wiki/pet_project'&gt;pet-project&lt;/a&gt; wherein i have to collect a lot of data. Currently I am storing these data files locally in the project git repository itself. However, since I collect this data from multiple machines, I get some pull conflicts. Note that I say &quot;pull&quot; conflicts, not &quot;merge&quot; conflicts. This is because, obviously, not the same file is updated from all these local repositories at the same time. But i mostly get &quot;non-fast-forward&quot; errors.&lt;/p&gt;&lt;p&gt;I think that this can be (almost) solved by first doing a pull before doing a commit, followed immediately by a push. Also, once I setup the cron jobs running on these different machines to run at non-overlapping times, I can remove these conflicts altogether.&lt;/p&gt;&lt;p&gt;However, my issue is not that. My issue is that since my code and data resides in the same git repository, and I don't pull/push my code changes from/to the central repository (due to reasons mentioned in the &quot;&lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;git week, part 1&lt;/a&gt;&quot; story), I end up facing these &quot;non-fast-forward&quot; errors anyways. The obvious solution that comes to mind is to manage the source code and data in 2 separate repositories. Since the data repositories are setup in a sync-ed fashion (as mentioned in the previous paragraph), and the source repository will be changed explicitly by me (in a non automated manner (using cron jobs, for example), I can explicitly push/pull the changes to/from the central repository (even if I change the code from multiple locations); that is by following the steps mentioned in &quot;&lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;git week, part 1&lt;/a&gt;&quot; story. This way I will get rid of all these &quot;non-fast-forward&quot; errors that git has been throwing at me previously. Yay!&lt;/p&gt;&lt;p&gt;Ok, now to get to the technical part. How to split the original repository into 2, one for the data and the other for the source. Before I proceed to that, I am going to describe my source folder setup.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#126;/my-projects/
    - project-A/
        - config/
        - src/
            - model/
            - view/
            - controller
        - data/
            - db/
            - static/
                - css/
                - js/
                - images/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I am sure that you would have guessed the project to be a web-based project following the MVC architecture. I would guess that your project directory structure would be somewhat similar too. Our objective in this exercise is to split out the project-A/data/db (let's call this &lt;new-repo&gt;) out from the current project repository (let's call this &lt;big-repo&gt;). Note, that the important goal of this exercise is to get the history of &lt;new-repo&gt; changes into the new repository. Simply doing the following&lt;/p&gt;&lt;pre&gt;&lt;code&gt;mv &amp;lt;new-repo&amp;gt; &amp;lt;big-repo&amp;gt;/..
git rm -rf &amp;lt;new-repo&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;would be easy to do, but would not achieve anything, because we would have lost all the history. Fortunately, it turns out that this operation, of splitting &lt;new-repo&gt; out of &lt;big-repo&gt; is so common, that git has introduced a 1-step procedure to achieve our goal. The steps are as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;cd &amp;#126;/my-projects/project-A
git subtree split -p data/db -b project-A-db
cd &amp;#126;/my-projects/
mkdir project-A-db
cd project-A-db
git init
git pull &amp;#126;/my-projects/project-A/ project-A-db
cd &amp;#126;/my-projects/project-A
git rm -rf data/db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above steps apply to my directory setup. But, generically, we would do the following steps:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;cd &amp;lt;big-repo&amp;gt;
git subtree split -p &amp;lt;name-of-data-folder&amp;gt; -b &amp;lt;new-branch-name&amp;gt;
cd &amp;lt;big-repo&amp;gt;/..
mkdir &amp;lt;new-repo&amp;gt;
cd &amp;lt;new-repo&amp;gt;
git init
git pull &amp;lt;path-to-big-repo&amp;gt; &amp;lt;new-branch-name&amp;gt;
cd &amp;lt;big-repo&amp;gt;
git rm -rf &amp;lt;name-of-data-folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The most important step of this whole flow, which I am sure you must have guessed by now, is the &quot;git subtree&quot; command. This is the one that does all the magic!&lt;/p&gt;&lt;p&gt;For more details of how the &quot;git subtree&quot; command works, please see &lt;a href='http://stackoverflow.com/questions/359424/detach-subdirectory-into-separate-git-repository/17864475#17864475'&gt;this&lt;/a&gt; and &lt;a href='http://blog.charlescy.com/blog/2013/08/17/git-subtree-tutorial/'&gt;this&lt;/a&gt;&lt;/p&gt;&lt;p&gt;I hope that you have found this story as fun and informative to read as I have found it to write. Do share your thoughts about both git and my writing :)&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 06 Jan 2014 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-21-git-week-part-3.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-21-git-week-part-3.html
</link>
<title>
Git week: part - 3
</title>
<description>
&lt;p&gt;Hi there. Welcome back to the 3rd post in the &quot;git week&quot; series; visit the previous 2 &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;. Today I will share another extremely helpful git feature with you all: &quot;git bisect&quot;.&lt;/p&gt;&lt;p&gt;I am sure you, like me, have faced situations when a feature, which you were sure was working previously, is found to have stopped working correctly. Obviously something changed in the recent past that broke this feature. If we have automated tests, one of which caught this breakage, then it should be pretty easy to fix. But what we are interested in, in this post, is not &quot;how&quot; to fix, but to understand &quot;what&quot; broke this feature, and &quot;when&quot;. Maybe, if you are like my manager, you are also looking out for the &quot;who&quot;, to put some blame on.&lt;/p&gt;&lt;p&gt;Anyways, coming back to the issue at hand. So basically, we want to find out that particular fix which broke this feature. This is surprisingly easy to do with &quot;git-bisect&quot;. I am sure, that by this time, based on my description above, and based on the name &quot;bisect&quot;, you would have guessed what this git feature really does. Well, if you have guessed that this feature will do some kind of a &quot;binary search&quot; or a &quot;divide-and-conquer&quot; strategy to find the bad commit, you are absolutely correct.&lt;/p&gt;&lt;p&gt;So how this works is that you find 2 commit hashes, G and B, which stand for &quot;good&quot; and &quot;bad&quot; respectively. Basically, G is where you know that the feature was working correctly (maybe a commit from a few days ago, or from your last QA certified stable build), and B is where the feature is not working (maybe as simple as the HEAD).&lt;/p&gt;&lt;p&gt;The steps are as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git bisect start
git bisect bad &amp;lt;B&amp;gt;
git bisect good &amp;lt;G&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once you have given atleast 1 good and 1 bad commit hashes, git goes to work. It will successively give you the middle commit between good and bad and ask you to test. Once git has checked out a particular commit, you should (depending on your development environment) build, deploy and test. Then you know whether this particular commit (the one that git gave you just now) is good or bad. That is whether the feature you are interested in works correctly in this commit or not. Accordingly, you need to tell this to git as follows&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git bisect good
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;OR&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git bisect bad
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;respectively.&lt;/p&gt;&lt;p&gt;Continuing this binary search algorithm, you would have eventually reached the particular commit that introduced the bug. Now you can analyze what went wrong and why and come out of the exercise a learned man :)&lt;/p&gt;&lt;p&gt;After you have finished this bisection exercise, run the following command&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git bisect reset
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;to come out of the bisect session back to where you started from.&lt;/p&gt;&lt;p&gt;One interesting situation you can also use git bisect is the other side of the coin. That is when you knew that there was a (hopefully minor) bug that was never getting fixed for a long time (because it was minor in priority, but was (maybe) difficult to fix) and you find out suddenly that it has been fixed (as a side effect of some recent commit). You can repeat the above exercise in this scenario to find out how the bug got fixed. And be an even more learned man ;)&lt;/p&gt;&lt;p&gt;For more details (advanced usage of git-bisect), RTFM and/or visit &lt;a href='http://git-scm.com/docs/git-bisect'&gt;here&lt;/a&gt;. Do share with us about if you have used git bisect before and how did it help you&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 21 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-19-git-week-part-2.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-19-git-week-part-2.html
</link>
<title>
Git week: part - 2
</title>
<description>
&lt;p&gt;Hello again! Today we will see some more git magic. (Click &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt; to see my previous post on git.)&lt;/p&gt;&lt;p&gt;This time we are going to learn about the &quot;filter-branch&quot; functionality which proved immensely helpful to me in 2 different scenarios, both unrelated, but both of which happened yesterday at different times of the day.&lt;/p&gt;&lt;p&gt;The 1st was that i use 2 different aliases for different projects. Unfortunately, in one recently started project, I used the default (wrong, in this project) user.name/user.email combination. Fortunately, I had not published it into any online repository (see my previous post as to why ;)). So I still had the opportunity to change this information. I knew that it could be done, but did not know how. So I learned it and will share it with you:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git filter-branch --commit-filter 'GIT&amp;#95;COMMITTER&amp;#95;NAME=&amp;quot;&amp;lt;new-name&amp;gt;&amp;quot;; GIT&amp;#95;AUTHOR&amp;#95;NAME=&amp;quot;&amp;lt;new-name&amp;gt;&amp;quot;; GIT&amp;#95;COMMITTER&amp;#95;EMAIL=&amp;quot;&amp;lt;new-email&amp;gt;&amp;quot;; GIT&amp;#95;AUTHOR&amp;#95;EMAIL=&amp;quot;&amp;lt;new-email&amp;gt;&amp;quot;; git commit-tree &amp;quot;$@&amp;quot;;' HEAD
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Of course, your case might not be as simple. That is, unlike me, you might need to change based on some condition, or only change a few particular commits. You can find a good example &lt;a href='http://stackoverflow.com/a/870367'&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The second case where filter-branch turned out to be extremely life-saving was in the same recently started project. I have been scraping some info from the Internet and storing these (separate) files in the local repository. It was big data; largest that I have worked on personally, growing at a rate of about 23Mb everyday. Now it turned out that if I would concatenate the contents of these files into a single file then it would keep my directory structure more manageable and also better for the analysis tool that I would use.&lt;/p&gt;&lt;p&gt;But the problem was that I had already scraped data for 3 days (70Mb of data), and even if I removed the original files, they would still be in the history. This would mean that my repository would simple double the moment I moved to the new file format. The solution was to remove these files (I did not really need them in the history since I was still retaining the &quot;data&quot; in the new format) from history. This is where &quot;filter-branch&quot; feature of git came in handy again. The commands to achieve this are:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git filter-branch --index-filter &amp;quot;git rm -rf --cached --ignore-unmatch $files&amp;quot; HEAD
rm -rf .git/refs/original/ &amp;amp;&amp;amp; git reflog expire --all &amp;amp;&amp;amp;&amp;amp;amp;nbsp; git gc --aggressive --prune&amp;lt;/blockquote&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Fortunately, even though I had hundreds of (small) files, I had created the filenames in the &lt;date-in-yyyymmdd-format&gt;-&lt;source&gt; format, so it was very easy for me to invoke the commands like so:&lt;/source&gt;&lt;/date-in-yyyymmdd-format&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git filter-branch --index-filter &amp;quot;git rm -rf --cached --ignore-unmatch &amp;lt;yyyymm&amp;gt;-&amp;#42;&amp;quot; HEAD
rm -rf .git/refs/original/ &amp;amp;&amp;amp; git reflog expire --all &amp;amp;&amp;amp;&amp;amp;amp;nbsp; git gc --aggressive --prune&amp;lt;/yyyymm&amp;gt;&amp;lt;/blockquote&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The 2nd &quot;rm -rf&quot; command is needed to remove temporary files that git maintains for a long time even after asking it to clear everything.&lt;/p&gt;&lt;p&gt;With this technique, I was able to reduce the repository size (after adding the new format files) to about 60%, a good saving :)&lt;/p&gt;&lt;p&gt;Remember, both the above actions are destructive and may mess up your repository, so it's best to try them out on a clone first. Also, do NOT do them if you have already published your changes to prevent others from facing non-fast-forward pull errors.&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 19 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-18-git-week-part-1.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-18-git-week-part-1.html
</link>
<title>
Git week: part - 1
</title>
<description>
&lt;p&gt;&lt;a href='http://www.git-scm.com/'&gt;git&lt;/a&gt; rocks! I have been using git for all my personal projects for at-least the last 2 years now. But what I have learned in this week is probably more than what I have learned in the last few months, and I am dying to share them with you!&lt;/p&gt;&lt;p&gt;&lt;strong&gt;git-bundle&lt;/strong&gt;: Honestly, I learned this maybe in the last month, or the month before that. But then, I wasn't blogging actively at that time. So what the heck? I will share it with you as if I have learned it in this week ;)&lt;/p&gt;&lt;p&gt;Anyways, coming to the real issue that &quot;git-bundle&quot; solves. I work in a jail environment; well not really, but what's the fun in something if there is not even a little hint of exaggeration? The point is that we are not allowed access to git hosting sites like &lt;a href='https://github.com/'&gt;github&lt;/a&gt;, &lt;a href='https://bitbucket.org/'&gt;bitbucket&lt;/a&gt;, etc. So I was having trouble moving my personal projects (that I work on during my free/slack (see my article &quot;don't break the chain&quot; to see how to make productive use of slack time) time between home and office without access to a central repository to sync between my home and office repositories. I searched for other github-like sites (even paid) that I might have access to from my jail, but without success. I was very frustrated.&lt;/p&gt;&lt;p&gt;And then, instead of looking outward, I looked inward (wow, that came out very philosophical!). That is, instead of finding websites that could help me, I searched for whether git has any functionality that could liberate me. And voila, the thing that I found, &quot;git-bundle&quot;, was exactly what I was searching for.&lt;/p&gt;&lt;p&gt;That's enough of history. Lets get to the details of how git-bundle helped me. For explaining the situation however, we need to set up some background first. Let's say the 2 repositories (home and office, in my case) that we want to keep in sync are A and B. We will cover 2 cases. case-1 is where we have created a new repository (A), and where B has not been setup yet. case-2 is where both A and B exist already (say before access to github was removed), and you have now made changes to one of them, say A. Lets start with the 1st case first. Perform the following steps at A:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;cd &amp;lt;repo&amp;gt;
git bundle create &amp;lt;repo-name&amp;gt;.bdl master
git tag -f &amp;quot;bdl&amp;quot; master
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that the &quot;.bdl&quot; extension is completely arbitrary, and git does not put any restrictions on it. Similarly you can decide on what tag you want to use. Tagging is useful when sending incremental patches (case-2), as we will show below.&lt;/p&gt;&lt;p&gt;Back to case-1. Transfer the &lt;repo-name&gt;.bdl file to B (via email, etc) and perform the following steps at B:&lt;/repo-name&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git clone -b master &amp;lt;repo-name&amp;gt;.bdl &amp;lt;repo&amp;gt;&amp;lt;repo-name&amp;gt;&amp;lt;repo&amp;gt;&amp;lt;/repo&amp;gt;&amp;lt;/repo-name&amp;gt;
git tag -f &amp;quot;bdl&amp;quot; master
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the initial setup is done, we simplify our life to be the same as case-2, that is, incremental sync. For this case, lets assume that you have made some changes in B and want to move them to A. Perform the following steps at B:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git bundle create &amp;lt;repo-name&amp;gt;-&amp;lt;date&amp;gt;.bdl bdl..master&amp;lt;repo-name&amp;gt;&amp;lt;date&amp;gt;&amp;lt;/date&amp;gt;&amp;lt;/repo-name&amp;gt;
git tag -f &amp;quot;bdl&amp;quot; master
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Again, the &lt;repo-name&gt;&lt;repo-name&gt;-&lt;date&gt;&lt;date&gt;.bdl filename is completely arbitrary. I like putting the date part in there because that helps me avoid confusion when I am emailing multiple bundles everyday between my office and home. The part where we say &quot;bdl..master&quot; is the &quot;tagging&quot; part that i had mentioned earlier. Basically, what this tells git-bundle is to bundle only those commits that happened between when we had cloned (or &quot;pull&quot;-ed as explained below) earlier and where the HEAD of the master branch is now.&lt;/date&gt;&lt;/repo-name&gt;&lt;/p&gt;&lt;p&gt;Then, we move the bundle file from B to A, and perform the following steps at A:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;git pull &amp;lt;repo-name&amp;gt;-&amp;lt;date&amp;gt;.bdl master:master&amp;lt;repo-name&amp;gt;&amp;lt;date&amp;gt;&amp;lt;/date&amp;gt;&amp;lt;/repo-name&amp;gt;
git tag -f &amp;quot;bdl&amp;quot; master
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The steps are exactly the same when you want to sync from B to A. Now, lather, rinse, repeat!&lt;/p&gt;&lt;p&gt;ps:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Read the beautifully written man page &quot;man git-bundle&quot; for more info&lt;/li&gt;&lt;li&gt;Looks like this has become a long post. Well I will write about the other git magic stuff in the next post!&lt;/li&gt;&lt;/ul&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 18 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-17-more-exciting-new-features-for-golbin.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-17-more-exciting-new-features-for-golbin.html
</link>
<title>
More exciting and new features for Golbin!
</title>
<description>
&lt;p&gt;Hello. Welcome back! I am sure that you are as excited to find out about the new features that we have added to our website this week as we are to tell you about it. As mentioned &lt;a href='http://www.golb.in/exciting-new-features-on-golbin-9.html'&gt;here&lt;/a&gt; previously, we will try our best to continuously add small and big (both user-facing and not) features to our &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt; website. So let's review what's new:&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RSS&lt;/strong&gt;: We have added a &lt;a href='http://en.wikipedia.org/wiki/RSS'&gt;RSS&lt;/a&gt; stream for all our index pages; that is for the &lt;a href='http://www.golb.in/'&gt;home&lt;/a&gt; page, and also for all category/sub-category (click &lt;a href='http://www.golb.in/category/lifestyle/'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/category/technology/internet/'&gt;here&lt;/a&gt; for example) and author pages (like &lt;a href='http://www.golb.in/author/spradnyesh/'&gt;this&lt;/a&gt; for example). So now viewers can access &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt; stories from your favorite feed reader. To use this functionality, simple search for and click the &quot;RSS&quot; link present at the bottom of any of our index pages.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;lazyload Google ads&lt;/strong&gt;: If you have noticed earlier, google-ads on &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt; pages have always been loading in a lazy fashion (that is, after the whole page has been loaded). But earlier, we were using the awesome &lt;a href='http://jqueryad.web2ajax.fr/'&gt;jQuery lazyload Ad&lt;/a&gt; plugin. While, it was doing an excellent job, it was adding about 8.5K of javascript (and 1 extra http get request) to the website. But, after google introduced the async form of &lt;a href='https://support.google.com/adsense/answer/3221666?hl%3Den'&gt;rendering ads&lt;/a&gt;, we implemented the same. This gives us lighter and faster loading web-pages. We believe that this will improve viewer experience on &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;story page backgrounds&lt;/strong&gt;: In our last &lt;a href='http://www.golb.in/exciting-new-features-on-golbin-9.html'&gt;post&lt;/a&gt;, we mentioned that story-tellers now have the ability to provide a custom image as background to all their pages. However, the functionality was limited; because they could only upload 1 image which would apply to all of their pages (story-teller index, and story pages). But now story tellers can have a different background image for each individual stories. If they do not provide an image for a new story but have defined a default image, then the new story will pick up the default image.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;As always, we would be delighted to hear from you about what features you would like to be added to our site! Also, don't feel shy to go ahead, register, and share your stories with the world! Click here to see the benefits of writing on &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 17 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-16-producer-consumer-problem-revisited.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-16-producer-consumer-problem-revisited.html
</link>
<title>
Producer-consumer problem, revisited
</title>
<description>
&lt;p&gt;Anyone who has learnt basic &quot;operating system&quot; concepts in their Computer Science engineering knows about the &lt;a href='http://en.wikipedia.org/wiki/Producer-consumer_problem'&gt;producer consumer problem&lt;/a&gt;. It is a classic synchronization problem, where there are 2 processes, the &quot;producer&quot; and the &quot;consumer&quot;, both sharing a fixed size common buffer. The producer produces something that is consumed by the consumer. The problem is to make sure that the producer does not try to add to an already full buffer and that the consumer does not try to remove from an empty buffer. But today I am not going to discuss this problem from a Computer Science perspective, but from a completely different angle.&lt;/p&gt;&lt;p&gt;I had mentioned in one of my previous articles, &lt;a href='http://www.golb.in/dont-break-the-chain-12.html'&gt;here&lt;/a&gt;, I am trying to write 1 new story every day, or atleast every weekday. Now you might ask what has that to do with the producer-consumer problem. Well, let me explain.&lt;/p&gt;&lt;p&gt;Before I started writing stories everyday, I was a pure consumer. I would only consume posts/stories/articles, really data, from different sources (blogs, forums, email, social-networking sites, etc). But now I am slowly making a shift towards being a producer as well. Obviously, one cannot become a pure producer &amp;ndash; one who only produces (articles/data) without consuming (articles/data), because for the human brain to perform it's magic, it needs a lot of input. A creative mind is one that takes to (apparently) unrelated things, and combines them in such a way as to create something new and coherent out of it.&lt;/p&gt;&lt;p&gt;Having laid out the background of &quot;producer&quot; and &quot;consumer&quot; in my story, let me now get to the &quot;problem&quot; part. Well, I notice that as I am making the move from being a consumer to trying to become a producer, I am falling short of things to write about. There are days, like when I was writing the &quot;git week&quot; series (see &lt;a href='http://www.golb.in/git-week-part-1-13.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-2-14.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-3-15.html'&gt;here&lt;/a&gt;, &lt;a href='http://www.golb.in/git-week-part-4-16.html'&gt;here&lt;/a&gt; and &lt;a href='http://www.golb.in/git-week-part-5-17.html'&gt;here&lt;/a&gt;) when writing was easy. But once the series concluded, I am finding it difficult to find new topics to write on.&lt;/p&gt;&lt;p&gt;I have noticed that I feel comfortable writing about technical things (eg, git-week series, &lt;a href='http://www.golb.in/svn-unmerge-10.html'&gt;svn-unmerge&lt;/a&gt;, &lt;a href='http://www.golb.in/how-to-include-environment-variables-in-crontab-19.html'&gt;environment variables in crontab&lt;/a&gt;, etc), than non-technical (&lt;a href='http://www.golb.in/difference-between-learnt-and-learned-18.html'&gt;different between learnt and learned&lt;/a&gt;, &lt;a href='http://www.golb.in/dont-break-the-chain-12.html'&gt;don't break the chain&lt;/a&gt;, etc). But sometimes, when I have written part of a post, I feel like either I am not able to reach to a good ending, or like the post is not feeling coherent enough. At such times, I tend to give up much more easily on the post, rather than struggling to complete it. My mind seems to be eager to jump off to a different topic, leaving an unfinished mess behind. And this is the &quot;problem&quot;!&lt;/p&gt;&lt;p&gt;Although I have done some homework on this subject and read techniques to maintain steady writing, it is easier said than done. I think the only way to solve this problem is to have a plan and keep it simple:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Don't break the chain&lt;/li&gt;&lt;li&gt;Practice makes perfect&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I believe that the 1st rule &quot;don't break the chain&quot; will take me a long way till the gates of the 1st part of the 2nd rule &quot;practice&quot;, which will eventually take me to the 2nd part &quot;perfection&quot;. What say you?&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 16 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-15-dont-break-the-chain.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-15-dont-break-the-chain.html
</link>
<title>
Don't break the chain
</title>
<description>
&lt;p&gt;Lately i have been having streaks of unproductive days. Days when i don't feel like doing anything. I think it's OK and normal to have a day or 2 like that, but when it becomes day&quot;s&quot;, then one starts to worry. Work starts to pile up and frustration starts to build. Things go from normal to bad to worse. The mounting frustration does little to help getting out of this situation.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Don't get me wrong, I'm not lazy. Quite on the contrary, I'm a &lt;a href='http://en.wikipedia.org/wiki/Getting_Things_Done'&gt;GTD&lt;/a&gt; fan. I use &lt;a href='http://www.gnu.org/s/emacs/'&gt;emacs&lt;/a&gt; and the super-excellent &lt;a href='http://orgmode.org/'&gt;org-mode&lt;/a&gt; to maintain both notes and tasks. In fact, org-mode was one big reason why i gave up my &quot;Doc-vim&quot; status and entered the life of emacs. Anyways, that's a story for a different day ;)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Coming back on track, so here I am, in the midst of such a streak when I stumbled upon &quot;don't break the chain&quot; approach to productivity. Earlier i have read about (and given a try) to methods like &lt;a href='http://pomodorotechnique.com/'&gt;pomodoro technique&lt;/a&gt;, etc. But they did not work for me.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Of course, i understand and agree that pomodoro technique is more for time-management and not motivation to get out of a slack phase. It works best when i have too much work to do but I am getting distracted and/or interrupted too often. It allows me to have uninterrupted time-slots for tasks, where i should be able to focus and concentrate without any distractions.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;But, we should also realize that it's not possible to be super busy 24x7. There will be times when one does not have too much work. Of course, these days are there to allow one to recuperate, regain that lost energy (from the last sprint), store some extra energy (for the next sprint). Also, these days are excellent times to learn something new and expand our knowledge base, or get involved in some hobby &amp;ndash; maybe something that you've wanted to do for a long time but never got the time to get started.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Good! I am in a slack period and have a lot of time. But there is no motivation. I don't feel like doing anything. I'm sure you have felt this too, at some time or the other. So what to do? Enter &quot;&lt;a href='http://lifehacker.com/5886128/how-seinfelds-productivity-secret-fixed-my-procrastination-problem'&gt;don't break the chain&lt;/a&gt;&quot;. I recently stumbled over it &lt;a href='http://lifehacker.com/5939526/lift-for-iphone-keeps-you-on-track-helps-you-master-seinfelds-productivity-method'&gt;here&lt;/a&gt;. I must agree that i liked it immediately and decided to try it out. I'm happy to report that it has worked fascinatingly for me till now, although it's been just a few days.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;In fact, maybe you have noticed that i have started writing articles almost everyday now here on &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt;. An I have also started doing some other stuff, including some hobby work. I think that this method is excellent while the slack period continues. After that I will move back to GTD :) To learn about more such methods, click &lt;a href='http://lifehacker.com/5890129/five-best-productivity-methods'&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Do you have productivity techniques that work well for you? I will be happy to learn about them, so do share!&lt;/p&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 15 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-13-svn-unmerge.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-13-svn-unmerge.html
</link>
<title>
SVN unmerge
</title>
<description>
&lt;p&gt;I recently came across the following scenario in my work-place. We use &lt;a href='http://en.wikipedia.org/wiki/Apache_Subversion'&gt;SVN&lt;/a&gt; as our &lt;a href='http://en.wikipedia.org/wiki/Revision_control'&gt;version control&lt;/a&gt; repository, and use a &lt;a href='http://en.wikipedia.org/wiki/Trunk_(software'&gt;trunk&lt;/a&gt;) for production pushes and feature branch for feature development. However, this time we made the mistake of merging feature-branch into trunk-prematurely; because the client team wanted to test both trunk features/bug-fixes and feature-branch changes together before the next launch (a very valid ask).&lt;/p&gt;&lt;p&gt;Now the issue is that if we get an critical bug that needs to be fixed ASAP, then the trunk is no longer clean to do a bug-fix and re-deploy.&lt;/p&gt;&lt;p&gt;However, on the other hand, we could not have merged the trunk changes into the feature-branch, because merging back (into trunk, after feature-branch work is done and tested) would have been a nightmare. Basically we chose the easier merge (feature-branch -&gt; trunk) now, in order to avoid the (super) messy merge (trunk -&gt; feature-branch -&gt; trunk) later on.&lt;/p&gt;&lt;p&gt;I think we faced this issue because how SVN handles merges. Basically, it takes the 2 copies of the same file at the HEAD of each branch and tries to do a intelligent merge; it does NOT take the history of the changes into consideration while merging. This causes conflicts&lt;/p&gt;&lt;ul&gt;&lt;li&gt;less when we do feature-branch -&gt; trunk, because (hopefully) the same files would not have been touched in both the branches (see part-1 of figure above)&lt;/li&gt;&lt;li&gt;but (too) many when we do trunk -&gt; feature-branch -&gt; trunk, because every (merge) file is touched in both branches (see part-2 of figure above)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So how do we solve this situation? What do we do?&lt;/p&gt;&lt;p&gt;Enter git! In git, then branch and commit situation is extremely different (and easy). After creating a feature-branch, one should do &quot;git rebase &lt;trunk&gt;&quot; regularly to pull changes from trunk (known as &quot;master&quot; in git-land) into the feature-branch (see parts-3, 4 and 5 of figure above). This way, the conflicts, if any, will be very small and easily resolvable. Once the feature development and testing is complete, on simply does &quot;git merge &lt;feature-branch&gt;&quot;.&lt;/feature-branch&gt;&lt;/trunk&gt;&lt;/p&gt;&lt;p&gt;The benefit of this approach is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the feature-branch always has all trunk changes, so the client-team request can be easily fulfilled right from the feature-branch&lt;/li&gt;&lt;li&gt;final merge (feature-branch -&gt; trunk) is super easy, because no 2 files have changed simultaneously (if they had, then these merge-conflicts have already been resolved during the &quot;rebase&quot; step)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I don't know how to achieve the same in SVN; atleast not easily. Please share if you do!&lt;/p&gt;&lt;p&gt;Fortunately, for us, the issue turned out to be a false-positive, and we got spared. ps: Read &lt;a href='http://paulhammant.com/2013/04/05/what-is-trunk-based-development/'&gt;this&lt;/a&gt;, &lt;a href='http://guides.beanstalkapp.com/version-control/branching-best-practices.html'&gt;this&lt;/a&gt; and &lt;a href='http://svnbook.red-bean.com/en/1.7/svn.branchmerge.commonpatterns.html'&gt;this&lt;/a&gt; for more info on common branching patterns&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 13 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-11-speed-your-website-100-times.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-11-speed-your-website-100-times.html
</link>
<title>
Speed up your website 100x
</title>
<description>
&lt;p&gt;If you remember, we had mentioned in one of our recent posts that we increased the speed of our editorial site by 100 times. That's right, 100 times! So what is the secret? How did we do it? Well, actually there's the short answer and the long answer.&lt;/p&gt;&lt;p&gt;For all you impatient folks out there, the short answer first: We put extremely long processes (like sending email) into the background and returned the page without waiting for its result!&lt;/p&gt;&lt;p&gt;However, in reality things are not so simple. As &lt;a href='http://www-cs-faculty.stanford.edu/~knuth/'&gt;Donald Knuth&lt;/a&gt; put it:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Premature optimization is the root of all evil &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;So we did what was correct: measure. We performed the following steps:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;measure page speed (as perceived by end users) using tools like &lt;a href='http://yslow.org/'&gt;yslow&lt;/a&gt;], &lt;a href='https://developers.google.com/speed/pagespeed/'&gt;pagespeed&lt;/a&gt;], etc&lt;/li&gt;&lt;li&gt;figure out which requests are taking time&lt;/li&gt;&lt;li&gt;split the above requests into frontend (network and client-side) and backend (server-side) categories (explanation of these categories is for another post ;). for now, lets concentrate only on backend issues)&lt;/li&gt;&lt;li&gt;during our measurements, we found that the article create/update page was the slowest&lt;/li&gt;&lt;li&gt;this was mainly due to activities like sending emails and submitting pages to web-archive.com took the most times. but, we realized that the user need not wait for these activities to finish. thus, by simply putting these activities into a separate thread (and not waiting for the thread to finish execution), we were able to bring down the page load time from 3 seconds to about 30 milliseconds; a whopping gain of 100x&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The lessons that we learned from the above exercise were&lt;/p&gt;&lt;ul&gt;&lt;li&gt;don't do premature optimization&lt;/li&gt;&lt;li&gt;measure before starting to optimize&lt;/li&gt;&lt;li&gt;go for the low hanging fruits (find the thing that will give the maximum ROI)&lt;/li&gt;&lt;li&gt;don't optimize parts that will not have any impact (for eg, improving the performance of some rarely called function from 5ms to 4ms by spending 2 hours is pretty much useless)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We would like to hear about your optimization experiences!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 11 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-12-09-exciting-new-features-on-golbin.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-12-09-exciting-new-features-on-golbin.html
</link>
<title>
Exciting new features on Golbin!
</title>
<description>
&lt;p&gt;We have been absent from writing articles for some time now because we were working on improving the &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt; experience. For both viewers and story-tellers! So lets see what's new in &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Load comments via &lt;a href='http://en.wikipedia.org/wiki/Ajax_(programming&amp;#41;'&gt;ajax&lt;/a&gt;&lt;/strong&gt;: This feature will load comments in a lazy fashion; that is, after the whole page has been loaded. Because of this, the page-load-time (as seen by search engines like Google, Bing, Yahoo!, etc) will improve and give &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt; a better &lt;a href='http://en.wikipedia.org/wiki/Search_engine_optimization'&gt;SEO&lt;/a&gt; ranking.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Threaded comments&lt;/strong&gt;: Viewers can now reply to existing comments, or add fresh new comments of their own. We hope that this will increase viewer engagement. Not only via interaction within themselves, but even the story-teller will be able to respond to particular comments and have a meaningful, relevant and easy-to-follow conversation!&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Page backgrounds&lt;/strong&gt;: Story-tellers can now define a custom image for all of their pages (story pages, and author page) and make their stories more beautiful and attractive for viewers. Also, this will help them distinguish themselves from other story-tellers, and portray their personality more effectively!&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Improved editorial site performance&lt;/strong&gt;: Last, but not least, we made some performance improvements to the editorial site. This small change alone brought the page timings down from 3 seconds to 30 milliseconds, an improvement of &lt;strong&gt;100x!&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That's not all. We will be adding new features, both small and big, regularly. Sometimes they will be user facing (affecting either viewers and/or story-tellers) and sometimes not. But either ways, we will keep you folks updated. See you soon, with the next set of updates!&lt;/p&gt;&lt;p&gt;ps: We would be delighted to hear from you about what features you would like to be added to our site! Also, don't feel shy to go ahead, register, and share your stories with the world! Click &lt;a href='http://ed.golb.in/register/why/'&gt;here&lt;/a&gt; to see the benefits of writing on &lt;a href='http://www.golb.in/'&gt;Golbin!&lt;/a&gt;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 09 Dec 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-10-15-give-a-man-a-fish.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-10-15-give-a-man-a-fish.html
</link>
<title>
Give a man a fish...
</title>
<description>
&lt;p&gt;I have this (weird?) habit of collecting sentences &amp;ndash; quotes, proverbs, and the like. Any time i read something interesting anywhere, be it a book, blog or a magazine, or even a billboard, anything really, i note it down into my small (about 550+ lines long now) file of quotes. I also have a small program i wrote more than 5 years ago that shows me 1 line from the file everyday, and also sets it as my messenger status and email signature.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Anyways, a few days ago, i read the following quote:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;a href='http://www.iwise.com/tocbM'&gt;Catch a man a fish and you can sell it to him. Teach a man to fish, and you ruin a wonderful business opportunity. &amp;ndash;Karl Marx&lt;/a&gt;&lt;/p&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;and immediately my thoughts went to the proverb:&lt;/p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;a href='http://www.quotationspage.com/quote/2279.html'&gt;Give a man a fish and you feed him for a day. Teach a man to fish and you feed him for a lifetime. &amp;ndash;Chinese proverb&lt;/a&gt;&lt;/p&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;There is hardly a man who does not know the above proverb.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;But what struck me about these 2 quotes was that more important than a &quot;thing&quot;, is the &quot;point-of-view&quot;. How you view something, or what you are thinking when viewing something might completely change the appearance of that thing. This applies to not only inanimate things, but also to people. We tend to have pre-conceived notions about certain people, or group of people and label them. &lt;a href='http://en.wikipedia.org/wiki/Chimamanda_Ngozi_Adichie'&gt;Chimamanda Adichie&lt;/a&gt; tells beautifully the dangers of having pre-conceived notions in this &lt;a href='http://www.ted.com/'&gt;ted&lt;/a&gt; &lt;a href='http://www.ted.com/talks/chimamanda_adichie_the_danger_of_a_single_story.html'&gt;talk&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;Not only that, quotes change over time, especially when different points-of-view come into play, as can be seen &lt;a href='http://www.amatecon.com/fish.html'&gt;here&lt;/a&gt;. Anyways, as they say, &quot;Beauty lies in the eyes of the be(er)-holder ;)&quot;&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Tue, 15 Oct 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-10-13-i-am-dreaming.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-10-13-i-am-dreaming.html
</link>
<title>
I am dreaming
</title>
<description>
&lt;p&gt;I was listening to a &lt;a href='http://www.radiolab.org/'&gt;radiolab&lt;/a&gt; episode a few days ago when I stumbled upon the words &quot;lucid dreaming&quot;. It piqued my curiosity and I looked it up only to find very interesting things about it. So what is &quot;lucid dreaming&quot;? To quote &lt;a href='http://www.dreamviews.com/content/what-lucid-dreaming-16/'&gt;dreamviews.com&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; A lucid dream is a dream in which you know you are dreaming. Typically this happens when the dreamer experiences something strange, and when they stop to question their reality, they realize they are in a dream. Lucid dreams happen naturally on occasion, although some people may have them naturally more often than others.&lt;/blockquote&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Now, why would anyone want to be aware of what dreams he&quot;s having? Well, there are some very interesting advantages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A dream (in which you are aware) is like talking with your subconscious. And I mean literally talk. You can ask it questions, and it will answer back. You can learn something about yourself which you never knew. Maybe find out your passions, fears, etc. Sometimes it&quot;s difficult to &quot;really&quot; know what you &quot;really&quot; like/dislike. This technique will help you find it.&lt;/li&gt;&lt;li&gt;Being aware that you are dreaming makes it possible to modify a dream and even control it. This makes A dream an ideal place for visualization and rehearsal. This is very useful due to the following 2 facts&lt;ul&gt;&lt;li&gt;Visualization techniques are great tools for &quot;goal achievement&quot; (for example, see &lt;a href='http://ezinearticles.com/?Visualization&amp;mdash;How-to-Achieve-Your-Goals&amp;amp;amp;id%3D8006701'&gt;here&lt;/a&gt;, &lt;a href='http://www.essentiallifeskills.net/visualization.html'&gt;here&lt;/a&gt;, &lt;a href='http://ezinearticles.com/?Visualization-Techniques&amp;mdash;Using-the-Mind-to-Achieve-Goals&amp;amp;amp;id%3D3534478'&gt;here&lt;/a&gt;, &lt;a href='http://www.huffingtonpost.com/frank-niles-phd/visualization-goals_b_878424.html'&gt;here&lt;/a&gt; and &lt;a href='http://voices.yahoo.com/goal-setting-visualization-techniques-263469.html'&gt;here&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;As mentioned &lt;a href='http://www.dreammoods.com/dreaminformation/dreamtypes/luciddreams.htm'&gt;here&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt; Because brain activity during the dream state is the same as during a real life event, what you &quot;learn&quot; or &quot;practice&quot; in your lucid dream state is similar to the training and preparation you do in the waking world.&amp;amp;nbsp; Your neuronal patterns are already being conditioned. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Lets come to the &quot;how&quot; of &quot;lucid dreaming&quot;. There is a ton of good material out there to help you if you are interested. But here are some starting points&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href='http://www.world-of-lucid-dreaming.com/'&gt;http://www.world-of-lucid-dreaming.com/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='http://www.dreammoods.com/dreaminformation/dreamtypes/luciddreams.htm'&gt;http://www.dreammoods.com/dreaminformation/dreamtypes/luciddreams.htm&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='http://www.dreamviews.com/content/what-lucid-dreaming-16/'&gt;http://www.dreamviews.com/content/what-lucid-dreaming-16/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='http://www.wikihow.com/Lucid-Dream'&gt;http://www.wikihow.com/Lucid-Dream&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='http://www.wikihow.com/Lucid-Dream'&gt;http://www.wikihow.com/Lucid-Dream&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 13 Oct 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-10-11-virtualbox-guest-additions-on-sabayon-linux.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-10-11-virtualbox-guest-additions-on-sabayon-linux.html
</link>
<title>
Virtualbox guest-additions on Sabayon linux
</title>
<description>
&lt;p&gt;First, things keep changing continuously in the Linux world. Secondly, configurations differ, ergo YMMV ;)&lt;/p&gt;&lt;p&gt;With that out of the way, lets attack the main issue. Thing is, i spent a total of 6 hours trying to get virtualbox guest additions to work my Sabayon Linux guest on a WindoZZZe-7 host :((&lt;/p&gt;&lt;p&gt;Unfortunately the &lt;a href='https://wiki.sabayon.org/index.php?title%4DHOWTO:_Install_VirtualBox'&gt;Sabayon wiki&lt;/a&gt; does not help much for when Sabayon is installed as a guest&lt;/p&gt;&lt;p&gt;I got all sorts of errors right from guest addition features like full-screen and shared folder mounting not working to xorg crash without much helpful error logs. Also, simply installing virtualbox, pulls a new kernel, which makes things even more difficult. Anyways, long story short, i was able to solve all these problems by following the steps below. Do note down your kernel and virtualbox version numbers as they might vary with the ones given below&lt;/p&gt;&lt;pre&gt;&lt;code&gt;$ equo install sys-kernel/sabayon-sources-3.11.2 sys-kernel/linux-sabayon-3.11.2 x11-drivers/xf86-video-virtualbox-4.2.18#3.11.2-sabayon app-emulation/virtualbox-modules-4.2.18#3.11.2-sabayon app-emulation/virtualbox-guest-additions-4.2.18#3.11.2-sabayon
$ kernel-switcher switch linux-server-3.11.2
add &amp;quot;modules=&amp;quot;vboxdrv vboxnetflt vboxnetadp vboxsf&amp;quot;&amp;quot; in /etc/conf.d/modules
add user to vboxusers and vboxguest groups
add virtualbox-guest-additions to default runlevel
$ eselect opengl set ati
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Do &lt;em&gt;not&lt;/em&gt; do the following. Basically don&quot;t use vboxvideo driver, instead use an empty xorg.conf file and things should just work fine&lt;/p&gt;&lt;pre&gt;&lt;code&gt;$ cp /usr/share/doc/virtualbox-guest-additions-4.2.18/xorg.conf.vbox /etc/X11/xorg.conf
$ eselect opengl set xorg-x11
append &amp;quot;vboxvideo&amp;quot; to /etc/conf.d/modules &amp;#40;in step-3 above&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Even after doing all above, i&quot;m still facing some issues. I&quot;d be grateful if someone could kindly tell me the solution, or even tell me a better/correct-er way of getting things to work:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;shared folders work weirdly: i cannot view/enter contents of shared-folder. however, i have a soft-link to a file inside a shared folder, and i am able to read/write from/to the file without any problem. weird, huh???
clipboard sharing does not work
&lt;/code&gt;&lt;/pre&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Fri, 11 Oct 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-03-03-to-blog-or-not-to-blog.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-03-03-to-blog-or-not-to-blog.html
</link>
<title>
To b(log) or not to b(log)
</title>
<description>
&lt;blockquote&gt;&lt;p&gt; &lt;a href='http://en.wikipedia.org/wiki/To_be,_or_not_to_be'&gt;To be, or not to be, that is the question&lt;/a&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;That is the opening phrase of a soliloquy in &lt;a href='http://en.wikipedia.org/wiki/William_Shakespeare'&gt;William Shakespeare&lt;/a&gt;'s play &lt;a href='http://en.wikipedia.org/wiki/Hamlet'&gt;Hamlet&lt;/a&gt;. That is a very serious question, and believe me, we are not going to discuss something that important. But what we are going to discuss is an interesting, if not important, question: to blog, or not to blog.&lt;/p&gt;&lt;p&gt;As you must have noticed, I am a &lt;a href='http://en.wikipedia.org/wiki/Rookie'&gt;rookie&lt;/a&gt; blogger. So what I am going to enumerate here are my thoughts about why one should start to blog, or not; I am not going to mention benefits or drawbacks of professional blogging (like making money, etc).&lt;/p&gt;&lt;p&gt;If you have ever blogged, or if you think about it deeply for just a short time, you will immediately realize the following benefits:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Clear thinking&lt;/strong&gt;: as a &lt;a href='http://www.thefreedictionary.com/wannabe'&gt;wannabe&lt;/a&gt; blogger, you have a lot of things in your mind that you want to pen down. The problem, nay challenge, is which of those things should be before others? In fact, which ones are suitable (good topics, not unnecessarily common/general, etc) to be blogged about in the first place. Also, once you have chosen the thing that you want to start writing, you might realize that the topic wasn't big enough to make a decent blog post. Or worse still, you might find that the thing in your mind was a very vague concept and you really don't know how to put it in words. Obviously you will have to think in detail about it and from various viewpoints. As you write more and more posts, you will do this exercise repeatedly and will get a hang of thinking more and more clearly. Starting to blog is like trying to clean up a wardrobe into which you have been dumping clothes your whole life. It's a mess. But slowly, you will remove the mess, throw away the old, torn, unnecessary clothes, clean the good ones, iron them and arrange them back neatly in the wardrobe. Needless to say, as you clear up your brain, other aspects of your life will start getting affected too.&lt;/p&gt;&lt;p&gt;I think this one reason itself is enough to start blogging!&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Ideas&lt;/strong&gt;: Once you have written a few posts, you will start looking for new ideas. Not that you have exhausted all of yours already, but because you are searching for that domain in which you are the most comfortable. You have ideas from all fields of life based on the experiences that you have had during your lifetime. But you want to write about things that are more relevant to your readers, probably related to what is happening in your society at this time, or probably about a certain technology that they are using, etc. What is important to note here is that a lot of ideas are flowing your way, and then who knows, probably you will yourself convert some idea into something more tangible and move from being a blogger to an &lt;a href='http://en.wikipedia.org/wiki/Entrepreneur'&gt;entrepreneur&lt;/a&gt;.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;a href='http://www.quotedb.com/quotes/147'&gt;There is nothing more powerful than an idea whose time has come. &amp;ndash;Victor Hugo&lt;/a&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;Reading and writing skills&lt;/strong&gt;: Obviously to gather ideas you will have to become a voracious reader. But at the same time, as your experience grows, you will also become a smart reader &amp;ndash; you will learn to differentiate between good content and bad. You will make for yourself a repository of places that you will visit frequently to hunt for ideas. Similarly, as you write more and more posts, your writing skills will improve too. You will learn to be able to break ideas into smaller digestible chunks and arrange them in a meaningful order. Also, you might already be good at writing, but might find out, like me, that what you wrote earlier is vastly different than blogging and will need to improve at it.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Discipline&lt;/strong&gt;: As with anything worth doing, blogging takes a lot of discipline. It is very easy to write a few posts in the excitement of starting something new and then getting bogged down with routine tasks and not finding time to blog. This is especially true at the beginning where it is easy to convince yourself that &quot;you were just trying it out&quot;, or &quot;you were never serious about it anyways&quot;, or &quot;i am not getting any benefit out of it&quot;, etc. But this is where you have to persist just long enough for it to become a &lt;a href='http://www.npr.org/2012/03/05/147192599/habits-how-they-form-and-how-to-break-them'&gt;habit&lt;/a&gt; and you will feel natural about it.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reach a wide audience&lt;/strong&gt;: The Internet has literally made the world a small place; you can reach a multitude of people from different countries, cultures, races and with different thinking. If you have &quot;comments&quot; feature on your blog, you will be surprised, sometimes, to find comments from varied people. I was pleasantly surprised to find out that there is someone on the other side of the earth who is interested in reading what i write; believe me, it was a very motivating moment.&lt;/p&gt;&lt;p&gt;Of course, like other things, blogging is a 2 sided coins and has drawbacks too.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Raw&lt;/strong&gt;: As a newbie, you will make a lot of mistakes. One of them is that you will either open up your heart too much or too little. In the former case you might get backlashed by your visitors (which is good for your blog ;), but not for you), whereas in the latter case your users might not connect to you and will become disinterested soon.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href='http://en.wikipedia.org/wiki/Analysis_paralysis'&gt;Analysis paralysis&lt;/a&gt;&lt;/strong&gt;: You might spend too much time reading other content to find ideas and analyzing what to write than actually writing something. This is actually not that bad; atleast you are increasing the horizon of your knowledge. It will surely come in handy at some point in your life.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Frustration&lt;/strong&gt;: This can be especially true with newbie bloggers like me, where we see no user comments for a long time and it might feel like speaking to an empty auditorium. Don't worry, try to figure out if you are doing something wrong and correct it. More importantly, be patient. Probably, the clock gears are already working in the background, but it will take some time for the hand to visibly move :)&lt;/p&gt;&lt;p&gt;For me, the pros outweigh the cons by a huge margin. I have started blogging and am not looking back now. I hope you will too!&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sun, 03 Mar 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2013-01-03-writing-flavors.html
</guid>
<link>
http://spradnyesh.github.io/posts/2013-01-03-writing-flavors.html
</link>
<title>
Writing Flavors
</title>
<description>
&lt;p&gt;As a birthday gift to my wife, I released this &lt;a href='http://www.golb.in/'&gt;website&lt;/a&gt; on her birthday and uploaded the articles that I had written already. I asked my wife to read them and give me feedback. Personally, I would have rated them in the order of:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;What makes you, you?&lt;/li&gt;&lt;li&gt;Forgetting &quot;forgive and forget&quot;&lt;/li&gt;&lt;li&gt;Discovering the joy of writing&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;But to my utter surprise, she rated them as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Discovering the joy of writing&lt;/li&gt;&lt;li&gt;Forgetting &quot;forgive and forget&quot;&lt;/li&gt;&lt;li&gt;What makes you, you?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href='http://www.golb.in/discovering-the-joy-of-writing-1.html'&gt;Discovering the joy of writing&lt;/a&gt; was my first article and I had written it without a very clear mind and without much purpose. For the other 2 articles, on the other hand, I had done quite some research and had taken sufficient time to write and rewrite them. How was it, then, that my wife liked &lt;a href='http://www.golb.in/discovering-the-joy-of-writing-1.html'&gt;Discovering the joy of writing&lt;/a&gt; the most? I asked her and she told me that she liked writings (articles, novels, etc) that talked to her. &lt;a href='http://www.golb.in/discovering-the-joy-of-writing-1.html'&gt;Discovering the joy of writing&lt;/a&gt; did just that. &lt;a href='http://www.golb.in/forgetting-forgive-and-forget-2.html'&gt;Forgetting &quot;forgive and forget&quot;&lt;/a&gt; and &lt;a href='http://www.golb.in/what-makes-you-you-3.html'&gt;What makes you, you?&lt;/a&gt;, however, are, what she calls, gyaan sessions, i.e. &quot;boring&quot;.&lt;/p&gt;&lt;p&gt;I thought, that if I am to write, I should understand the different styles of writing. And I am not talking here about grammar, but the different genres, if you will, of writing. I thought about it a lot and the following are the different styles that I could recognize:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Imagination&lt;/strong&gt;: Novels, poems fall into this category.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Reporting&lt;/strong&gt;: This is mostly laying down the facts about an event or incident.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Contemplation&lt;/strong&gt;: In this style, the writer has thought over and pondered about something and is writing his learnings about that topic.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Share&lt;/strong&gt;: Sometimes you find out something interesting or clever, most probably in your domain of work, and you want to share it with your friends and/or the community at large. You also want to note it down somewhere where it is easy to find it later if you need. Many people use personal blogs to accomplish this.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I am sure there will be many more styles. But these are the ones that I could think of. While enumerating them, I also realized that there are multiple axes across which writing styles could be calibrated. I can think about only two right now: the one above, and the &quot;talking&quot; versus &quot;boring&quot; that started this whole train of thought. However, I think that some combinations are more compatible with each other than others. For example, &quot;Imagination&quot; goes well with &quot;talking&quot;, whereas &quot;Reporting&quot; goes well with &quot;boring&quot; ;)&lt;/p&gt;&lt;p&gt;Of course, as a budding writer, I have to realize that I cannot be good at all styles, and neither do all people like to read all styles. So it is not possible to please everyone, and that is OK. What I have to do is to try my hands at various styles, figure out what I am most comfortable at, get feedback, and improve!&lt;/p&gt;&lt;p&gt;So to start off, I thought of doing an experiment. I decided to write this article in the &quot;talk&quot; style that my wife had told me earlier. It seems to be on the &quot;contemplation&quot; point on the other axis. Through this experiment, I want to find out whether I have the ability or knack of writing in this style and also see what are the views and opinions of my readers. It is for this reason that you will find no links pointing out to &lt;a href='http://en.wikipedia.org/wiki/Main_Page'&gt;wikipedia&lt;/a&gt; or other sites from this article. It is all my thoughts, and my thoughts only :)&lt;/p&gt;&lt;p&gt;Fortunately, I think that I have figured out the key to success. It&quot;s &quot;curiosity&quot;. It is the one thing that cuts across all barriers and all axes. It is curiosity, of any form, and in any domain, that makes us humans. I am sure, you must be curious that where I have used the curiosity factor in this article. Guess? Well, it is the &quot;title&quot; of this article: &quot;writing flavors&quot;. Did you not think that flavors are associated with taste, not writing. You must have guessed that this article would either be about some recipe or about writing recipes on blogs or somewhere. But then the summary of the article should tell you that that is not the case. So now you want to know what the article is really about, and that is what brought you till here!&lt;/p&gt;&lt;p&gt;So what do you think? Does this flavor suit me well?&lt;/p&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 03 Jan 2013 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2012-12-26-what-makes-you-you.html
</guid>
<link>
http://spradnyesh.github.io/posts/2012-12-26-what-makes-you-you.html
</link>
<title>
What makes you you?
</title>
<description>
&lt;p&gt;I'm sure you have wondered, at some time or the other, &quot;who am i?&quot; Probably, you are the kind of person who has meditated over it, or you might be the kind who does not give it a second thought. Probably you were born in a culture, like the &lt;a href='http://en.wikipedia.org/wiki/Culture_of_India'&gt;Indian culture&lt;/a&gt;, where you have heard this question from your elders, or from stories in your childhood. Probably you have gone one step further and wondered or heard questions like &quot;&lt;a href='http://www.gocomics.com/calvinandhobbes/1988/04/01'&gt;why am i here&lt;/a&gt;?&quot; or &quot;what am i here for?&quot;&lt;/p&gt;&lt;p&gt;But have you ever wondered what does it mean to &quot;be me&quot;? Over any period of time, be it short or long, we are continuously changing. Changing physically - children grow up, adults grow old; emotionally - our moods keep changing constantly; and in many other ways - life is change! But there is one thing that never changes, the feeling of &quot;me&quot;. No matter where we are, in what place, in what situation, we always know our &quot;selves&quot;. How does this happen? In fact, what is this self?&lt;/p&gt;&lt;p&gt;These questions can be looked at from both a philosophical as well as a scientific point of view. Today we are going to look at it from the scientific angle and delve further into questions like &quot;what is consciousness?&quot;, &quot;what does it mean to say 'i am me'?&quot;, &quot;where does this consciousness come from?&quot;, &quot;can we point to a point or location in the brain which generates consciousness?&quot;, and the like.&lt;/p&gt;&lt;p&gt;Lets start with the question of &quot;Do other creatures, have a sense of self?&quot; It appears that there are a few animals and birds that do have a recognition of self. It has been found that they can identify themselves and know the self from the others. It is very interesting to know how scientists figure out whether animals, or birds, can identify themselves. In fact, it's a simple but very clever trick; it's called the &lt;a href='http://en.wikipedia.org/wiki/Mirror_test'&gt;mirror test&lt;/a&gt; developed by &lt;a href='http://en.wikipedia.org/wiki/Gordon_G._Gallup'&gt;Dr. Gordon Gallup&lt;/a&gt; while doing some &lt;a href='http://www.sciencemag.org/content/167/3914/86'&gt;studies&lt;/a&gt;. Consciousness, or self, is a very intellectual thing. In the mirror test, the animal is able to create a representation of itself that floats free of its body. It can't touch it or smell it, but just by seeing it, it knows that &quot;that's me&quot;. This is very fascinating!&lt;/p&gt;&lt;p&gt;&lt;a href='http://www.montclair.edu/profilepages/view_profile.php?username%3Dkeenanj'&gt;Professor Julian Keenan&lt;/a&gt; has been able to pinpoint the location of where the idea of the self is generated in our brains. He created an &lt;a href='http://mentalhealth.about.com/library/sci/0201/blbeth0201.htm'&gt;experiment&lt;/a&gt; using some morphing software and images of famous people like Albert Einstein and Bill Clinton to &lt;a href='http://ttbook.org/book/transcript/transcript-you-your-brain'&gt;find&lt;/a&gt; out that if you&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; press your thumb to the bridge of your nose. Now draw it slowly over the crown of your head to about where you might have a ponytail. That area under your skull is where 'you' are. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;a href='http://en.wikipedia.org/wiki/Vilayanur_S._Ramachandran'&gt;Professor V. S. Ramachandran&lt;/a&gt;, a world renowned neurologist, says that &quot;self is the activities of the neurons.&quot; Although, simple when stated that way, it is, he says, one of the greatest realization in the last 100 years. He explains how the human brain is different from that of other animals. A worm, for example, does not have a brain big enough to form any sort of images in its mind. A monkey, on the other hand, although able to form images, cannot manipulate them in his mind. A human being, however, has a much more powerful brain. Only man can take images from the real world, pull them into his head, divide them into parts and then start turning those parts into abstractions, or tokens. He can manipulate these tokens and juxtapose them in counterintuitive ways; he can create even outlandish scenarios - what we call imagination. We are not different from other animals, we are only more than them. We are imagining so often and so thoroughly, that we can eventually imagine ourselves! He says, self is the ability of humans to spin a story around themselves. We can take experiences, memories of good and bad events, and abstract a story out of it - that is self!&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;The strange case of Dr. Jekyll and Mr. Hyde&quot; src=&quot;http://ecx.images-amazon.com/images/I/51eDu25PvnL.&lt;i&gt;SX311&lt;/i&gt;BO1,204,203,200_.jpg&quot; width=600 /&gt;&lt;br /&gt; &lt;a href='htww.amazon.com/Strange-Jekyll-Dover-Thrift-Editions/dp/0486266885'&gt;photo attribution&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Readers of classic novels are not unaware of &lt;a href='&quot;&lt;a href='http://www.amazon.com/Strange-Jekyll-Dover-Thrift-Editions/dp/0486266885'&gt;The strange case of Dr. Jekyll and Mr. Hyde&lt;/a&gt;&quot; by &lt;a href='http://en.wikipedia.org/wiki/Robert_Louis_Stevenson'&gt;Robert Louis Stevenson&lt;/a&gt;. He said that he used to dream of &quot;&lt;a href='http://blog.beliefnet.com/dreamgates/2011/07/robert-louis-stevenson-and-his-dream-helpers.html'&gt;little people&lt;/a&gt;&quot; who helped him write his stories. He trained himself to remember his dreams and to dream plots for his books. Dreams bring us to a very interesting question about consciousness. If we think about this in modern terms, like when you see a movie, you, the viewer, has no idea what is going to happen next. You scream at the scary parts, laugh at the jokes, and cry during the sad scenes. But in order for you to have that experience, someone needed to write the movie, someone needed to direct it, someone other than you. How is it, when we dream, that we do all 3 at the same time? We write, direct and watch the film as if we've never seen it before.&lt;/p&gt;&lt;p&gt;This is how &lt;a href='http://www.wnyc.org/people/jad-abumrad/'&gt;Jad Abumrad&lt;/a&gt; and &lt;a href='http://www.wnyc.org/people/robert-krulwich/'&gt;Robert Krulwich&lt;/a&gt; of &lt;a href='http://www.radiolab.org/'&gt;Radiolab&lt;/a&gt;, in their episode &quot;&lt;a href='http://www.radiolab.org/2007/may/07/'&gt;Who Am I?&lt;/a&gt;&quot; find it:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; If you look scientifically into a brain, what you encounter is hundreds of thousands of players, not just little people, but teeny teeny teeny teeny brain cells which do others fashion back and forth. If you would go to any one of the cells and say, &quot;so, are you the author of 'Jackyl and Hyde'?&quot;, the cell would just go on or off. It is only in the group that you can see the electrical outline of a thought, or ultimately of a self. While you think of yourself as a 'one', even the thought, 'I am a one', springs from a 100 million cells connecting through a trillion synapses and that all of this multiple activities, paradoxically, creates the you of this moment. You are always plural!&lt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;There is also a medical side to our consciousness. There is a condition of the brain called &lt;a href='http://en.wikipedia.org/wiki/Aneurysm'&gt;cerebral aneurysm&lt;/a&gt; in which the person loses his sense of self; he becomes a completely different person. Writer and producer Hannah Palin writes about her experience when her mother suffered from aneurysm in &quot;&lt;a href='http://www.transom.org/shows/2004/200403_head_explode.html'&gt;The day my mother's head exploded&lt;/a&gt;&quot;. I like the following paragraph from the passage a lot:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; But then those moments pass and you're consumed by the trivia of daily life once again. Sometimes, when i'm overwhelmed by making my way through the world, i try to focus on the fact that the electric bill does not matter. The idiot driver glued to his cellphone, does not matter. The mind numbing day job, truly, does not matter. But welcoming the strange and the different, being open and available for my husband, my friends, my family, experiencing love and laughter as often as possible - that's what matters. Because it can all be taken away in one brilliant flash. &lt;/p&gt;&lt;/blockquote&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 26 Dec 2012 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2012-12-24-forgetting-forgive-and-forget.html
</guid>
<link>
http://spradnyesh.github.io/posts/2012-12-24-forgetting-forgive-and-forget.html
</link>
<title>
Forgetting &quot;forgive and forget&quot;
</title>
<description>
&lt;p&gt;&lt;a href='http://en.wikipedia.org/wiki/Albert_Einstein'&gt;Albert Einstein&lt;/a&gt; once said&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Use the brain to process (think), not to store (remember). &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Frankly, I don't know if &lt;a href='http://en.wikipedia.org/wiki/Albert_Einstein'&gt;Einstein&lt;/a&gt; really said that, but I know that &lt;a href='http://en.wikipedia.org/wiki/Studs_Terkel'&gt;Studs Terkel&lt;/a&gt; said&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; I like quoting Einstein. Know why? Because nobody dares contradict you. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Anyways, the point I am trying to make is about forgetting. I think &quot;forget&quot; is a very important part in the preaching &quot;forgive and forget&quot; because forgiveness, although ideal and saintly, is extremely hard to practice, especially equally and towards all. But it is easy to forget. Actually, it is difficult, nay, impossible, to remember. Now that seems like a bold, very bold, claim. But hang on, and I will explain it shortly.&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;File Cabinet&quot; src=&quot;http://img.diytrade.com/cdimg/845903/7734917/0/1230259307.jpg&quot; width=600 /&gt;&lt;br /&gt; &lt;a href='http://xingfei169.diytrade.com/sdp/845903/4/pd-4400116/4969664.html'&gt;photo attribution&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Lets start by understanding what our memory is like, and how does the process of remembering really work. For a long time it was believed that the memory is like a file cabinet, or a computer hard disk (for the geeks among you). When a memory is created, it is like a file is written and stored in a drawer or a folder. Later, when you remember the memory, it is like retrieving the file and reading the contents off of it. There is absolutely no difference between what is written in the file and what is later read from it, no matter how many times. Of course, one might forget which drawer or folder the file was kept in, and he won't be able to remember that memory; but the memory itself is very much still there.&lt;/p&gt;&lt;p&gt;However, studies have found that the above analogy is wrong, very wrong! The brain does not function this way. To understand memory, we have to understand the structure of the brain. The brain is made up of a lot of cells called &lt;a href='http://en.wikipedia.org/wiki/Neuron'&gt;neuron&lt;/a&gt;s which are connected to each other via protein chains or links. It is these neurons and the links that make up memories. Let us map this understanding of the brain structure to a memory. A memory is really an event in our past that consists of items (for lack of a better word) like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;living things like people, animals, plants, etc&lt;/li&gt;&lt;/li&gt;&lt;li&gt;inanimate objects like buildings, cars, lamp posts, furniture, etc&lt;/li&gt;&lt;/li&gt;&lt;li&gt;location, weather, etc&lt;/li&gt;&lt;/li&gt;&lt;li&gt;conversations, emotions, etc&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt=&quot;At one end of an elongated structure is a branching mass. At the centre of this mass is the nucleus and the branches are dendrites. A thick axon trails away from the mass, ending with further branching which are labeled as axon terminals. Along the axon are a number of protuberances labeled as myelin sheaths.&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/b/bc/Neuron_Hand-tuned.svg&quot; width=600&gt;&lt;br /&gt; &lt;a href='http://en.wikipedia.org/wiki/Neuron'&gt;photo attribution&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Each of these items are stored in neurons and creation of a memory is like creating links or bridges between these neuron cells. Thus memory is inherently associative in nature, and the act of creating a memory is actually creating these associations.&lt;/p&gt;&lt;p&gt;Remembering is the process of scanning these neurons and associations between them. It is a highly imperfect process. It is very much possible that during remembering some of the neurons might not fire, that is, we might miss out on some associations. It is also possible, that some other neurons might fire, which means that we may remember (actually, imagine) something that never happened, or wasn't present, in the particular memory that we're remembering. Thus remembering is a highly creative process; we are, astonishingly, (re-)creating the memory!&lt;/p&gt;&lt;p&gt;Now we know that a memory has a physical presence in the brain; in the form of protein bridges between neurons. We also know that the process of creation and retrieval of a memory is not static, like that of a file cabinet or a computer hard drive, but a very dynamic one. Good! But what are it's implications? That is the next question that naturally comes to mind. Lets find out.&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;Eternal Sunshine of the Spotless Mind&quot; src=&quot;http://ia.media-imdb.com/images/M/MV5BMTY4NzcwODg3Nl5BMl5BanBnXkFtZTcwNTEwOTMyMw@@.&lt;i&gt;V1&lt;/i&gt;SX214&lt;i&gt;AL&lt;/i&gt;.jpg&quot; width=600 /&gt;&lt;br /&gt; &lt;a href='http://www.imdb.com/title/tt0338013/'&gt;photo attribution&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Selective deletion&lt;a href='http://www.wired.com/magazine/2012/02/ff_forgettingpill/all/'&gt;1&lt;/a&gt;&lt;a href='http://www.scientificamerican.com/article.cfm?id%3Dtotaling-recall'&gt;2&lt;/a&gt;&lt;a href='http://www.dailymail.co.uk/sciencetech/article-2206291/The-fear-factor-Researchers-discovery-technique-erase-short-newly-formed-memories.html'&gt;3&lt;/a&gt;&lt;a href='http://discovermagazine.com/2008/jan/how-to-erase-a-single-memory/'&gt;4&lt;/a&gt;: There is a drug called &lt;a href='http://en.wikipedia.org/wiki/Anisomycin'&gt;Anisomycin&lt;/a&gt; that prevents the creation of the linkage protein. This drug can be used to prevent a memory from being created if administered at the moment when the memory is being formed. However, it can also be used to delete a memory when it is being remembered. This is used clinically to treat &lt;a href='http://en.wikipedia.org/wiki/Posttraumatic_stress_disorder'&gt;post traumatic stress disorder&lt;/a&gt; or &lt;a href='http://www.nimh.nih.gov/health/topics/post-traumatic-stress-disorder-ptsd/index.shtml'&gt;PTSD&lt;/a&gt;. A very good movie &lt;a href='http://www.imdb.com/title/tt0338013/'&gt;Eternal sunshine of the spotless mind&lt;/a&gt; was made using this concept. It also gives us some clues about why &lt;a href='http://en.wikipedia.org/wiki/Amnesia'&gt;amnesia&lt;/a&gt; happens. And of course, who would not remember about the movie &lt;a href='http://www.imdb.com/title/tt0209144/'&gt;Memento&lt;/a&gt; when we talk about amnesia? The scariest and most mysterious case about amnesia is about &lt;a href='http://en.wikipedia.org/wiki/Clive_Wearing'&gt;Clive Wearing&lt;/a&gt; &lt;a href='http://www.brainathlete.com/cold-sore-virus-led-severe-amnesia/'&gt;1&lt;/a&gt; &lt;a href='http://www.human-memory.net/disorders_retrograde.html'&gt;2&lt;/a&gt; &lt;a href='http://www.youtube.com/watch?v%3DWmzU47i2xgw'&gt;3&lt;/a&gt; who cannot remember anything for more than a few moments. Every moment for him is as if he has woken up from a long night's sleep, except that this night has been going on for every moment of more than the last 2 decades.&lt;/p&gt;&lt;p&gt;Of course, &quot;selective deletion&quot; of memory screams for ethics &lt;a href='http://www.livescience.com/15621-memory-altering-drugs-debate.html'&gt;1&lt;/a&gt; &lt;a href='http://www.salon.com/2011/12/31/should_we_erase_painful_memories/'&gt;2&lt;/a&gt;. Is it ethical to delete a memory from a person's brain? As always, this coin has 2 sides too:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;No&lt;ul&gt;&lt;li&gt;A memory is a very private and personal part of who one is. It is like stealing, forever, a part of your body!&lt;/li&gt;&lt;li&gt;Every person is a sum total of his experiences, his memories. Without memories, he is not what he is!&lt;/li&gt;&lt;li&gt;It can be used for criminal activities.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Yes&lt;ul&gt;&lt;li&gt;A memory is not static thing that is stored (created) and retrieved (remembered) with the exact same content every time. It is a process of re-creation; it is the most recent recollection of what is true. This means that it changes slightly every time you remember it; it becomes more about you (your perception/interpretation of the event) than about the event itself.&lt;/li&gt;&lt;li&gt;It can be used for treatment and giving a new life to patients.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt=&quot;Inception&quot; src=&quot;http://ia.media-imdb.com/images/M/MV5BMjAxMzY3NjcxNF5BMl5BanBnXkFtZTcwNTI5OTM0Mw@@.&lt;i&gt;V1&lt;/i&gt;SX214&lt;i&gt;AL&lt;/i&gt;.jpg&quot; width=600 /&gt;&lt;br /&gt; &lt;a href='http://www.imdb.com/title/tt1375666/'&gt;photo attribution&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The fact that memory is not static, but gets re-created anew, modified slightly, each time, has huge implications; like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Implanting false memories: I'm sure that you are thinking about the movie &lt;a href='http://www.imdb.com/title/tt1375666/'&gt;Inception&lt;/a&gt; right now. And thinking that this guy is surely kidding. But believe me, I am not. It is not only possible, but relatively easy to do. &lt;a href='http://en.wikipedia.org/wiki/Elizabeth_Loftus'&gt;Dr. Elizabeth Loftus&lt;/a&gt; and &lt;a href='http://en.wikipedia.org/wiki/Oliver_Sacks'&gt;Dr. Oliver Sacks&lt;/a&gt; have done a lot of ground breaking research on this subject. The reason it is easy to achieve is because of the fact that it is very much possible for some unrelated neurons to fire, just as it is possible for some related neurons to not fire.&lt;/li&gt;&lt;/li&gt;&lt;li&gt;This also means that nothing you remember, or worse still, know, is provably true. Watch the excellent &lt;a href='http://www.ted.com/'&gt;Ted talk&lt;/a&gt; &quot;&lt;a href='http://www.ted.com/talks/scott_fraser_the_problem_with_eyewitness_testimony.html'&gt;Scott Fraser: Why eyewitnesses get it wrong&lt;/a&gt;&quot; on this subject to know how this can have huge, even legal, implications:&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt; All our memories are reconstructed memories. They are the product of what we originally experienced and everything that's happened afterwards. &amp;ndash; &lt;a href='http://www.ted.com/speakers/scott_fraser.html'&gt;Scott Fraser&lt;/a&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Finally I would like to end with an awesome short essay by the Romanian essayist &lt;a href='http://en.wikipedia.org/wiki/Andrei_Codrescu'&gt;Andrei Codrescu&lt;/a&gt; from the book &lt;a href='http://en.wikipedia.org/wiki/101_Damnations_(book'&gt;101 Damnations&lt;/a&gt;)&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; The other day a friend of mine was explaining how she had to move these pixels around her computer and had to add 20 megabytes of memory to handle the operation. I had the disquieting thought that all this memory she was adding had to come from somewhere. Maybe it was coming from me because I couldn't remember a thing that day. And then it became blindingly obvious: All the memory that everybody keeps adding comes from people. Nobody can remember a damn thing. Every time someone adds memory to their machine, thousands of people forget everything they knew. Americans are singularly devoid of memory these days. We don't remember where we came from, who raised us, when our wars use to be, what happened last year, last month, or even last week. School children remember practically nothing. I take the greyhound bus every week, and I swear people on there don't know where they got on or where they're supposed to get off. They explanation is simple: computer companies are stealing human memory to stuff their hard drives. Greyhound I believe has some kind of contract with IBM to steal the memory of everybody riding the bus. They're probably connected by a cable or something. Every hundred miles, poof! Another 500 megabytes get sucked out of the passengers' brains. The computer's thirst for memory is bottomless: the more they suck, the more they need. Eventually, we'll be walking around with a glazed look in our eyes, trying to figure out who it is we live with. And then we'll forget our names and addresses and we'll just be milling around trying to remember them. The only thing visible about us will be these cables sticking out of our behinds, feeding the scraps of our memory to computer central, somewhere in Oblivion, USA. I think it's time for all these memory sucking companies to start some kind of system to feel and shelter us when we forget how to eat, walk, and sleep. &lt;/p&gt;&lt;/blockquote&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 24 Dec 2012 00:00:00 +0530
</pubDate>
</item>
<item>
<guid>
http://spradnyesh.github.io/posts/2012-12-22-discovering-the-joy-of-writing.html
</guid>
<link>
http://spradnyesh.github.io/posts/2012-12-22-discovering-the-joy-of-writing.html
</link>
<title>
Discovering the joy of writing
</title>
<description>
&lt;p&gt;Getting this &lt;a href='http://www.golb.in'&gt;website&lt;/a&gt; built was a difficult but manageable task. But where do I get the content from? I am no writer myself. I have written a few blog &lt;a href='http://blog.spradnyesh.org'&gt;posts&lt;/a&gt;, but those were long ago and there was definitely no discipline there. Those were some random, and more importantly, sporadic thoughts that I penned down. However what I am trying to do now is a totally different beast. Of course, the idea is to attract people to write on this website, but before that there needs to be atleast some initial content. It is easier to add content and ideas to an existing work, but very difficult to start something oneself. So that is the responsibility I must take onto myself, to initiate and to lay down some basic foundation.&lt;/p&gt;&lt;p&gt;Ok, so now I have got myself a goal and some determination too. But that does not solve the problem of skill. Having a goal and determination does not make me a writer. So I do the only thing that makes sense, procrastinate! After having delayed the task for a few weeks, I decided to do the inevitable:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; The best way to get started is to get started. &amp;ndash; Anonymous &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The first thing that I needed to do was get over &lt;a href='http://en.wikipedia.org/wiki/Inertia'&gt;inertia&lt;/a&gt;; I needed to get rid of the &lt;a href='http://en.wikipedia.org/wiki/A_rolling_stone_gathers_no_moss'&gt;moss&lt;/a&gt;, get the stone rolling. I started to think about what I should write. That's when I remembered a sentence from one of &lt;a href='http://en.wikipedia.org/wiki/Purushottam_Laxman_Deshpande'&gt;P. L. Deshpande&lt;/a&gt;'s essays 'Majhe Poshtk Jeevan':&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Shevati kaay ho, apan sagle jana aahot na, te pattyatlya navache dhani. Majkuracha maalak niralach asto. &amp;ndash; P. L. Deshpande &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Literally translated, it means &quot;Finally, we all are just the owners of the name on the address. The owner of the content is someone completely different.&quot;&lt;/p&gt;&lt;p&gt;I have always been fascinated with quotations. I have a collection of quotations and phrases, not only from great people, but also anonymous, and even short sentences that I have found very interesting; anything that I have read or heard from anywhere. The most interesting thing that i find about quotations or phrases is that they are open to interpretations. The same sentence can mean different things to different people. Actually, it can mean different things to the same person when read at different times, in different moods, in different circumstances. I think that more important than the sentence is its interpretation; what one gets or understands, what one perceives from it.&lt;/p&gt;&lt;p&gt;For me, in the context of writing my first article on &lt;a href='http://www.golb.in'&gt;Golbin&lt;/a&gt;, the above sentence meant a lot:     &lt;/p&gt;&lt;ul&gt;&lt;li&gt;I am the author, but what i write is consciously or sub-consciously told to me by &lt;i&gt;the&lt;/i&gt; author himself&lt;/li&gt;&lt;li&gt;This takes away some pressure from my mind about how will the article be received by the audience&lt;/li&gt;&lt;li&gt;I realize that this is only my first attempt; with practice and perseverance I am only going to improve&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is my &lt;a href='http://en.wikipedia.org/wiki/Eureka_effect'&gt;eureka&lt;/a&gt; moment! I now understand what is actually meant by inspiration and it culminates in the following &lt;a href='http://en.wikiquote.org/wiki/Thomas_Edison'&gt;quote&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Genius is one percent inspiration, ninety nine percent perspiration. &amp;ndash; Thomas Alva Edison &lt;/p&gt;&lt;/blockquote&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Sat, 22 Dec 2012 00:00:00 +0530
</pubDate>
</item>
</channel>
</rss>
